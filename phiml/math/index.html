<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>phiml.math API documentation</title>
<meta name="description" content="Vectorized operations, tensors with named dimensions …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phiml.math</code></h1>
</header>
<section id="section-intro">
<p>Vectorized operations, tensors with named dimensions.</p>
<p>This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.</p>
<p>Main classes: <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code>, <code>Extrapolation</code>.</p>
<p>The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiML/">https://tum-pbs.github.io/PhiML/</a></p>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phiml.math.extrapolation" href="extrapolation.html">phiml.math.extrapolation</a></code></dt>
<dd>
<div class="desc"><p>Extrapolations are used for padding tensors and sampling coordinates lying outside the tensor bounds.
Standard extrapolations are listed as global …</p></div>
</dd>
<dt><code class="name"><a title="phiml.math.magic" href="magic.html">phiml.math.magic</a></code></dt>
<dd>
<div class="desc"><p>Magic methods allow custom classes to be compatible with various functions defined in <code><a title="phiml.math" href="#phiml.math">phiml.math</a></code>, analogous to how implementing <code>__hash__</code> allows …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phiml.math.INF"><code class="name">var <span class="ident">INF</span></code></dt>
<dd>
<div class="desc"><p>Floating-point representation of positive infinity.</p></div>
</dd>
<dt id="phiml.math.NAN"><code class="name">var <span class="ident">NAN</span></code></dt>
<dd>
<div class="desc"><p>Floating-point representation of NaN (not a number).</p></div>
</dd>
<dt id="phiml.math.NUMPY"><code class="name">var <span class="ident">NUMPY</span></code></dt>
<dd>
<div class="desc"><p>Default backend for NumPy arrays and SciPy objects.</p></div>
</dd>
<dt id="phiml.math.PI"><code class="name">var <span class="ident">PI</span></code></dt>
<dd>
<div class="desc"><p>Value of π to double precision</p></div>
</dd>
<dt id="phiml.math.f"><code class="name">var <span class="ident">f</span></code></dt>
<dd>
<div class="desc"><p>Automatic mapper for broadcast string formatting of tensors, resulting in tensors of strings.
Used with the special <code>-f-</code> syntax.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from phiml.math import f
&gt;&gt;&gt; -f-f'String containing {tensor1} and {tensor2:.1f}'
# Result is a str tensor containing all dims of tensor1 and tensor2
</code></pre></div>
</dd>
<dt id="phiml.math.math"><code class="name">var <span class="ident">math</span></code></dt>
<dd>
<div class="desc"><p>Convenience alias for the module <code><a title="phiml.math" href="#phiml.math">phiml.math</a></code>.
This way, you can import the module and contained items in one line.</p>
<pre><code>from phiml.math import math, Tensor, wrap, extrapolation, l2_loss
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Vectorized operations, tensors with named dimensions.

This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.

Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.

The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.

See the documentation at https://tum-pbs.github.io/PhiML/
&#34;&#34;&#34;

from ..backend._dtype import DType
from ..backend import NUMPY, precision, set_global_precision, get_precision, set_global_default_backend as use

from ._shape import (
    shape, Shape, EMPTY_SHAPE, DimFilter,
    spatial, channel, batch, instance, dual,
    non_batch, non_spatial, non_instance, non_channel, non_dual, non_primal, primal,
    merge_shapes, concat_shapes, IncompatibleShapes,
    enable_debug_checks,
)

from ._magic_ops import (
    slice_ as slice, unstack,
    stack, concat, ncat, tcat, ccat, scat, icat, dcat, expand,
    rename_dims, rename_dims as replace_dims, pack_dims, dpack, ipack, spack, cpack, unpack_dim, flatten, squeeze,
    b2i, c2b, c2d, i2b, s2b, si2d, d2i, d2s,
    copy_with, replace, find_differences
)

from ._tensors import Tensor, wrap, tensor, layout, native, numpy_ as numpy, reshaped_numpy, Dict, to_dict, from_dict, is_scalar, BROADCAST_FORMATTER as f, save, load

from ._sparse import dense, get_sparsity, get_format, to_format, is_sparse, sparse_tensor, stored_indices, stored_values, tensor_like, matrix_rank

from .extrapolation import Extrapolation, as_extrapolation

from ._ops import (
    choose_backend_t as choose_backend, all_available, convert, seed, to_device,
    reshaped_native, reshaped_tensor, copy, native_call,
    print_ as print,
    slice_off,
    zeros, ones, fftfreq, random_normal, random_normal as randn, random_uniform, random_uniform as rand, random_permutation, pick_random,
    meshgrid, linspace, arange, arange as range, range_tensor,  # creation operators (use default backend)
    zeros_like, ones_like,
    pad,
    swap_axes,  # reshape operations
    sort,
    safe_div,
    where, nonzero, ravel_index,
    sum_ as sum, finite_sum, dsum, isum, ssum, csum,
    mean, finite_mean, dmean, imean, smean, cmean,
    std,
    prod, dprod, sprod, iprod, cprod,
    max_ as max, dmax, smax, imax, cmax, finite_max,
    min_ as min, dmin, smin, imin, cmin, finite_min,
    any_ as any, all_ as all, quantile, median,  # reduce
    at_max, at_min, argmax, argmin,
    dot,
    abs_ as abs, sign,
    round_ as round, ceil, floor,
    maximum, minimum, clip,
    sqrt, exp, erf, log, log2, log10, sigmoid, soft_plus, softmax,
    sin, cos, tan, sinh, cosh, tanh, arcsin, arccos, arctan, arcsinh, arccosh, arctanh, log_gamma, factorial, incomplete_gamma,
    to_float, to_int32, to_int64, to_complex, imag, real, conjugate, angle,
    radians_to_degrees, degrees_to_radians,
    boolean_mask,
    is_finite, is_nan, is_inf, nan_to_0,
    closest_grid_values, grid_sample, scatter, gather,
    histogram,
    fft, ifft, convolve, cumulative_sum,
    dtype, cast,
    close, always_close, assert_close, equal,
    stop_gradient,
    pairwise_differences, pairwise_differences as pairwise_distances, map_pairs,
    with_diagonal,
    eigenvalues, svd,
    contains, count_occurrences, count_intersections,
)

from ._nd import (
    shift, index_shift,
    vec, const_vec, norm, squared_norm, normalize, normalize as vec_normalize,
    dim_mask,
    normalize_to,
    l1_loss, l2_loss, frequency_loss,
    spatial_gradient, laplace,
    neighbor_reduce, neighbor_mean, neighbor_sum, neighbor_max, neighbor_min, at_min_neighbor, at_max_neighbor,
    fourier_laplace, fourier_poisson, abs_square,
    downsample2x, upsample2x, sample_subgrid,
    masked_fill, finite_fill,
    find_closest,
)

from ._trace import matrix_from_function

from ._functional import (
    LinearFunction, jit_compile_linear, jit_compile,
    jacobian, gradient, custom_gradient, print_gradient,
    safe_mul,
    map_types, map_s2b, map_i2b, map_c2b, map_d2b, map_d2c, map_c2d,
    broadcast,
    iterate,
    identity,
    trace_check,
    map_ as map,
    when_available,
    perf_counter,
)

from ._optimize import solve_linear, solve_nonlinear, minimize, Solve, SolveInfo, ConvergenceException, NotConverged, Diverged, SolveTape, factor_ilu

from ._deprecated import clip_length, cross_product, cross_product as cross, rotate_vector, rotation_matrix, length, length as vec_length, vec_squared

import sys as _sys
math = _sys.modules[__name__]
&#34;&#34;&#34;Convenience alias for the module `phiml.math`.
This way, you can import the module and contained items in one line.
```
from phiml.math import math, Tensor, wrap, extrapolation, l2_loss
```&#34;&#34;&#34;

PI = 3.14159265358979323846
&#34;&#34;&#34;Value of π to double precision &#34;&#34;&#34;
pi = PI  # intentionally undocumented, use PI instead. Exists only as an anlog to numpy.pi

INF = float(&#34;inf&#34;)
&#34;&#34;&#34; Floating-point representation of positive infinity. &#34;&#34;&#34;
inf = INF  # intentionally undocumented, use INF instead. Exists only as an anlog to numpy.inf


NAN = float(&#34;nan&#34;)
&#34;&#34;&#34; Floating-point representation of NaN (not a number). &#34;&#34;&#34;
nan = NAN  # intentionally undocumented, use NAN instead. Exists only as an anlog to numpy.nan

NUMPY = NUMPY  # to show up in pdoc
&#34;&#34;&#34;Default backend for NumPy arrays and SciPy objects.&#34;&#34;&#34;

f = f
&#34;&#34;&#34;
Automatic mapper for broadcast string formatting of tensors, resulting in tensors of strings.
Used with the special `-f-` syntax.

Examples:
    &gt;&gt;&gt; from phiml.math import f
    &gt;&gt;&gt; -f-f&#39;String containing {tensor1} and {tensor2:.1f}&#39;
    # Result is a str tensor containing all dims of tensor1 and tensor2
&#34;&#34;&#34;

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]

__pdoc__ = {
    &#39;Extrapolation&#39;: False,
    &#39;Shape.__init__&#39;: False,
    &#39;SolveInfo.__init__&#39;: False,
    &#39;TensorDim.__init__&#39;: False,
    &#39;ConvergenceException.__init__&#39;: False,
    &#39;Diverged.__init__&#39;: False,
    &#39;NotConverged.__init__&#39;: False,
    &#39;LinearFunction.__init__&#39;: False,
}</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phiml.math.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>||x||<sub>1</sub></em>.
Complex <code>x</code> result in matching precision float values.</p>
<p><em>Note</em>: The gradient of this operation is undefined for <em>x=0</em>.
TensorFlow and PyTorch return 0 while Jax returns 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Absolute value of <code>x</code> of same type as <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.abs_square"><code class="name flex">
<span>def <span class="ident">abs_square</span></span>(<span>complex_values: Union[phiml.math._tensors.Tensor, complex]) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Squared magnitude of complex values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>complex_values</code></strong></dt>
<dd>complex <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dt>
<dd>real valued magnitude squared</dd>
</dl></div>
</dd>
<dt id="phiml.math.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>boolean_value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Tests whether all entries of <code>boolean_tensor</code> are <code>True</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boolean_value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.all_available"><code class="name flex">
<span>def <span class="ident">all_available</span></span>(<span>*values) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if all tensors contained in the given <code>values</code> are currently known and can be read.
Placeholder tensors used to trace functions for just-in-time compilation or matrix construction are considered not available, even when they hold example values like with PyTorch's JIT.</p>
<p>Tensors are not available during <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>, <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code> or while using TensorFlow's legacy graph mode.</p>
<p>Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.</p>
<p>This can be used instead of the native checks</p>
<ul>
<li>PyTorch: <code>torch._C._get_tracing_state()</code></li>
<li>TensorFlow: <code>tf.executing_eagerly()</code></li>
<li>Jax: <code>isinstance(x, jax.core.Tracer)</code></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>True</code> if no value is a placeholder or being traced, <code>False</code> otherwise.</p></div>
</dd>
<dt id="phiml.math.always_close"><code class="name flex">
<span>def <span class="ident">always_close</span></span>(<span>t1: Union[numbers.Number, phiml.math._tensors.Tensor, bool], t2: Union[numbers.Number, phiml.math._tensors.Tensor, bool], rel_tolerance=1e-05, abs_tolerance=0, equal_nan=False) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether two tensors are guaranteed to be <code><a title="phiml.math.close" href="#phiml.math.close">close()</a></code> in all values.
Unlike <code><a title="phiml.math.close" href="#phiml.math.close">close()</a></code>, this function can be used with JIT compilation and with tensors of incompatible shapes.
Incompatible tensors are never close.</p>
<p>If one of the given tensors is being traced, the tensors are only equal if they reference the same native tensor.
Otherwise, an element-wise equality check is performed.</p>
<p>See Also:
<code><a title="phiml.math.close" href="#phiml.math.close">close()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>t1</code></strong></dt>
<dd>First tensor or number to compare.</dd>
<dt><strong><code>t2</code></strong></dt>
<dd>Second tensor or number to compare.</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>Relative tolerance, only used if neither tensor is traced.</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>Absolute tolerance, only used if neither tensor is traced.</dd>
<dt><strong><code>equal_nan</code></strong></dt>
<dd>If <code>True</code>, tensors are considered close if they are NaN in the same places.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code></p></div>
</dd>
<dt id="phiml.math.angle"><code class="name flex">
<span>def <span class="ident">angle</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the angle of a complex number.
This is equal to <em>atan(Im/Re)</em> for most values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Angle of complex number in radians.</p></div>
</dd>
<dt id="phiml.math.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>boolean_value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Tests whether any entry of <code>boolean_tensor</code> is <code>True</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boolean_value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.arange"><code class="name flex">
<span>def <span class="ident">arange</span></span>(<span>dim: phiml.math._shape.Shape, start_or_stop: Optional[int] = None, stop: Optional[int] = None, step=1, backend=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns evenly spaced values between <code>start</code> and <code>stop</code>.
If only one limit is given, <code>0</code> is used for the start.</p>
<p>See Also:
<code><a title="phiml.math.range_tensor" href="#phiml.math.range_tensor">range_tensor()</a></code>, <code><a title="phiml.math.linspace" href="#phiml.math.linspace">linspace()</a></code>, <code><a title="phiml.math.meshgrid" href="#phiml.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension name and type as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.
The <code>size</code> of <code>dim</code> is interpreted as <code>stop</code> unless <code>start_or_stop</code> is specified.</dd>
<dt><strong><code>start_or_stop</code></strong></dt>
<dd>(Optional) <code>int</code>. Interpreted as <code>start</code> if <code>stop</code> is specified as well. Otherwise this is <code>stop</code>.</dd>
<dt><strong><code>stop</code></strong></dt>
<dd>(Optional) <code>int</code>. <code>stop</code> value.</dd>
<dt><strong><code>step</code></strong></dt>
<dd>Distance between values.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Backend to use for creating the tensor. If unspecified, uses the current default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.arccos"><code class="name flex">
<span>def <span class="ident">arccos</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>cos(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.
For real arguments, the result lies in the range [0, π].</p></div>
</dd>
<dt id="phiml.math.arccosh"><code class="name flex">
<span>def <span class="ident">arccosh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>cosh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.arcsin"><code class="name flex">
<span>def <span class="ident">arcsin</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>sin(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.
For real arguments, the result lies in the range [-π/2, π/2].</p></div>
</dd>
<dt id="phiml.math.arcsinh"><code class="name flex">
<span>def <span class="ident">arcsinh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>sinh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.arctan"><code class="name flex">
<span>def <span class="ident">arctan</span></span>(<span>x: ~TensorOrTree, divide_by=None) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>tan(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input. The single-argument <code><a title="phiml.math.arctan" href="#phiml.math.arctan">arctan()</a></code> function cannot output π/2 or -π/2 since tan(π/2) is infinite.</dd>
<dt><strong><code>divide_by</code></strong></dt>
<dd>If specified, computes <code>arctan(x/divide_by)</code> so that it can return π/2 and -π/2.
This is equivalent to the common <code>arctan2</code> function.</dd>
</dl></div>
</dd>
<dt id="phiml.math.arctanh"><code class="name flex">
<span>def <span class="ident">arctanh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>tanh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.argmax"><code class="name flex">
<span>def <span class="ident">argmax</span></span>(<span>x: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], index_dim=(indexᶜ=None))</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the maximum value along one or multiple dimensions and returns the corresponding index.</p>
<p>See Also:
<code><a title="phiml.math.argmin" href="#phiml.math.argmin">argmin()</a></code>, <code><a title="phiml.math.at_max" href="#phiml.math.at_max">at_max()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions along which the maximum should be determined. These are reduced in the operation.</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>Dimension listing the index components for multidimensional argmax.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Index tensor <code>idx</code>, such that <code>x[idx] = max(x)</code>.</p></div>
</dd>
<dt id="phiml.math.argmin"><code class="name flex">
<span>def <span class="ident">argmin</span></span>(<span>x: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], index_dim=(indexᶜ=None))</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the minimum value along one or multiple dimensions and returns the corresponding index.</p>
<p>See Also:
<code><a title="phiml.math.argmax" href="#phiml.math.argmax">argmax()</a></code>, <code><a title="phiml.math.at_min" href="#phiml.math.at_min">at_min()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions along which the minimum should be determined. These are reduced in the operation.</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>Dimension listing the index components for multidimensional argmin.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Index tensor <code>idx</code>, such that <code>x[idx] = min(x)</code>.</p></div>
</dd>
<dt id="phiml.math.as_extrapolation"><code class="name flex">
<span>def <span class="ident">as_extrapolation</span></span>(<span>obj) ‑> <a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates an <code>Extrapolation</code> from a descriptor object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>
<p>Extrapolation specification, one of the following:</p>
<ul>
<li><code>Extrapolation</code></li>
<li>Primitive name as <code>str</code>: periodic, zero, one, zero-gradient, symmetric, symmetric-gradient, antisymmetric, reflect, antireflect</li>
<li><code>dict</code> containing exactly the keys <code>'normal'</code> and <code>'tangential'</code></li>
<li><code>dict</code> mapping spatial dimension names to extrapolations</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>Extrapolation</code></p></div>
</dd>
<dt id="phiml.math.assert_close"><code class="name flex">
<span>def <span class="ident">assert_close</span></span>(<span>*values, rel_tolerance: float = 1e-05, abs_tolerance: float = 0, msg: str = '', verbose: bool = True, equal_nan=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks that all given tensors have equal values within the specified tolerance.
Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.</p>
<p>Does not check that the shapes match as long as they can be broadcast to a common shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors or native tensors or numbers or sequences of numbers.</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>Relative tolerance.</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>Absolute tolerance.</dd>
<dt><strong><code>msg</code></strong></dt>
<dd>Optional error message.</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to print conflicting values.</dd>
<dt><strong><code>equal_nan</code></strong></dt>
<dd>If <code>False</code>, <code>NaN</code> values will always trigger an assertion error.</dd>
</dl></div>
</dd>
<dt id="phiml.math.at_max"><code class="name flex">
<span>def <span class="ident">at_max</span></span>(<span>value, key: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Looks up the values of <code>value</code> at the positions where the maximum values in <code>key</code> are located along <code>dim</code>.</p>
<p>See Also:
<code><a title="phiml.math.at_min" href="#phiml.math.at_min">at_min()</a></code>, <code><a title="phiml.math.max" href="#phiml.math.max">max_()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Tensors or trees from which to lookup and return values. These tensors are indexed at the maximum index in `key´.
You can pass <code><a title="phiml.math.range" href="#phiml.math.range">arange()</a></code> (the type) to retrieve the picked indices.</dd>
<dt><strong><code>key</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing at least one dimension of <code>dim</code>. The maximum index of <code>key</code> is determined.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions along which to compute the maximum of <code>key</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The values of <code>other_tensors</code> at the positions where the maximum values in <code>value</code> are located along <code>dim</code>.</p></div>
</dd>
<dt id="phiml.math.at_max_neighbor"><code class="name flex">
<span>def <span class="ident">at_max_neighbor</span></span>(<span>values, key_grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, offsets=(0, 1), diagonal=True) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the min of neighboring values in <code>key_grid</code> along each dimension in <code>dims</code> and retrieves the corresponding values from <code>values</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Values to look up and return. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree structure.</dd>
<dt><strong><code>key_grid</code></strong></dt>
<dd>Values to compare.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which neighbors should be averaged.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding at the upper edges of <code>grid</code> along <code>dims'. If not</code>None<code>, the result <a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a> will have the same <a title="phiml.math.shape" href="#phiml.math.shape">shape()</a> as </code>grid`.</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Relative neighbor indices as <code>int</code>. <code>0</code> refers to self, negative values to earlier (left) neighbors and positive values to later (right) neighbors.</dd>
<dt><strong><code>diagonal</code></strong></dt>
<dd>If <code>True</code>, performs sequential reductions along each axis, determining the minimum value along each axis independently.
If the values of <code>key_grid</code> depend on <code>values</code> or their position in the grid, this can lead to undesired behavior.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tree or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> like values.</p></div>
</dd>
<dt id="phiml.math.at_min"><code class="name flex">
<span>def <span class="ident">at_min</span></span>(<span>value, key: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Looks up the values of <code>value</code> at the positions where the minimum values in <code>key</code> are located along <code>dim</code>.</p>
<p>See Also:
<code><a title="phiml.math.at_max" href="#phiml.math.at_max">at_max()</a></code>, <code><a title="phiml.math.min" href="#phiml.math.min">min_()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Tensors or trees from which to lookup and return values. These tensors are indexed at the minimum index in `key´.
You can pass <code><a title="phiml.math.range" href="#phiml.math.range">arange()</a></code> (the type) to retrieve the picked indices.</dd>
<dt><strong><code>key</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing at least one dimension of <code>dim</code>. The minimum index of <code>key</code> is determined.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions along which to compute the minimum of <code>key</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The values of <code>other_tensors</code> at the positions where the minimum values in <code>value</code> are located along <code>dim</code>.</p></div>
</dd>
<dt id="phiml.math.at_min_neighbor"><code class="name flex">
<span>def <span class="ident">at_min_neighbor</span></span>(<span>values, key_grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, offsets=(0, 1), diagonal=True) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the max of neighboring values in <code>key_grid</code> along each dimension in <code>dims</code> and retrieves the corresponding values from <code>values</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Values to look up and return.</dd>
<dt><strong><code>key_grid</code></strong></dt>
<dd>Values to compare.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which neighbors should be averaged.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding at the upper edges of <code>grid</code> along <code>dims'. If not</code>None<code>, the result <a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a> will have the same <a title="phiml.math.shape" href="#phiml.math.shape">shape()</a> as </code>grid`.</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Relative neighbor indices as <code>int</code>. <code>0</code> refers to self, negative values to earlier (left) neighbors and positive values to later (right) neighbors.</dd>
<dt><strong><code>diagonal</code></strong></dt>
<dd>If <code>True</code>, performs sequential reductions along each axis, determining the minimum value along each axis independently.
If the values of <code>key_grid</code> depend on <code>values</code> or their position in the grid, this can lead to undesired behavior.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tree or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> like values.</p></div>
</dd>
<dt id="phiml.math.b2i"><code class="name flex">
<span>def <span class="ident">b2i</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>batch</em> dimensions of <code>value</code> to <em>instance</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('<a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the batch dimensions of an existing <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or creates a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only batch dimensions.</p>
<p>Usage for filtering batch dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; batch_dims = batch(shape)
&gt;&gt;&gt; batch_dims = batch(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only batch dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; batch_shape = batch('undef', batch=2)
(batch=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> and <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> containing only dimensions of type batch.</p></div>
</dd>
<dt id="phiml.math.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>x, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], mask: phiml.math._tensors.Tensor, preserve_names=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Discards values <code>x.dim[i]</code> where <code>mask.dim[i]=False</code>.
All dimensions of <code>mask</code> that are not <code>dim</code> are treated as batch dimensions.</p>
<p>Alternative syntax: <code>x.dim[mask]</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: Slicing</li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.masked_select.html"><code>masked_select</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask"><code>tf.boolean_mask</code></a></li>
<li>Jax: Slicing</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.Sliceable" href="magic.html#phiml.math.magic.Sliceable">Sliceable</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension of <code>x</code> to along which to discard slices.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Boolean <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> marking which values to keep. Must have the dimension <code>dim</code> matching `x´.</dd>
<dt><strong><code>preserve_names</code></strong></dt>
<dd>This only supports uniform 1D slicing. Batched slicing will remove item names if incompatible.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Selected values of <code>x</code> as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with dimensions from <code>x</code> and <code>mask</code>.</p></div>
</dd>
<dt id="phiml.math.broadcast"><code class="name flex">
<span>def <span class="ident">broadcast</span></span>(<span>function=None, dims=&lt;function shape&gt;, range=builtins.range, unwrap_scalars=True, simplify=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function decorator for non-vectorized functions.
When passing <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> arguments to a broadcast function, the function is called once for each slice of the tensor.
How tensors are sliced is determined by <code>dims</code>.
Decorating a function with <code><a title="phiml.math.broadcast" href="#phiml.math.broadcast">broadcast()</a></code> is equal to passing the function to <code>phi.math.map()</code>.</p>
<p>See Also:
<code><a title="phiml.math.map" href="#phiml.math.map">map_()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to broadcast.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions which should be sliced.
<code>function</code> is called once for each element in <code>dims</code>, i.e. <code>dims.volume</code> times.
If <code>dims</code> is not specified, all dimensions from the <code><a title="phiml.math.magic.Sliceable" href="magic.html#phiml.math.magic.Sliceable">Sliceable</a></code> values in <code>args</code> and <code>kwargs</code> will be mapped.</dd>
<dt><strong><code>range</code></strong></dt>
<dd>Optional range function. Can be used to generate <code>tqdm</code> output by passing <code>trange</code>.</dd>
<dt><strong><code>unwrap_scalars</code></strong></dt>
<dd>If <code>True</code>, passes the contents of scalar <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>s instead of the tensor objects.</dd>
<dt><strong><code>simplify</code></strong></dt>
<dd>If <code>True</code>, reduces constant dims of output tensors that don't vary across broadcast slices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Broadcast function</p></div>
</dd>
<dt id="phiml.math.c2b"><code class="name flex">
<span>def <span class="ident">c2b</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>channel</em> dimensions of <code>value</code> to <em>batch</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.c2d"><code class="name flex">
<span>def <span class="ident">c2d</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>channel</em> dimensions of <code>value</code> to <em>dual</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>x: ~MagicType, dtype: Union[phiml.backend._dtype.DType, type]) ‑> ~OtherMagicType</span>
</code></dt>
<dd>
<div class="desc"><p>Casts <code>x</code> to a different data type.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="numpy.ndarray.astype"><code>x.astype()</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to"><code>x.to()</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/cast"><code>tf.cast</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html"><code>jax.numpy.array</code></a></li>
</ul>
<p>See Also:
<code><a title="phiml.math.to_float" href="#phiml.math.to_float">to_float()</a></code>, <code><a title="phiml.math.to_int32" href="#phiml.math.to_int32">to_int32()</a></code>, <code><a title="phiml.math.to_int64" href="#phiml.math.to_int64">to_int64()</a></code>, <code><a title="phiml.math.to_complex" href="#phiml.math.to_complex">to_complex()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>New data type as <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code>, e.g. <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a>(int, 16)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with data type <code><a title="phiml.math.dtype" href="#phiml.math.dtype">dtype()</a></code></p></div>
</dd>
<dt id="phiml.math.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>⌈x⌉</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.channel"><code class="name flex">
<span>def <span class="ident">channel</span></span>(<span>*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('<a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the channel dimensions of an existing <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or creates a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only channel dimensions.</p>
<p>Usage for filtering channel dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; channel_dims = channel(shape)
&gt;&gt;&gt; channel_dims = channel(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only channel dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; channel_shape = channel('undef', vector=2)
(vector=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> and <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> containing only dimensions of type channel.</p></div>
</dd>
<dt id="phiml.math.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False) ‑> phiml.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Choose backend for given <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or native tensor values.
Backends need to be registered to be available, e.g. via <code>init()</code> or <code><a title="phiml.math.use" href="#phiml.math.use">set_global_default_backend()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*values</code></strong></dt>
<dd>Sequence of <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>s, native tensors or constants.</dd>
<dt><strong><code>prefer_default</code></strong></dt>
<dd>Whether to always select the default backend if it can work with <code>values</code>, see <code>default_backend()</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The selected <code>phiml.math.backend.Backend</code></p></div>
</dd>
<dt id="phiml.math.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>x: phiml.math._tensors.Tensor, lower_limit: Union[float, phiml.math._tensors.Tensor] = 0, upper_limit: Union[float, phiml.math._tensors.Tensor, phiml.math._shape.Shape] = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Limits the values of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> <code>x</code> to lie between <code>lower_limit</code> and <code>upper_limit</code> (inclusive).</p></div>
</dd>
<dt id="phiml.math.clip_length"><code class="name flex">
<span>def <span class="ident">clip_length</span></span>(<span>vec: phiml.math._tensors.Tensor, min_len=0, max_len=1, vec_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, eps: Union[float, phiml.math._tensors.Tensor] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Clips the length of a vector to the interval <code>[min_len, max_len]</code> while keeping the direction.
Zero-vectors remain zero-vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>min_len</code></strong></dt>
<dd>Lower clipping threshold.</dd>
<dt><strong><code>max_len</code></strong></dt>
<dd>Upper clipping threshold.</dd>
<dt><strong><code>vec_dim</code></strong></dt>
<dd>Dimensions to compute the length over. By default, all channel dimensions are used to compute the vector length.</dd>
<dt><strong><code>eps</code></strong></dt>
<dd>Minimum vector length. Use to avoid <code>inf</code> gradients for zero-length vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with same shape as <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code>.</p></div>
</dd>
<dt id="phiml.math.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>*tensors, rel_tolerance: Union[float, phiml.math._tensors.Tensor] = 1e-05, abs_tolerance: Union[float, phiml.math._tensors.Tensor] = 0, equal_nan=False, reduce=&lt;function shape&gt;) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all tensors have equal values within the specified tolerance.</p>
<p>Does not check that the shapes exactly match but if shapes are incompatible, returns <code>False</code>.
Unlike with <code><a title="phiml.math.always_close" href="#phiml.math.always_close">always_close()</a></code>, all shapes must be compatible and tensors with different shapes are reshaped before comparing.</p>
<p>See Also:
<code><a title="phiml.math.always_close" href="#phiml.math.always_close">always_close()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*tensors</code></strong></dt>
<dd>At least two
<code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tensor-like objects or <code>None</code>.
The shapes of all tensors must be compatible but not all tensors must have all dimensions.
If any argument is <code>None</code>, returns <code>True</code> only if all are <code>None</code>.</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>Relative tolerance</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>Absolute tolerance</dd>
<dt><strong><code>equal_nan</code></strong></dt>
<dd>If <code>True</code>, tensors are considered close if they are NaN in the same places.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code>, whether all given tensors are equal to the first tensor within the specified tolerance.</p></div>
</dd>
<dt id="phiml.math.closest_grid_values"><code class="name flex">
<span>def <span class="ident">closest_grid_values</span></span>(<span>grid: phiml.math._tensors.Tensor, coordinates: phiml.math._tensors.Tensor, extrap: e_.Extrapolation, stack_dim_prefix='closest_', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the neighboring grid points in all directions and returns their values.
The result will have 2^d values for each vector in coordinates in d dimensions.</p>
<p>If <code>coordinates</code> does not have a channel dimension with item names, the spatial dims of <code>grid</code> will be used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid data. The grid is spanned by the spatial dimensions of the tensor</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>tensor with 1 channel dimension holding vectors pointing to locations in grid index space</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>stack_dim_prefix</code></strong></dt>
<dd>For each spatial dimension <code>dim</code>, stacks lower and upper closest values along dimension <code>stack_dim_prefix+dim</code>.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional information for the extrapolation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of shape (batch, coord_spatial, grid_spatial=(2, 2,&hellip;), grid_channel)</p></div>
</dd>
<dt id="phiml.math.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>values: Sequence[~PhiTreeNodeType], dim: Union[str, phiml.math._shape.Shape], /, expand_values=False, **kwargs) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates a sequence of <code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code> objects, e.g. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, along one dimension.
All values must have the same spatial, instance and channel dimensions and their sizes must be equal, except for <code>dim</code>.
Batch dimensions will be added as needed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tuple or list of <code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Concatenation dimension, must be present in all <code>values</code>.
The size along <code>dim</code> is determined from <code>values</code> and can be set to undefined (<code>None</code>).
Alternatively, a <code>str</code> of the form <code>'t-&gt;name:t'</code> can be specified, where <code>t</code> is on of <code>b d i s c</code> denoting the dimension type.
This first packs all dimensions of the input into a new dim with given name and type, then concatenates the values along this dim.</dd>
<dt><strong><code>expand_values</code></strong></dt>
<dd>If <code>True</code>, will first add missing dimensions to all values, not just batch dimensions.
This allows tensors with different dimensions to be concatenated.
The resulting tensor will have all dimensions that are present in <code>values</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Concatenated <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], 'b')
(bᵇ=20) 0.500 ± 0.500 (0e+00...1e+00)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; concat([vec(x=1, y=0), vec(z=2.)], 'vector')
(x=1.000, y=0.000, z=2.000) float64
</code></pre></div>
</dd>
<dt id="phiml.math.concat_shapes"><code class="name flex">
<span>def <span class="ident">concat_shapes</span></span>(<span>*shapes: Union[phiml.math._shape.Shape, Any]) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> listing the dimensions of all <code>shapes</code> in the given order.</p>
<p>See Also:
<code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shapes</code></strong></dt>
<dd>Shapes to concatenate. No two shapes must contain a dimension with the same name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Combined <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p></div>
</dd>
<dt id="phiml.math.conjugate"><code class="name flex">
<span>def <span class="ident">conjugate</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phiml.math.imag" href="#phiml.math.imag">imag()</a></code>, <code><a title="phiml.math.real" href="#phiml.math.real">real()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Real or complex <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex conjugate of <code>x</code> if <code>x</code> is complex, else <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.const_vec"><code class="name flex">
<span>def <span class="ident">const_vec</span></span>(<span>value: Union[float, phiml.math._tensors.Tensor], dim: Union[phiml.math._shape.Shape, tuple, list, str])</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a single-dimension tensor with all values equal to <code>value</code>.
<code>value</code> is not converted to the default backend, even when it is a Python primitive.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Value for filling the vector.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Either single-dimension non-spatial Shape or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> consisting of any number of spatial dimensions.
In the latter case, a new channel dimension named <code>'vector'</code> will be created from the spatial shape.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.contains"><code class="name flex">
<span>def <span class="ident">contains</span></span>(<span>values: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor, feature_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>For each query item, checks whether it is contained in <code>values</code>.</p>
<p>See Also:
<code><a title="phiml.math.count_occurrences" href="#phiml.math.count_occurrences">count_occurrences()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Data <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing all <code>feature_dims</code>.
All non-batch and dims not specified as <code>feature_dims</code> are flattened.</dd>
<dt><strong><code>query</code></strong></dt>
<dd>Items to count the occurrences of. Must contain all <code>feature_dims</code>.</dd>
<dt><strong><code>feature_dims</code></strong></dt>
<dd>One item is considered to be the set of all values along <code>feature_dims</code>.
The number of items in a tensor is given by all dims except <code>feature_dims</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Integer <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> matching <code>query</code> without <code>feature_dims</code>.</p></div>
</dd>
<dt id="phiml.math.convert"><code class="name flex">
<span>def <span class="ident">convert</span></span>(<span>x, backend: phiml.backend._backend.Backend = None, use_dlpack=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the native representation of a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> to the native format of <code>backend</code>.</p>
<p><em>Warning</em>: This operation breaks the automatic differentiation chain.</p>
<p>See Also:
<code>phiml.math.backend.convert()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to convert. If <code>x</code> is a <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>, its variable attributes are converted.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Target backend. If <code>None</code>, uses the current default backend, see <code>phiml.math.backend.default_backend()</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with native representation belonging to <code>backend</code>.</p></div>
</dd>
<dt id="phiml.math.convolve"><code class="name flex">
<span>def <span class="ident">convolve</span></span>(<span>value: phiml.math._tensors.Tensor, kernel: phiml.math._tensors.Tensor, extrapolation: e_.Extrapolation = None, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the convolution of <code>value</code> and <code>kernel</code> along the specified dims.</p>
<p>Dual dims of <code>kernel</code> are reduced against the corresponding primal dims of <code>value</code>.
All other primal dims of <code>value</code> are treated as batch.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> whose shape includes all spatial dimensions of <code>kernel</code>.</dd>
<dt><strong><code>kernel</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> used as convolutional filter.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Which dimensions to convolve over. Defaults to all spatial dims.</dd>
<dt><strong><code>extrapolation</code></strong></dt>
<dd>If not None, pads <code>value</code> so that the result has the same shape as <code>value</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with all non-reduced dims of <code>value</code> and additional non-dual dims from <code>kernel</code>.</p></div>
</dd>
<dt id="phiml.math.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>value: phiml.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Copies the data buffer and encapsulating <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to be copied.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.copy_with"><code class="name flex">
<span>def <span class="ident">copy_with</span></span>(<span>obj: ~PhiTreeNodeType, **updates) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a copy of the given <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> with updated values as specified in <code>updates</code>.</p>
<p>If <code>obj</code> overrides <code>__with_attrs__</code>, the copy will be created via that specific implementation.
Otherwise, the <code><a title="phiml.math.copy" href="#phiml.math.copy">copy()</a></code> module and <code>setattr</code> will be used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code></dd>
<dt><strong><code>**updates</code></strong></dt>
<dd>Values to be replaced.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>obj</code> with updated values.</p></div>
</dd>
<dt id="phiml.math.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>cos(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.cosh"><code class="name flex">
<span>def <span class="ident">cosh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>cosh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.count_intersections"><code class="name flex">
<span>def <span class="ident">count_intersections</span></span>(<span>values: phiml.math._tensors.Tensor, arg_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], list_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function instance&gt;, feature_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Counts the number of elements that are part of each pair of lists.</p>
<h2 id="args">Args</h2>
<dl>
<dt>values:</dt>
<dt><strong><code>arg_dims</code></strong></dt>
<dd>Dims enumerating the input lists.</dd>
<dt><strong><code>list_dims</code></strong></dt>
<dd>Dims listing the elements.</dd>
<dt><strong><code>feature_dims</code></strong></dt>
<dd>Vector dims of one element. Elements are equal if all values along <code>feature_dims</code> are equal.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p></div>
</dd>
<dt id="phiml.math.count_occurrences"><code class="name flex">
<span>def <span class="ident">count_occurrences</span></span>(<span>values: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor, feature_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>For each query item, counts how often this value occurs in <code>values</code>.</p>
<p>See Also:
<code><a title="phiml.math.contains" href="#phiml.math.contains">contains()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Data <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing all <code>feature_dims</code>.
All non-batch and dims not specified as <code>feature_dims</code> are flattened.</dd>
<dt><strong><code>query</code></strong></dt>
<dd>Items to count the occurrences of. Must contain all <code>feature_dims</code>.</dd>
<dt><strong><code>feature_dims</code></strong></dt>
<dd>One item is considered to be the set of all values along <code>feature_dims</code>.
The number of items in a tensor is given by all dims except <code>feature_dims</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Integer <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> matching <code>query</code> without <code>feature_dims</code>.</p></div>
</dd>
<dt id="phiml.math.cpack"><code class="name flex">
<span>def <span class="ident">cpack</span></span>(<span>value, packed_dim: Union[str, phiml.math._shape.Shape], pos: Optional[int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Short for `pack_dims(&hellip;, dims=channel)</p></div>
</dd>
<dt id="phiml.math.cross"><code class="name flex">
<span>def <span class="ident">cross</span></span>(<span>vec1: phiml.math._tensors.Tensor, vec2: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the cross product of two vectors in 2D.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec1</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
<dt><strong><code>vec2</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.cross_product"><code class="name flex">
<span>def <span class="ident">cross_product</span></span>(<span>vec1: phiml.math._tensors.Tensor, vec2: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the cross product of two vectors in 2D.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec1</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
<dt><strong><code>vec2</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.cumulative_sum"><code class="name flex">
<span>def <span class="ident">cumulative_sum</span></span>(<span>x: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], include_0=False, include_sum=True, index_dim: Union[str, phiml.math._shape.Shape, None] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a cumulative sum of <code>x</code> along <code>dim</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html"><code>cumsum</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html"><code>cumsum</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/math/cumsum"><code>cumsum</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html"><code>cumsum</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension along which to sum, as <code>str</code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>. If multiple dims are passed, <code>x</code> the cumulative sum will be computed on the flattened array.</dd>
<dt><strong><code>include_0</code></strong></dt>
<dd>If <code>True</code>, adds a 0 to the result before the first value.</dd>
<dt><strong><code>include_sum</code></strong></dt>
<dd>If <code>False</code>, the total sum will be sliced off the result.</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>If given, adds an index dimension for <code>dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the same shape as <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.custom_gradient"><code class="name flex">
<span>def <span class="ident">custom_gradient</span></span>(<span>f: Callable, gradient: Callable, auxiliary_args: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function based on <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> that uses a custom gradient for the backpropagation pass.</p>
<p><em>Warning</em> This method can lead to memory leaks if the gradient function is not called.
Make sure to pass tensors without gradients if the gradient is not required, see <code><a title="phiml.math.stop_gradient" href="#phiml.math.stop_gradient">stop_gradient()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Forward function mapping <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> arguments <code>x</code> to a single <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> output or sequence of tensors <code>y</code>.</dd>
<dt><strong><code>gradient</code></strong></dt>
<dd>Function to compute the vector-Jacobian product for backpropagation.
Will be called as <code>gradient(input_dict, *y, *dy) -&gt; output_dict</code> where <code>input_dict</code> contains all named arguments passed to the forward function
and <code>output_dict</code> contains only those parameters for which a gradient is defined.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Comma-separated parameter names of arguments that are not relevant to backpropagation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>. However, the returned function does not support keyword arguments.</p></div>
</dd>
<dt id="phiml.math.d2i"><code class="name flex">
<span>def <span class="ident">d2i</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>dual</em> dimensions of <code>value</code> to <em>instance</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.d2s"><code class="name flex">
<span>def <span class="ident">d2s</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>dual</em> dimensions of <code>value</code> to <em>spatial</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.degrees_to_radians"><code class="name flex">
<span>def <span class="ident">degrees_to_radians</span></span>(<span>deg: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Convert degrees to radians.</p></div>
</dd>
<dt id="phiml.math.dense"><code class="name flex">
<span>def <span class="ident">dense</span></span>(<span>x: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a sparse tensor representation to an equivalent dense one in which all values are explicitly stored contiguously in memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Any <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Python primitives like <code>float</code>, <code>int</code> or <code>bool</code> will be converted to <code>Tensors</code> in the process.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dense tensor.</p></div>
</dd>
<dt id="phiml.math.dim_mask"><code class="name flex">
<span>def <span class="ident">dim_mask</span></span>(<span>all_dims: Union[phiml.math._shape.Shape, tuple, list], dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], mask_dim=(vectorᶜ=None)) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a masked vector with 1 elements for <code>dims</code> and 0 for all other dimensions in <code>all_dims</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>all_dims</code></strong></dt>
<dd>All dimensions for which the vector should have an entry.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions marked as 1.</dd>
<dt><strong><code>mask_dim</code></strong></dt>
<dd>Dimension of the masked vector. Item names are assigned automatically.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>x: phiml.math._tensors.Tensor, x_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], y: phiml.math._tensors.Tensor, y_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None]) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the dot product along the specified dimensions.
Contracts <code>x_dims</code> with <code>y_dims</code> by first multiplying the elements and then summing them up.</p>
<p>For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.</p>
<p>The function replaces the traditional <code><a title="phiml.math.dot" href="#phiml.math.dot">dot()</a></code> / <code>tensordot</code> / <code>matmul</code> / <code>einsum</code> functions.</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html"><code>numpy.tensordot</code></a>, <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>numpy.einsum</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot"><code>torch.tensordot</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.einsum.html"><code>torch.einsum</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/tensordot"><code>tf.tensordot</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/einsum"><code>tf.einsum</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html"><code>jax.numpy.tensordot</code></a>, <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html"><code>jax.numpy.einsum</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>First <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>x_dims</code></strong></dt>
<dd>Dimensions of <code>x</code> to reduce against <code>y</code></dd>
<dt><strong><code>y</code></strong></dt>
<dd>Second <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>y_dims</code></strong></dt>
<dd>Dimensions of <code>y</code> to reduce against <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dot product as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p></div>
</dd>
<dt id="phiml.math.downsample2x"><code class="name flex">
<span>def <span class="ident">downsample2x</span></span>(<span>grid: phiml.math._tensors.Tensor, padding: <a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a> = zero-gradient, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to half the number of spatial sample points per dimension.
The grid values at the new points are determined via mean (linear interpolation).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation. Used to insert an additional value for odd spatial dims</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which down-sampling is applied. If None, down-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>half-size grid</p></div>
</dd>
<dt id="phiml.math.dpack"><code class="name flex">
<span>def <span class="ident">dpack</span></span>(<span>value, packed_dim: Union[str, phiml.math._shape.Shape], pos: Optional[int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Short for `pack_dims(&hellip;, dims=dual)</p></div>
</dd>
<dt id="phiml.math.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>x) ‑> phiml.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the data type of <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code></p></div>
</dd>
<dt id="phiml.math.dual"><code class="name flex">
<span>def <span class="ident">dual</span></span>(<span>*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('<a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the dual dimensions of an existing <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or creates a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only dual dimensions.</p>
<p>Dual dimensions are assigned the prefix <code>~</code> to distinguish them from regular dimensions.
This way, a regular and dual dimension of the same name can exist in one <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p>
<p>Dual dimensions represent the input space and are typically only present on matrices or higher-order matrices.
Dual dimensions behave like batch dimensions in regular operations, if supported.
During matrix multiplication, they are matched against their regular counterparts by name (ignoring the <code>~</code> prefix).</p>
<p>Usage for filtering dual dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dual_dims = dual(shape)
&gt;&gt;&gt; dual_dims = dual(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only dual dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dual('undef', points=2)
(~undefᵈ=None, ~pointsᵈ=2)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> and <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> containing only dimensions of type dual.</p></div>
</dd>
<dt id="phiml.math.eigenvalues"><code class="name flex">
<span>def <span class="ident">eigenvalues</span></span>(<span>matrix: phiml.math._tensors.Tensor, eigen_dim=(eigenvaluesᶜ=None))</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the eigenvalues of a square matrix.
The matrix columns are listed along dual dimensions and the rows are listed along the corresponding non-dual dimensions.
Row dims are matched by name if possible, else all primal dims are used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Square matrix. Must have at least one dual dim and corresponding non-dual dim.</dd>
<dt><strong><code>eigen_dim</code></strong></dt>
<dd>Dimension along which eigenvalues should be listed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> listing the eigenvalues along <code>eigen_dim</code>.</p></div>
</dd>
<dt id="phiml.math.enable_debug_checks"><code class="name flex">
<span>def <span class="ident">enable_debug_checks</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Once called, additional type checks are enabled.
This may result in a noticeable drop in performance.</p></div>
</dd>
<dt id="phiml.math.equal"><code class="name flex">
<span>def <span class="ident">equal</span></span>(<span>*objects, equal_nan=False) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all objects are equal.</p>
<p>See Also:
<code><a title="phiml.math.close" href="#phiml.math.close">close()</a></code>, <code><a title="phiml.math.always_close" href="#phiml.math.always_close">always_close()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*objects</code></strong></dt>
<dd>Objects to compare. Can be tensors or other objects or <code>None</code></dd>
<dt><strong><code>equal_nan</code></strong></dt>
<dd>If all objects are tensor-like, whether to count <code>NaN</code> values as equal.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code>, whether all given objects are equal to the first one.</p></div>
</dd>
<dt id="phiml.math.erf"><code class="name flex">
<span>def <span class="ident">erf</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the error function <em>erf(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>exp(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>value, *dims: Union[str, phiml.math._shape.Shape], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds dimensions to a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tensor-like object by implicitly repeating the tensor values along the new dimensions.
If <code>value</code> already contains any of the new dimensions, a size and type check is performed for these instead.</p>
<p>If any of <code>dims</code> varies along a dimension that is present neither in <code>value</code> nor on <code>dims</code>, it will also be added to <code>value</code>.</p>
<p>This function replaces the usual <code>tile</code> / <code>repeat</code> functions of
<a href="https://numpy.org/doc/stable/reference/generated/numpy.tile.html">NumPy</a>,
<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat">PyTorch</a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/tile">TensorFlow</a> and
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html">Jax</a>.</p>
<p>Additionally, it replaces the traditional <code>unsqueeze</code> / <code>expand_dims</code> functions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>
For tree nodes, expands all value attributes by <code>dims</code> or the first variable attribute if no value attributes are set.</dd>
<dt><strong><code>*dims</code></strong></dt>
<dd>Dimensions to be added as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.factor_ilu"><code class="name flex">
<span>def <span class="ident">factor_ilu</span></span>(<span>matrix: phiml.math._tensors.Tensor, iterations: int, safe=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Incomplete LU factorization for dense or sparse matrices.</p>
<p>For sparse matrices, keeps the sparsity pattern of <code>matrix</code>.
L and U will be trimmed to the respective areas, i.e. stored upper elements in L will be dropped,
unless this would lead to varying numbers of stored elements along a batch dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Dense or sparse matrix to factor.
Currently, compressed sparse matrices are decompressed before running the ILU algorithm.</dd>
<dt><strong><code>iterations</code></strong></dt>
<dd>(Optional) Number of fixed-point iterations to perform.
If not given, will be automatically determined from matrix size and sparsity.</dd>
<dt><strong><code>safe</code></strong></dt>
<dd>If <code>False</code> (default), only matrices with a rank deficiency of up to 1 can be factored as all values of L and U are uniquely determined.
For matrices with higher rank deficiencies, the result includes <code>NaN</code> values.
If <code>True</code>, the algorithm runs slightly slower but can factor highly rank-deficient matrices as well.
However, then L is undeterdetermined and unused values of L are set to 0.
Rank deficiencies of 1 occur frequently in periodic settings but higher ones are rare.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>L</code></dt>
<dd>Lower-triangular matrix as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with all diagonal elements equal to 1.</dd>
<dt><code>U</code></dt>
<dd>Upper-triangular matrix as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; matrix = wrap([[-2, 1, 0],
&gt;&gt;&gt;                [1, -2, 1],
&gt;&gt;&gt;                [0, 1, -2]], channel('row'), dual('col'))
&gt;&gt;&gt; L, U = math.factor_ilu(matrix)
&gt;&gt;&gt; math.print(L)
row=0      1.          0.          0.         along ~col
row=1     -0.5         1.          0.         along ~col
row=2      0.         -0.6666667   1.         along ~col
&gt;&gt;&gt; math.print(L @ U, &quot;L @ U&quot;)
            L @ U
row=0     -2.   1.   0.  along ~col
row=1      1.  -2.   1.  along ~col
row=2      0.   1.  -2.  along ~col
</code></pre></div>
</dd>
<dt id="phiml.math.factorial"><code class="name flex">
<span>def <span class="ident">factorial</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>factorial(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.
For floating-point numbers computes the continuous factorial using the gamma function.
For integer numbers computes the exact factorial and returns the same integer type.
However, this results in integer overflow for inputs larger than 12 (int32) or 19 (int64).</p></div>
</dd>
<dt id="phiml.math.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>x: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a fast Fourier transform (FFT) on all spatial dimensions of x.</p>
<p>The inverse operation is <code><a title="phiml.math.ifft" href="#phiml.math.ifft">ifft()</a></code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html"><code>np.fft.fft</code></a>,
<a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html"><code>numpy.fft.fft2</code></a>,
<a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html"><code>numpy.fft.fftn</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/fft.html"><code>torch.fft.fft</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft"><code>tf.signal.fft</code></a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft2d"><code>tf.signal.fft2d</code></a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft3d"><code>tf.signal.fft3d</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html"><code>jax.numpy.fft.fft</code></a>,
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html"><code>jax.numpy.fft.fft2</code></a>
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html"><code>jax.numpy.fft.fft</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Uniform complex or float <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with at least one spatial dimension.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to perform the FFT.
If <code>None</code>, performs the FFT along all spatial dimensions of <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><em>Ƒ(x)</em> as complex <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.fftfreq"><code class="name flex">
<span>def <span class="ident">fftfreq</span></span>(<span>resolution: phiml.math._shape.Shape, dx: Union[float, phiml.math._tensors.Tensor] = 1, dtype: phiml.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the discrete Fourier transform sample frequencies.
These are the frequencies corresponding to the components of the result of <code>math.fft</code> on a tensor of shape <code>resolution</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>Grid resolution measured in cells</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Distance between sampling points in real space.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type of the returned tensor (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> holding the frequencies of the corresponding values computed by math.fft</p></div>
</dd>
<dt id="phiml.math.find_closest"><code class="name flex">
<span>def <span class="ident">find_closest</span></span>(<span>vectors: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor, method='kd', index_dim=(indexᶜ=None))</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the closest vector to <code>query</code> from <code>vectors</code>.
This is implemented using a k-d tree built from <code>vectors</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vectors</code></strong></dt>
<dd>Points to find.</dd>
<dt><strong><code>query</code></strong></dt>
<dd>Target locations.</dd>
<dt><strong><code>method</code></strong></dt>
<dd>
<p>One of the following:</p>
<ul>
<li><code>'dense'</code>: compute the pair-wise distances between all vectors and query points, then return the index of the smallest distance for each query point.</li>
<li><code>'kd'</code> (default): Build a k-d tree from <code>vectors</code> and use it to query all points in <code>query</code>. The tree will be cached if this call is jit-compiled and <code>vectors</code> is constant.</li>
</ul>
</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>Dimension along which components should be listed as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.
Pass <code>None</code> to get 1D indices as scalars.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Index tensor <code>idx</code> so that the closest points to <code>query</code> are <code>vectors[idx]</code>.</p></div>
</dd>
<dt id="phiml.math.find_differences"><code class="name flex">
<span>def <span class="ident">find_differences</span></span>(<span>tree1, tree2, compare_tensors_by_id=False, attr_type=&lt;function value_attributes&gt;, tensor_equality=None) ‑> Sequence[Tuple[str, str, Any, Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Compares <code>tree1</code> and <code>tree2</code> and returns all differences in the form <code>(difference_description: str, variable_identifier: str, value1, value2)</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tree1</code></strong></dt>
<dd>Nested tree or leaf</dd>
<dt><strong><code>tree2</code></strong></dt>
<dd>Nested tree or leaf</dd>
<dt><strong><code>compare_tensors_by_id</code></strong></dt>
<dd>Whether <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> objects should be compared by identity or values.</dd>
<dt><strong><code>attr_type</code></strong></dt>
<dd>What attributes to compare, either <code>value_attributes</code> or <code>variable_attributes</code>.</dd>
<dt><strong><code>tensor_equality</code></strong></dt>
<dd>Function that compares two tensors for equality. <code>None</code> defaults to <code><a title="phiml.math.equal" href="#phiml.math.equal">equal()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of differences, each represented as a <code>tuple</code>.</p></div>
</dd>
<dt id="phiml.math.finite_fill"><code class="name flex">
<span>def <span class="ident">finite_fill</span></span>(<span>values: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, distance: int = 1, diagonal: bool = True, padding=zero-gradient, padding_kwargs: dict = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Fills non-finite (NaN, inf, -inf) values from nearby finite values.
Extrapolates the finite values of <code>values</code> for <code>distance</code> steps along <code>dims</code>.
Where multiple finite values could fill an invalid value, the average is computed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Floating-point <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>. All non-numeric values (<code>NaN</code>, <code>inf</code>, <code>-inf</code>) are interpreted as invalid.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to fill invalid values from finite ones.</dd>
<dt><strong><code>distance</code></strong></dt>
<dd>Number of extrapolation steps, each extrapolating one cell out.</dd>
<dt><strong><code>diagonal</code></strong></dt>
<dd>Whether to extrapolate values to their diagonal neighbors per step.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation of <code>values</code>. Determines whether to extrapolate from the edges as well.</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of same shape as <code>values</code>.</p></div>
</dd>
<dt id="phiml.math.finite_max"><code class="name flex">
<span>def <span class="ident">finite_max</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, default: Union[complex, float] = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the maximum along <code>dim</code> ignoring all non-finite values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.finite_mean"><code class="name flex">
<span>def <span class="ident">finite_mean</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, default: Union[complex, float] = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the mean value of all finite values in <code>value</code> along <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.finite_min"><code class="name flex">
<span>def <span class="ident">finite_min</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, default: Union[complex, float] = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the minimum along <code>dim</code> ignoring all non-finite values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.finite_sum"><code class="name flex">
<span>def <span class="ident">finite_sum</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, default: Union[complex, float] = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Sums all finite values in <code>value</code> along <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>value, flat_dim: phiml.math._shape.Shape = (flatⁱ=None), flatten_batch=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the same values as <code>value</code> but only a single dimension <code>flat_dim</code>.
The order of the values in memory is not changed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
If a non-<code><a title="phiml.math.magic.Shaped" href="magic.html#phiml.math.magic.Shaped">Shaped</a></code> object or one with an empty <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> is passed, it is returned without alteration.</dd>
<dt><strong><code>flat_dim</code></strong></dt>
<dd>Dimension name and type as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object. The size is ignored.</dd>
<dt><strong><code>flatten_batch</code></strong></dt>
<dd>Whether to flatten batch dimensions as well.
If <code>False</code>, batch dimensions are kept, only onn-batch dimensions are flattened.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; flatten(math.zeros(spatial(x=4, y=3)))
(flatⁱ=12) const 0.0
</code></pre></div>
</dd>
<dt id="phiml.math.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>⌊x⌋</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.fourier_laplace"><code class="name flex">
<span>def <span class="ident">fourier_laplace</span></span>(<span>grid: phiml.math._tensors.Tensor, dx: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape, float, list, tuple], times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the spatial laplace operator to the given tensor with periodic boundary conditions.</p>
<p><em>Note:</em> The results of <code><a title="phiml.math.fourier_laplace" href="#phiml.math.fourier_laplace">fourier_laplace()</a></code> and <code><a title="phiml.math.laplace" href="#phiml.math.laplace">laplace()</a></code> are close but not identical.</p>
<p>This implementation computes the laplace operator in Fourier space.
The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>tensor, assumed to have periodic boundary conditions</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>distance between grid points, tensor-like, scalar or vector</dd>
<dt><strong><code>times</code></strong></dt>
<dd>number of times the laplace operator is applied. The computational cost is independent of this parameter.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor:</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple:</dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape as <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code></p></div>
</dd>
<dt id="phiml.math.fourier_poisson"><code class="name flex">
<span>def <span class="ident">fourier_poisson</span></span>(<span>grid: phiml.math._tensors.Tensor, dx: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape, float, list, tuple], times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse operation to <code><a title="phiml.math.fourier_laplace" href="#phiml.math.fourier_laplace">fourier_laplace()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<p>Returns:</p></div>
</dd>
<dt id="phiml.math.frequency_loss"><code class="name flex">
<span>def <span class="ident">frequency_loss</span></span>(<span>x, frequency_falloff: float = 100, threshold=1e-05, ignore_mean=False, n=2) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Penalizes the squared <code>values</code> in frequency (Fourier) space.
Lower frequencies are weighted more strongly then higher frequencies, depending on <code>frequency_falloff</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> Values to penalize, typically <code>actual - target</code>.</dd>
<dt><strong><code>frequency_falloff</code></strong></dt>
<dd>Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
<em>Note</em>: The total loss is not normalized. Varying the value will result in losses of different magnitudes.</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>Frequency amplitudes below this value are ignored.
Setting this to zero may cause infinities or NaN values during backpropagation.</dd>
<dt><strong><code>ignore_mean</code></strong></dt>
<dd>If <code>True</code>, does not penalize the mean value (frequency=0 component).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Scalar loss value</p></div>
</dd>
<dt id="phiml.math.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>dict_: dict, convert=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> from a serialized form.</p>
<p>See Also:
<code><a title="phiml.math.to_dict" href="#phiml.math.to_dict">to_dict()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dict_</code></strong></dt>
<dd>Serialized tensor properties.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>Whether to convert the data to the current backend format or keep it as a Numpy array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p></div>
</dd>
<dt id="phiml.math.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>values, indices: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = None, pref_index_dim='index')</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers the entries of <code>values</code> at positions described by <code>indices</code>.
All non-channel dimensions of <code>indices</code> that are part of <code>values</code> but not indexed are treated as batch dimensions.</p>
<p>See Also:
<code><a title="phiml.math.scatter" href="#phiml.math.scatter">scatter()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>phiml.math.matic.PhiTreeNode</code> containing values to gather.</dd>
<dt><strong><code>indices</code></strong></dt>
<dd><code>int</code> <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>. Multidimensional position references in <code>values</code>.
Must contain a single channel dimension for the index vector matching the number of dimensions to index.
This channel dimension should list the dimension names to index as item names unless explicitly specified as <code>dims</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>(Optional) Dimensions indexed by <code>indices</code>.
Alternatively, the dimensions can be specified as the item names of the channel dimension of <code>indices</code>.
If <code>None</code> and no index item names are specified, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).</dd>
<dt><strong><code>pref_index_dim</code></strong></dt>
<dd>In case <code>indices</code> has multiple channel dims, use this dim as the index, treating the others as batch.
Has no effect if <code>indices</code> only has one channel dim.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with combined batch dimensions, channel dimensions of <code>values</code> and spatial/instance dimensions of <code>indices</code>.</p></div>
</dd>
<dt id="phiml.math.get_format"><code class="name flex">
<span>def <span class="ident">get_format</span></span>(<span>x: phiml.math._tensors.Tensor) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the sparse storage format of a tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>One of <code>'coo'</code>, <code>'csr'</code>, <code>'csc'</code>, <code>'dense'</code>.</p></div>
</dd>
<dt id="phiml.math.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phiml.math.set_global_precision" href="#phiml.math.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
</dd>
<dt id="phiml.math.get_sparsity"><code class="name flex">
<span>def <span class="ident">get_sparsity</span></span>(<span>x: phiml.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Fraction of values currently stored on disk for the given <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> <code>x</code>.
For sparse tensors, this is <code>nnz / shape</code>.</p>
<p>This is a lower limit on the number of values that will need to be processed for operations involving <code>x</code>.
The actual number is often higher since many operations require data be laid out in a certain format.
In these cases, missing values, such as zeros, are filled in before the operation.</p>
<p>The following operations may return tensors whose values are only partially stored:</p>
<ul>
<li><code><a title="phiml.math.expand" href="#phiml.math.expand">expand()</a></code></li>
<li><code>phiml.math.pairwise_distance()</code> with <code>max_distance</code> set.</li>
<li>Tracers used in <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code></li>
<li>Stacking any of the above.</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The number of values that are actually stored on disk.
This does not include additional information, such as position information / indices.
For sparse matrices, this is equal to the number of nonzero values.</p></div>
</dd>
<dt id="phiml.math.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>f: Callable, wrt: str = None, get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the gradient of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p>
<p>Example:</p>
<pre><code class="language-python">def loss_function(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx = gradient(loss_function, 'x', get_output=False)(x, y)

(loss, prediction), (dx, dy) = gradient(loss_function,
                                        'x,y', get_output=True)(x, y)
</code></pre>
<p>Functional gradients are implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"><code>torch.autograd.grad</code></a> / <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward"><code>torch.autograd.backward</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code>tf.GradientTape</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad"><code>jax.grad</code></a></li>
</ul>
<p>When the gradient function is invoked, <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> must return a floating point <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> that returns the value of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>, auxiliary data and gradient of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> if <code>get_output=True</code>, else just the gradient of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p></div>
</dd>
<dt id="phiml.math.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>grid: phiml.math._tensors.Tensor, coordinates: phiml.math._tensors.Tensor, extrap: Union[ForwardRef('e_.Extrapolation'), float, str], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Samples values of <code>grid</code> at the locations referenced by <code>coordinates</code>.
Values lying in between sample points are determined via linear interpolation.</p>
<p>If <code>coordinates</code> has a channel dimension, its item names are used to determine the grid dimensions of <code>grid</code>.
Otherwise, the spatial dims of <code>grid</code> will be used.</p>
<p>For values outside the valid bounds of <code>grid</code> (<code>coord &lt; 0 or coord &gt; grid.shape - 1</code>), <code>extrap</code> is used to determine the neighboring grid values.
If the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.
In that case, values lying further outside will not be sampled according to the extrapolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Grid with at least one spatial dimension and no instance dimensions.</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Coordinates with a single channel dimension called <code>'vector'</code>.
The size of the <code>vector</code> dimension must match the number of spatial dimensions of <code>grid</code>.</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>Extrapolation used to determine the values of <code>grid</code> outside its valid bounds.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional information for the extrapolation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with channel dimensions of <code>grid</code>, spatial and instance dimensions of <code>coordinates</code> and combined batch dimensions.</p></div>
</dd>
<dt id="phiml.math.histogram"><code class="name flex">
<span>def <span class="ident">histogram</span></span>(<span>values: phiml.math._tensors.Tensor, bins: phiml.math._shape.Shape = (binsˢ=30), weights=1, same_bins: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute a histogram of a distribution of values.</p>
<p><em>Important Note:</em> In its current implementation, values outside the range of bins may or may not be added to the outermost bins.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> listing the values to be binned along spatial or instance dimensions.
`values´ may not contain channel or dual dimensions.</dd>
<dt><strong><code>bins</code></strong></dt>
<dd>Either <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> specifying the number of equally-spaced bins to use or bin edge positions as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with a spatial or instance dimension.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> assigning a weight to every value in <code>values</code> that will be added to the bin, default 1.</dd>
<dt><strong><code>same_bins</code></strong></dt>
<dd>Only used if <code>bins</code> is given as a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.
Use the same bin sizes and positions across these batch dimensions.
By default, bins will be chosen independently for each example.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>hist</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing all batch dimensions and the <code>bins</code> dimension with dtype matching <code>weights</code>.</dd>
<dt><code>bin_edges</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><code>bin_center</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl></div>
</dd>
<dt id="phiml.math.i2b"><code class="name flex">
<span>def <span class="ident">i2b</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>instance</em> dimensions of <code>value</code> to <em>batch</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.identity"><code class="name flex">
<span>def <span class="ident">identity</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Identity function for one argument.
Vararg functions cannot be transformed as the argument names are unknown.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Positional argument.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>x</code></p></div>
</dd>
<dt id="phiml.math.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>k: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse of <code><a title="phiml.math.fft" href="#phiml.math.fft">fft()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>Complex or float <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with at least one spatial dimension.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to perform the inverse FFT.
If <code>None</code>, performs the inverse FFT along all spatial dimensions of <code>k</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><em>Ƒ<sup>-1</sup>(k)</em> as complex <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the imaginary part of <code>x</code>.
If <code>x</code> does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.</p>
<p>See Also:
<code><a title="phiml.math.real" href="#phiml.math.real">real()</a></code>, <code><a title="phiml.math.conjugate" href="#phiml.math.conjugate">conjugate()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Imaginary component of <code>x</code> if <code>x</code> is complex, zeros otherwise.</p></div>
</dd>
<dt id="phiml.math.incomplete_gamma"><code class="name flex">
<span>def <span class="ident">incomplete_gamma</span></span>(<span>a: ~TensorOrTree, x: ~TensorOrTree, upper=False, regularized=True) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the incomplete gamma function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Positive parameter, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree.</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Non-negative argument, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree.</dd>
<dt><strong><code>upper</code></strong></dt>
<dd>Whether to complete the upper integral (x to infinity) or the lower integral (0 to x).</dd>
<dt><strong><code>regularized</code></strong></dt>
<dd>Whether the integral is divided by Γ(a).</dd>
</dl></div>
</dd>
<dt id="phiml.math.index_shift"><code class="name flex">
<span>def <span class="ident">index_shift</span></span>(<span>x: phiml.math._tensors.Tensor, offsets: Sequence[Union[int, phiml.math._tensors.Tensor]], padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None) ‑> List[phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns shifted versions of <code>x</code> according to <code>offsets</code> where each offset is an <code>int</code> vector indexing some dimensions of <code>x</code>.</p>
<p>See Also:
<code><a title="phiml.math.shift" href="#phiml.math.shift">shift()</a></code>, <code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input grid-like <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Sequence of offset vectors. Each offset is an <code>int</code> vector indexing some dimensions of <code>x</code>.
Offsets can have different subsets of the dimensions of <code>x</code>. Missing dimensions count as 0.
The value <code>0</code> can also be passed as a zero-shift.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding to be performed at the boundary so that the shifted versions have the same size as <code>x</code>.
Must be one of the following: <code>Extrapolation</code>, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or number for constant extrapolation, name of extrapolation as <code>str</code>.
Can be set to <code>None</code> to disable padding. Then the result tensors will be smaller than <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>list</code> of shifted tensors. The number of return tensors is equal to the number of <code>offsets</code>.</p></div>
</dd>
<dt id="phiml.math.instance"><code class="name flex">
<span>def <span class="ident">instance</span></span>(<span>*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('<a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the instance dimensions of an existing <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or creates a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only instance dimensions.</p>
<p>Usage for filtering instance dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; instance_dims = instance(shape)
&gt;&gt;&gt; instance_dims = instance(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only instance dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; instance_shape = instance('undef', points=2)
(points=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> and <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> containing only dimensions of type instance.</p></div>
</dd>
<dt id="phiml.math.ipack"><code class="name flex">
<span>def <span class="ident">ipack</span></span>(<span>value, packed_dim: Union[str, phiml.math._shape.Shape], pos: Optional[int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Short for `pack_dims(&hellip;, dims=instance)</p></div>
</dd>
<dt id="phiml.math.is_finite"><code class="name flex">
<span>def <span class="ident">is_finite</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> matching <code>x</code> with values <code>True</code> where <code>x</code> has a finite value and <code>False</code> otherwise.</p></div>
</dd>
<dt id="phiml.math.is_inf"><code class="name flex">
<span>def <span class="ident">is_inf</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> matching <code>x</code> with values <code>True</code> where <code>x</code> is <code>+inf</code> or <code>-inf</code> and <code>False</code> otherwise.</p></div>
</dd>
<dt id="phiml.math.is_nan"><code class="name flex">
<span>def <span class="ident">is_nan</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> matching <code>x</code> with values <code>True</code> where <code>x</code> is <code>NaN</code> and <code>False</code> otherwise.</p></div>
</dd>
<dt id="phiml.math.is_scalar"><code class="name flex">
<span>def <span class="ident">is_scalar</span></span>(<span>value) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether <code>value</code> has no dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or Python primitive or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code></p></div>
</dd>
<dt id="phiml.math.is_sparse"><code class="name flex">
<span>def <span class="ident">is_sparse</span></span>(<span>x: phiml.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether a tensor is represented in COO, CSR or CSC format.
If the tensor is neither sparse nor dense, this function raises an error.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to test.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>True</code> if <code>x</code> is sparse, <code>False</code> if <code>x</code> is dense.</p>
<h2 id="raises">Raises</h2>
<p><code>AssertionError</code> if <code>x</code> is neither sparse nor fully dense.</p></div>
</dd>
<dt id="phiml.math.iterate"><code class="name flex">
<span>def <span class="ident">iterate</span></span>(<span>map_function: Callable, iterations: Union[int, phiml.math._shape.Shape], *x0, f_kwargs: dict = None, range: Callable = builtins.range, measure: Callable = None, substeps: int = 1, **f_kwargs_)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeatedly call <code>function</code>, passing the previous output as the next input.</p>
<p>If the function outputs more values than the number of arguments in <code>x0</code>, only the first <code>len(x0)</code> ones are passed to <code>map_function</code>.
However, all outputs will be returned by <code><a title="phiml.math.iterate" href="#phiml.math.iterate">iterate()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>map_function</code></strong></dt>
<dd>Function to call. Must be callable as <code>f(x0, **f_kwargs)</code> and <code>f(f(x0, **f_kwargs), **f_kwargs)</code>.</dd>
<dt><strong><code>iterations</code></strong></dt>
<dd>Number of iterations as <code>int</code> or single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.
If <code>int</code>, returns the final output of <code>map_function</code>.
If <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, returns the trajectory (<code>x0</code> and all outputs of <code>map_function</code>), stacking the values along this dimension.</dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial positional arguments for <code>map_function</code>.
Values that are initially <code>None</code> are not stacked with the other values if <code>iterations</code> is a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>range</code></strong></dt>
<dd>Range function. Can be used to generate tqdm output by passing <code>trange</code>.</dd>
<dt><strong><code>measure</code></strong></dt>
<dd>Function without arguments to call at the start and end (and in between if <code>isinstance(iterations, <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>)</code>) calls to <code>map_function</code>.
The measure of each call to <code>map_function</code> is <code>measure()</code> after minus <code>measure()</code> before the call.</dd>
<dt><strong><code>substeps</code></strong></dt>
<dd>If &gt; 1, iterates the function multiple times for each recorded step.
The returned trajectories as well as measurements only record the large steps, not the sub-steps.
The <code><a title="phiml.math.range" href="#phiml.math.range">arange()</a></code> is also only used on large steps, not sub-steps.</dd>
<dt><strong><code>f_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code>map_function</code>.
These arguments can be of any type.</dd>
<dt><strong><code>f_kwargs_</code></strong></dt>
<dd>More keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>final_or_trajectory</code></dt>
<dd>Stacked trajectory or final output of <code>map_function</code>, depending on <code>iterations</code>.</dd>
<dt><code>measured</code></dt>
<dd>Only if <code>measure</code> was specified, returns the measured value or trajectory tensor.</dd>
</dl></div>
</dd>
<dt id="phiml.math.jacobian"><code class="name flex">
<span>def <span class="ident">jacobian</span></span>(<span>f: Callable, wrt: str = None, get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the Jacobian matrix of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.
For scalar functions, consider using <code><a title="phiml.math.gradient" href="#phiml.math.gradient">gradient()</a></code> instead.</p>
<p>Example:</p>
<pre><code class="language-python">def f(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx = jacobian(loss_function, wrt='x', get_output=False)(x, y)

(loss, prediction), (dx, dy) = jacobian(loss_function,
                                    wrt='x,y', get_output=True)(x, y)
</code></pre>
<p>Functional gradients are implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"><code>torch.autograd.grad</code></a> / <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward"><code>torch.autograd.backward</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code>tf.GradientTape</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad"><code>jax.grad</code></a></li>
</ul>
<p>When the gradient function is invoked, <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> must return a floating point <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> that returns the value of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>, auxiliary data and Jacobian of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> if <code>get_output=True</code>, else just the Jacobian of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p></div>
</dd>
<dt id="phiml.math.jit_compile"><code class="name flex">
<span>def <span class="ident">jit_compile</span></span>(<span>f: Callable = None, auxiliary_args: str = '', forget_traces: bool = None) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Compiles a graph based on the function <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.
The graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.</p>
<p>The traced function will compute the same result as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> but may run much faster.
Some checks may be disabled in the compiled function.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile
def my_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>Invoking the returned function may invoke re-tracing / re-compiling <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> after the first call if either</p>
<ul>
<li>it is called with a different number of arguments,</li>
<li>the tensor arguments have different dimension names or types (the dimension order also counts),</li>
<li>any <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> arguments require a different backend than previous invocations,</li>
<li><code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> positional arguments do not match in non-variable properties.</li>
</ul>
<p>Compilation is implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/jit.html"><code>torch.jit.trace</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/guide/function"><code>tf.function</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions"><code>jax.jit</code></a></li>
</ul>
<p>Jit-compilations cannot be nested, i.e. you cannot call <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code> while another function is being compiled.
An exception to this is <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code> which can be called from within a jit-compiled function.</p>
<p>See Also:
<code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be traced.
All positional arguments must be of type <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> returning a single <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Comma-separated parameter names of arguments that are not relevant to backpropagation.</dd>
<dt><strong><code>forget_traces</code></strong></dt>
<dd>If <code>True</code>, only remembers the most recent compiled instance of this function.
Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p></div>
</dd>
<dt id="phiml.math.jit_compile_linear"><code class="name flex">
<span>def <span class="ident">jit_compile_linear</span></span>(<span>f: Callable[[~X], ~Y] = None, auxiliary_args: str = None, forget_traces: bool = None) ‑> phiml.math._functional.LinearFunction[~X, ~Y]</span>
</code></dt>
<dd>
<div class="desc"><p>Compile an optimized representation of the linear function <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.
For backends that support sparse tensors, a sparse matrix will be constructed for <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile_linear
def my_linear_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>Unlike <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>, <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code> can be called during a regular jit compilation.</p>
<p>See Also:
<code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function that is linear in its positional arguments.
All positional arguments must be of type <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> and <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> must return a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Which parameters <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.</dd>
<dt><strong><code>forget_traces</code></strong></dt>
<dd>If <code>True</code>, only remembers the most recent compiled instance of this function.
Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.LinearFunction" href="#phiml.math.LinearFunction">LinearFunction</a></code> with similar signature and return values as <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p></div>
</dd>
<dt id="phiml.math.l1_loss"><code class="name flex">
<span>def <span class="ident">l1_loss</span></span>(<span>x, reduce: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub></em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or 0D or 1D native tensor.
For <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> objects, only value the sum over all value attributes is computed.</dd>
<dt><strong><code>reduce</code></strong></dt>
<dd>Dimensions to reduce as <code>DimFilter</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl></div>
</dd>
<dt id="phiml.math.l2_loss"><code class="name flex">
<span>def <span class="ident">l2_loss</span></span>(<span>x, reduce: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2</em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or 0D or 1D native tensor.
For <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> objects, only value the sum over all value attributes is computed.</dd>
<dt><strong><code>reduce</code></strong></dt>
<dd>Dimensions to reduce as <code>DimFilter</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl></div>
</dd>
<dt id="phiml.math.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>x: phiml.math._tensors.Tensor, dx: Union[float, phiml.math._tensors.Tensor] = 1, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = zero-gradient, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, weights: phiml.math._tensors.Tensor = None, padding_kwargs: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Laplace operator as defined for scalar fields.
If a vector field is passed, the laplace is computed component-wise.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional field of shape (batch, spacial dimensions&hellip;, components)</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>scalar or 1d tensor</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding mode.
Must be one of the following: <code>Extrapolation</code>, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or number for constant extrapolation, name of extrapolation as <code>str</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>The second derivative along these dimensions is summed over</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>(Optional) Multiply the axis terms by these factors before summation.
Must be a Tensor with a single channel dimension that lists all laplace dims by name.</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of same shape as <code>x</code></p></div>
</dd>
<dt id="phiml.math.layout"><code class="name flex">
<span>def <span class="ident">layout</span></span>(<span>objects, *shape: Union[str, phiml.math._shape.Shape]) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps a Python tree in a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, allowing elements to be accessed via dimensions.
A python tree is a structure of nested <code>tuple</code>, <code>list</code>, <code>dict</code> and <em>leaf</em> objects where leaves can be any Python object.</p>
<p>All keys of <code>dict</code> containers must be of type <code>str</code>.
The keys are automatically assigned as item names along that dimension unless conflicting with other elements.</p>
<p>Strings may also be used as containers.</p>
<p>Example:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; t = layout({'a': 'text', 'b': [0, 1]}, channel('dict,inner'))
&gt;&gt;&gt; t.inner[1].dict['a'].native()
'e'
</code></pre>
<p>See Also:
<code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>objects</code></strong></dt>
<dd>PyTree of <code>list</code> or <code>tuple</code>.</dd>
<dt><strong><code>*shape</code></strong></dt>
<dd>Tensor dimensions</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Calling <code><a title="phiml.math.Tensor.native" href="#phiml.math.Tensor.native">Tensor.native()</a></code> on the returned tensor will return <code>objects</code>.</p></div>
</dd>
<dt id="phiml.math.length"><code class="name flex">
<span>def <span class="ident">length</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Deprecated. Use <code><a title="phiml.math.norm" href="#phiml.math.norm">norm()</a></code> instead.</p></div>
</dd>
<dt id="phiml.math.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>start: Union[float, phiml.math._tensors.Tensor, tuple, list], stop: Union[float, phiml.math._tensors.Tensor, tuple, list], dim: phiml.math._shape.Shape) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns <code>number</code> evenly spaced numbers between <code>start</code> and <code>stop</code> along <code>dim</code>.</p>
<p>If <code>dim</code> contains multiple dimensions, evenly spaces values along each dimension, then stacks the result along a new channel dimension called <code>vector</code>.</p>
<p>See Also:
<code><a title="phiml.math.arange" href="#phiml.math.arange">arange()</a></code>, <code><a title="phiml.math.meshgrid" href="#phiml.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>start</code></strong></dt>
<dd>First value, <code>int</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>stop</code></strong></dt>
<dd>Last value, <code>int</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Linspace dimension of integer size.
The size determines how many values to linearly space between <code>start</code> and <code>stop</code>.
The values will be laid out along <code>dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.linspace(0, 1, spatial(x=5))
(0.000, 0.250, 0.500, 0.750, 1.000) along xˢ
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.linspace(0, (-1, 1), spatial(x=3))
(0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (xˢ=3, vectorᶜ=2)
</code></pre></div>
</dd>
<dt id="phiml.math.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>file: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree from a file previously written using <code><a title="phiml.math.save" href="#phiml.math.save">save()</a></code>.</p>
<p>All tensors are restored as NumPy arrays, not the backend-specific tensors they may have been written as.
Use <code><a title="phiml.math.convert" href="#phiml.math.convert">convert()</a></code> to convert all or some of the tensors to a different backend.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>File to read.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as what was written.</p></div>
</dd>
<dt id="phiml.math.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the natural logarithm of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.log10"><code class="name flex">
<span>def <span class="ident">log10</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>log(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code> with base 10.</p></div>
</dd>
<dt id="phiml.math.log2"><code class="name flex">
<span>def <span class="ident">log2</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>log(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code> with base 2.</p></div>
</dd>
<dt id="phiml.math.log_gamma"><code class="name flex">
<span>def <span class="ident">log_gamma</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>log(gamma(x))</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>function: Callable[..., ~Y], *args, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function shape&gt;, range=builtins.range, unwrap_scalars=True, expand_results=False, simplify=False, **kwargs) ‑> Union[None, phiml.math._tensors.Tensor, ~Y]</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>function</code> on slices of the arguments and returns the stacked result.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to be called on slices of <code>args</code> and <code>kwargs</code>.
Must return one or multiple values that can be stacked.
<code>None</code> may be returned but if any return value is <code>None</code>, all calls to <code>function</code> must return <code>None</code> in that position.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Positional arguments for <code>function</code>.
Values that are <code><a title="phiml.math.magic.Sliceable" href="magic.html#phiml.math.magic.Sliceable">Sliceable</a></code> will be sliced along <code>dims</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments for <code>function</code>.
Values that are <code><a title="phiml.math.magic.Sliceable" href="magic.html#phiml.math.magic.Sliceable">Sliceable</a></code> will be sliced along <code>dims</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions which should be sliced.
<code>function</code> is called once for each element in <code>dims</code>, i.e. <code>dims.volume</code> times.
If <code>dims</code> is not specified, all dimensions from the <code><a title="phiml.math.magic.Sliceable" href="magic.html#phiml.math.magic.Sliceable">Sliceable</a></code> values in <code>args</code> and <code>kwargs</code> will be mapped.
Pass <code>object</code> to map only objects, not tensors of primitives (<code>dtype.kind == object</code>). This will select only <code><a title="phiml.math.layout" href="#phiml.math.layout">layout()</a></code>-type dimensions.</dd>
<dt><strong><code>range</code></strong></dt>
<dd>Optional range function. Can be used to generate <code>tqdm</code> output by passing <code>trange</code>.</dd>
<dt><strong><code>unwrap_scalars</code></strong></dt>
<dd>If <code>True</code>, passes the contents of scalar <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>s instead of the tensor objects.</dd>
<dt><strong><code>simplify</code></strong></dt>
<dd>If <code>True</code>, reduces constant dims of output tensors that don't vary across mapped slices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of same shape as <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.map_pairs"><code class="name flex">
<span>def <span class="ident">map_pairs</span></span>(<span>map_function: Callable, values: phiml.math._tensors.Tensor, connections: phiml.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates <code>map_function</code> on all pairs of elements present in the sparsity pattern of <code>connections</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>map_function</code></strong></dt>
<dd>Function with signature <code>(Tensor, Tensor) -&gt; Tensor</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Values to evaluate <code>map_function</code> on.
Needs to have a spatial or instance dimension but must not have a dual dimension.</dd>
<dt><strong><code>connections</code></strong></dt>
<dd>Sparse tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the sparse dimensions of <code>connections</code> and all non-instance dimensions returned by <code>map_function</code>.</p></div>
</dd>
<dt id="phiml.math.map_types"><code class="name flex">
<span>def <span class="ident">map_types</span></span>(<span>f: Callable, dims: Union[phiml.math._shape.Shape, tuple, list, str, Callable], dim_type: Union[str, Callable]) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps a function to change the dimension types of its <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> and <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> arguments.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to wrap.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Concrete dimensions or dimension type, such as <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code> or <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>.
These dimensions will be mapped to <code>dim_type</code> for all positional function arguments.</dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>Dimension type, such as <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code> or <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>.
<code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> will be called with dimensions remapped to this type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with signature matching <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</p></div>
</dd>
<dt id="phiml.math.masked_fill"><code class="name flex">
<span>def <span class="ident">masked_fill</span></span>(<span>values: phiml.math._tensors.Tensor, valid: phiml.math._tensors.Tensor, distance: int = 1) ‑> Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolates the values of <code>values</code> which are marked by the nonzero values of <code>valid</code> for <code>distance</code> steps in all spatial directions.
Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor which holds the values for extrapolation</dd>
<dt><strong><code>valid</code></strong></dt>
<dd>Tensor with same size as <code>x</code> marking the values for extrapolation with nonzero values</dd>
<dt><strong><code>distance</code></strong></dt>
<dd>Number of extrapolation steps</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>values</code></dt>
<dd>Extrapolation result</dd>
<dt><code>valid</code></dt>
<dd>mask marking all valid values after extrapolation</dd>
</dl></div>
</dd>
<dt id="phiml.math.matrix_from_function"><code class="name flex">
<span>def <span class="ident">matrix_from_function</span></span>(<span>f: Callable, *args, auxiliary_args=None, auto_compress=False, sparsify_batch=None, separate_independent=False, **kwargs) ‑> Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Trace a linear function and construct a matrix.
Depending on the functional form of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>, the returned matrix may be dense or sparse.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to trace.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Arguments for <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Arguments in which the function is not linear.
These parameters are not traced but passed on as given in <code>args</code> and <code>kwargs</code>.</dd>
<dt><strong><code>auto_compress</code></strong></dt>
<dd>If <code>True</code>, returns a compressed matrix if supported by the backend.</dd>
<dt><strong><code>sparsify_batch</code></strong></dt>
<dd>If <code>False</code>, the matrix will be batched.
If <code>True</code>, will create dual dimensions for the involved batch dimensions.
This will result in one large matrix instead of a batch of matrices.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments for <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matrix</code></dt>
<dd>Matrix representing the linear dependency of the output <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> on the input of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.
Input dimensions will be <code><a title="phiml.math.dual" href="#phiml.math.dual">dual()</a></code> dimensions of the matrix while output dimensions will be regular.</dd>
<dt><code>bias</code></dt>
<dd>Bias for affine functions or zero-vector if the function is purely linear.</dd>
</dl></div>
</dd>
<dt id="phiml.math.matrix_rank"><code class="name flex">
<span>def <span class="ident">matrix_rank</span></span>(<span>matrix: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Approximates the rank of a matrix.
The tolerances used depend on the current precision.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Sparse or dense matrix, i.e. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with primal and dual dims.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Matrix rank.</p></div>
</dd>
<dt id="phiml.math.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>value: ~TensorOrTree, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, key: phiml.math._tensors.Tensor = None) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the maximum value of <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>(Sparse) <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>key</code></strong></dt>
<dd>Optional comparison values. If specified, returns the value where <code>key</code> is maximal, see <code><a title="phiml.math.at_max" href="#phiml.math.at_max">at_max()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: Union[float, phiml.math._tensors.Tensor], y: Union[float, phiml.math._tensors.Tensor], allow_none=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the element-wise maximum of <code>x</code> and <code>y</code>.</p></div>
</dd>
<dt id="phiml.math.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, weight: Union[phiml.math._tensors.Tensor, list, tuple] = None, where_no_weight=nan, epsilon=1e-10) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the mean over <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>(Sparse) <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>weight</code></strong></dt>
<dd>Optionally perform a weighted mean operation. Must broadcast to <code>value</code>.</dd>
<dt><strong><code>where_no_weight</code></strong></dt>
<dd>Value to use when the sum of all weights are smaller than <code>epsilon</code>.</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>Only if <code>where_no_weight</code>. Threshold for using <code>where_no_weight</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.median"><code class="name flex">
<span>def <span class="ident">median</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces <code>dim</code> of <code>value</code> by picking the median value.
For odd dimension sizes (ambigous choice), the linear average of the two median values is computed.</p>
<p>Currently implemented via <code><a title="phiml.math.quantile" href="#phiml.math.quantile">quantile()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.merge_shapes"><code class="name flex">
<span>def <span class="ident">merge_shapes</span></span>(<span>*objs: Union[phiml.math._shape.Shape, Any], order=(&lt;function batch&gt;, &lt;function dual&gt;, &lt;function instance&gt;, &lt;function spatial&gt;, &lt;function channel&gt;), allow_varying_sizes=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Combines <code>shapes</code> into a single <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, grouping dimensions by type.
If dimensions with equal names are present in multiple shapes, their types and sizes must match.</p>
<p>The shorthand <code>shape1 &amp; shape2</code> merges shapes with <code>check_exact=[spatial]</code>.</p>
<p>See Also:
<code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*objs</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code>Shaped</code> objects to combine.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>Dimension type order as <code>tuple</code> of type filters (<code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code> or <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>). Dimensions are grouped by type while merging.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Merged <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p>
<h2 id="raises">Raises</h2>
<p>IncompatibleShapes if the shapes are not compatible</p></div>
</dd>
<dt id="phiml.math.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>dims: Union[Callable, phiml.math._shape.Shape] = &lt;function spatial&gt;, stack_dim=(vectorᶜ=None), **dimensions: Union[int, phiml.math._tensors.Tensor, tuple, list, Any]) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a mesh-grid <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> from keyword dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>Mesh-grid dimensions, mapping names to values.
Values may be <code>int</code>, 1D <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or 1D native tensor.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimension type of mesh-grid dimensions, one of <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>.</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>Channel dim along which grids are stacked.
This is optional for 1D mesh-grids. In that case returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without a stack dim if <code>None</code> or an empty <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> is passed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Mesh-grid <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the dimensions of <code>dims</code> / <code>dimensions</code> and <code>stack_dim</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.meshgrid(x=2, y=2)
(xˢ=2, yˢ=2, vectorᶜ=x,y) 0.500 ± 0.500 (0e+00...1e+00)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.meshgrid(x=2, y=(-1, 1))
(xˢ=2, yˢ=2, vectorᶜ=x,y) 0.250 ± 0.829 (-1e+00...1e+00)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.meshgrid(x=2, stack_dim=None)
(0, 1) along xˢ
</code></pre></div>
</dd>
<dt id="phiml.math.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, key: phiml.math._tensors.Tensor = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the minimum value of <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>(Sparse) <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>key</code></strong></dt>
<dd>Optional comparison values. If specified, returns the value where <code>key</code> is minimal, see <code><a title="phiml.math.at_min" href="#phiml.math.at_min">at_min()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.minimize"><code class="name flex">
<span>def <span class="ident">minimize</span></span>(<span>f: Callable[[~X], ~Y], solve: phiml.math._optimize.Solve[~X, ~Y]) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Finds a minimum of the scalar function <em>f(x)</em>.
The <code>method</code> argument of <code>solve</code> determines which optimizer is used.
All optimizers supported by <code>scipy.optimize.minimize</code> are supported,
see <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a> .
Additionally a gradient descent solver with adaptive step size can be used with <code>method='GD'</code>.</p>
<p><code>math.minimize()</code> is limited to backends that support <code><a title="phiml.math.jacobian" href="#phiml.math.jacobian">jacobian()</a></code>, i.e. PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.solve_nonlinear" href="#phiml.math.solve_nonlinear">solve_nonlinear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is subject to minimization.
All positional arguments of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> are optimized and must be <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.
If <code>solve.x0</code> is a <code>tuple</code> or <code>list</code>, it will be passed to <em>f</em> as varargs, <code>f(*x0)</code>.
To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.
The first return value of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> must be a scalar float <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> object to specify method type, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution, the minimum point <code>x</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code></dt>
<dd>If the optimization failed prematurely.</dd>
</dl></div>
</dd>
<dt id="phiml.math.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x: Union[float, phiml.math._tensors.Tensor], y: Union[float, phiml.math._tensors.Tensor], allow_none=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the element-wise minimum of <code>x</code> and <code>y</code>.</p></div>
</dd>
<dt id="phiml.math.nan_to_0"><code class="name flex">
<span>def <span class="ident">nan_to_0</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Replaces all NaN values in <code>x</code> with <code>0</code>.</p></div>
</dd>
<dt id="phiml.math.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>value: Union[phiml.math._tensors.Tensor, numbers.Number, tuple, list, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the native tensor representation of <code>value</code>.
If <code>value</code> is a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phiml.math.Tensor.native" href="#phiml.math.Tensor.native">Tensor.native()</a></code>.
Otherwise, checks that <code>value</code> is a valid tensor object and returns it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or native tensor or tensor-like.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
</dd>
<dt id="phiml.math.native_call"><code class="name flex">
<span>def <span class="ident">native_call</span></span>(<span>f: Callable, *inputs: phiml.math._tensors.Tensor, channels_last=None, channel_dim='vector', spatial_dim=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> with the native representations of the <code>inputs</code> tensors in standard layout and returns the result as a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p>
<p>All inputs are converted to native tensors (including precision cast) depending on <code>channels_last</code>:</p>
<ul>
<li><code>channels_last=True</code>: Dimension layout <code>(total_batch_size, spatial_dims&hellip;, total_channel_size)</code></li>
<li><code>channels_last=False</code>: Dimension layout <code>(total_batch_size, total_channel_size, spatial_dims&hellip;)</code></li>
</ul>
<p>All batch dimensions are compressed into a single dimension with <code>total_batch_size = input.shape.batch.volume</code>.
The same is done for all channel dimensions.</p>
<p>Additionally, missing batch and spatial dimensions are added so that all <code>inputs</code> have the same batch and spatial shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be called on native tensors of <code>inputs</code>.
The function output must have the same dimension layout as the inputs, unless overridden by <code>spatial_dim</code>,
and the batch size must be identical.</dd>
<dt><strong><code>*inputs</code></strong></dt>
<dd>Uniform <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> arguments</dd>
<dt><strong><code>channels_last</code></strong></dt>
<dd>(Optional) Whether to put channels as the last dimension of the native representation.
If <code>None</code>, the channels are put in the default position associated with the current backend,
see <code>phiml.math.backend.Backend.prefers_channels_last()</code>.</dd>
<dt><strong><code>channel_dim</code></strong></dt>
<dd>Name of the channel dimension of the result.</dd>
<dt><strong><code>spatial_dim</code></strong></dt>
<dd>Name of the spatial dimension of the result.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with batch and spatial dimensions of <code>inputs</code>, unless overridden by <code>spatial_dim</code>,
and single channel dimension <code>channel_dim</code>.</p></div>
</dd>
<dt id="phiml.math.ncat"><code class="name flex">
<span>def <span class="ident">ncat</span></span>(<span>values: Sequence[~PhiTreeNodeType], dim: phiml.math._shape.Shape, expand_values=False) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenate named components along <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Each value can contain multiple components of <code>dim</code> if <code>dim</code> is present in its shape.
Else, it is interpreted as a single component whose name will be determined from the leftover item names of <code>dim</code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Single dimension that has item names matching components of <code>values</code>.</dd>
<dt><strong><code>expand_values</code></strong></dt>
<dd>If <code>True</code>, will add all missing dimensions to values, not just batch dimensions.
This allows tensors with different dimensions to be concatenated.
The resulting tensor will have all dimensions that are present in <code>values</code>.
If <code>False</code>, this may return a non-numeric object instead.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as any value from <code>values</code>.</p></div>
</dd>
<dt id="phiml.math.neighbor_max"><code class="name flex">
<span>def <span class="ident">neighbor_max</span></span>(<span>grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, extend_bounds=0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code> with <code>reduce_fun</code> set to <code><a title="phiml.math.max" href="#phiml.math.max">max_()</a></code>.</p></div>
</dd>
<dt id="phiml.math.neighbor_mean"><code class="name flex">
<span>def <span class="ident">neighbor_mean</span></span>(<span>grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, extend_bounds=0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code> with <code>reduce_fun</code> set to <code><a title="phiml.math.mean" href="#phiml.math.mean">mean()</a></code>.</p></div>
</dd>
<dt id="phiml.math.neighbor_min"><code class="name flex">
<span>def <span class="ident">neighbor_min</span></span>(<span>grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, extend_bounds=0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code> with <code>reduce_fun</code> set to <code><a title="phiml.math.min" href="#phiml.math.min">min_()</a></code>.</p></div>
</dd>
<dt id="phiml.math.neighbor_reduce"><code class="name flex">
<span>def <span class="ident">neighbor_reduce</span></span>(<span>reduce_fun: Callable, grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, padding_kwargs: dict = None, extend_bounds=0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the sum/mean/min/max/prod/etc. of two neighboring values along each dimension in <code>dim</code>.
The result tensor has one entry less than <code>grid</code> in each averaged dimension unless <code>padding</code> is specified.</p>
<p>With two <code>dims</code>, computes the mean of 4 values, in 3D, the mean of 8 values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>reduce_fun</code></strong></dt>
<dd>Reduction function, such as <code><a title="phiml.math.sum" href="#phiml.math.sum">sum_()</a></code>, <code><a title="phiml.math.mean" href="#phiml.math.mean">mean()</a></code>, <code><a title="phiml.math.max" href="#phiml.math.max">max_()</a></code>, <code><a title="phiml.math.min" href="#phiml.math.min">min_()</a></code>, <code><a title="phiml.math.prod" href="#phiml.math.prod">prod()</a></code>.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Values to reduce.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which neighbors should be reduced.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding at the upper edges of <code>grid</code> along <code>dims'. If not</code>None<code>, the result <a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a> will have the same <a title="phiml.math.shape" href="#phiml.math.shape">shape()</a> as </code>grid`.</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.neighbor_sum"><code class="name flex">
<span>def <span class="ident">neighbor_sum</span></span>(<span>grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = None, extend_bounds=0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code> with <code>reduce_fun</code> set to <code><a title="phiml.math.sum" href="#phiml.math.sum">sum_()</a></code>.</p></div>
</dd>
<dt id="phiml.math.non_batch"><code class="name flex">
<span>def <span class="ident">non_batch</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-batch dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.non_channel"><code class="name flex">
<span>def <span class="ident">non_channel</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-channel dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.non_dual"><code class="name flex">
<span>def <span class="ident">non_dual</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-dual dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.non_instance"><code class="name flex">
<span>def <span class="ident">non_instance</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-instance dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.non_primal"><code class="name flex">
<span>def <span class="ident">non_primal</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the batch and dual dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.non_spatial"><code class="name flex">
<span>def <span class="ident">non_spatial</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-spatial dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>value: phiml.math._tensors.Tensor, list_dim: Union[phiml.math._shape.Shape, str, int] = (nonzeroⁱ=None), index_dim: phiml.math._shape.Shape = (vectorᶜ=None), element_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, list_dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;, preserve_names=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get spatial indices of non-zero / True values.</p>
<p>Batch dimensions are preserved by this operation.
If channel dimensions are present, this method returns the indices where any component is nonzero.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html"><code>numpy.argwhere</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.nonzero.html"><code>torch.nonzero</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/where"><code>tf.where(tf.not_equal(values, 0))</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html"><code>jax.numpy.nonzero</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>spatial tensor to find non-zero / True values in.</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimension listing non-zero values. If size specified, lists only the first <code>size</code> non-zero values.
Special case: For retrieving only the first non-zero value, you may pass <code>1</code> instead of a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> of size 1.</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>Index dimension.</dd>
<dt><strong><code>element_dims</code></strong></dt>
<dd>Dims listing components of one value. A value is only considered <code>zero</code> if all components are 0.</dd>
<dt><strong><code>list_dims</code></strong></dt>
<dd>Dims in which non-zero elements are searched. These will be stored in the item names of <code>index_dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of shape (batch dims&hellip;, <code>list_dim</code>=#non-zero, <code>index_dim</code>=value.shape.spatial_rank)</p></div>
</dd>
<dt id="phiml.math.norm"><code class="name flex">
<span>def <span class="ident">norm</span></span>(<span>vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, eps: Union[float, phiml.math._tensors.Tensor] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the vector norm (L2 norm) of <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code> defined as √∑v².</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong></dt>
<dd>Minimum valid vector length. Use to avoid <code>inf</code> gradients for zero-norm vectors.
Lengths shorter than <code>eps</code> are set to 0.</dd>
</dl></div>
</dd>
<dt id="phiml.math.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, epsilon=None, allow_infinite=False, allow_zero=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes the vectors in <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code>. If <code>vec_dim</code> is None, the combined channel dimensions of <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code> are interpreted as a vector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to normalize.</dd>
<dt><strong><code>vec_dim</code></strong></dt>
<dd>Dimensions to normalize over. By default, all channel dimensions are used to compute the vector length.</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>(Optional) Zero-length threshold. Vectors shorter than this length yield the unit vector (1, 0, 0, &hellip;).
If not specified, the zero-vector yields <code>NaN</code> as it cannot be normalized.</dd>
<dt><strong><code>allow_infinite</code></strong></dt>
<dd>Allow infinite components in vectors. These vectors will then only points towards the infinite components.</dd>
<dt><strong><code>allow_zero</code></strong></dt>
<dd>Whether to return zero vectors for inputs smaller <code>epsilon</code> instead of a unit vector.</dd>
</dl></div>
</dd>
<dt id="phiml.math.normalize_to"><code class="name flex">
<span>def <span class="ident">normalize_to</span></span>(<span>target: phiml.math._tensors.Tensor, source: Union[float, phiml.math._tensors.Tensor], epsilon=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies the target so that its sum matches the source.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>source</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or constant</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>Small number to prevent division by zero.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Normalized tensor of the same shape as target</p></div>
</dd>
<dt id="phiml.math.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>value: Union[phiml.math._tensors.Tensor, numbers.Number, tuple, list, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Converts <code>value</code> to a <code>numpy.ndarray</code> where value must be a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, backend tensor or tensor-like.
If <code>value</code> is a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phiml.math.Tensor.numpy" href="#phiml.math.Tensor.numpy">Tensor.numpy()</a></code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phiml.math.Tensor.native" href="#phiml.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If <code>value</code> is a NumPy array, it may be returned directly.</p>
<h2 id="returns">Returns</h2>
<p>NumPy representation of <code>value</code></p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
</dd>
<dt id="phiml.math.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value <code>1.0</code>/ <code>1</code> / <code>True</code> everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<p>See Also:
<code><a title="phiml.math.ones_like" href="#phiml.math.ones_like">ones_like()</a></code>, <code><a title="phiml.math.zeros" href="#phiml.math.zeros">zeros()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type as <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code> object. Defaults to <code>float</code> matching the current precision setting.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>value: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing only <code>1.0</code> / <code>1</code> / <code>True</code> with the same shape and dtype as <code>obj</code>.</p></div>
</dd>
<dt id="phiml.math.pack_dims"><code class="name flex">
<span>def <span class="ident">pack_dims</span></span>(<span>value, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], packed_dim: Union[str, phiml.math._shape.Shape], pos: Optional[int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses multiple dimensions into a single dimension by concatenating the elements.
Elements along the new dimensions are laid out according to the order of <code>dims</code>.
If the order of <code>dims</code> differs from the current dimension order, the tensor is transposed accordingly.
This function replaces the traditional <code>reshape</code> for these cases.</p>
<p>The type of the new dimension will be equal to the types of <code>dims</code>.
If <code>dims</code> have varying types, the new dimension will be a batch dimension.</p>
<p>If none of <code>dims</code> exist on <code>value</code>, <code>packed_dim</code> will be added only if it is given with a definite size and <code>value</code> is not a primitive type.</p>
<p>See Also:
<code><a title="phiml.math.unpack_dim" href="#phiml.math.unpack_dim">unpack_dim()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to be compressed in the specified order.</dd>
<dt><strong><code>packed_dim</code></strong></dt>
<dd>Single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>pos</code></strong></dt>
<dd>Index of new dimension. <code>None</code> for automatic, <code>-1</code> for last, <code>0</code> for first.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance('points'))
(pointsⁱ=12) const 0.0
</code></pre></div>
</dd>
<dt id="phiml.math.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>value: phiml.math._tensors.Tensor, widths: Union[dict, tuple, list], mode: Union[ForwardRef('e_.Extrapolation'), phiml.math._tensors.Tensor, numbers.Number, str, dict] = 0, **kwargs) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
Unlike <code>Extrapolation.pad()</code>, this function can handle negative widths which slice off outer values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>
<p>Number of values to add at the edge of <code>value</code>. Negative values can be used to slice off edge values. Must be one of the following:</p>
<ul>
<li><code>tuple</code> containing <code>(lower: int, upper: int)</code>. This will pad all non-batch dimensions by <code>lower</code> and <code>upper</code> at the lower and upper edge, respectively.</li>
<li><code>dict</code> mapping <code>dim: str -&gt; (lower: int, upper: int)</code></li>
<li>Sequence of slicing <code>dict</code>s. This will add all values specified by the slicing dicts and is the inverse operation to <code><a title="phiml.math.slice_off" href="#phiml.math.slice_off">slice_off()</a></code>. Exactly one value in each slicing dict must be a <code><a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a></code> object.</li>
</ul>
</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Padding mode used to determine values added from positive <code>widths</code>.
Must be one of the following: <code>Extrapolation</code>, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or number for constant extrapolation, name of extrapolation as <code>str</code>.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional padding arguments.
These are ignored by the standard extrapolations defined in <code><a title="phiml.math.extrapolation" href="extrapolation.html">phiml.math.extrapolation</a></code> but can be used to pass additional contextual information to custom extrapolations.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Padded <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, 1), 'y': (2, 1)}, 0)
(xˢ=12, yˢ=13) 0.641 ± 0.480 (0e+00...1e+00)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, -1)}, 0)
(xˢ=10, yˢ=10) 0.900 ± 0.300 (0e+00...1e+00)
</code></pre></div>
</dd>
<dt id="phiml.math.pairwise_differences"><code class="name flex">
<span>def <span class="ident">pairwise_differences</span></span>(<span>positions: phiml.math._tensors.Tensor, max_distance: Union[float, phiml.math._tensors.Tensor] = None, format: Union[str, phiml.math._tensors.Tensor] = 'dense', domain: Optional[Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]] = None, periodic: Union[bool, phiml.math._tensors.Tensor] = False, method: str = 'auto', default: float = nan, avg_neighbors=8.0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the distance matrix containing the pairwise position differences between each pair of points.
The matrix will consist of the channel and batch dimension of <code>positions</code> and the primal dimensions plus their dual counterparts, spanning the matrix.
Points that are further apart than <code>max_distance</code> (if specified) are assigned an invalid value given by <code>default</code>.
The diagonal of the matrix (self-distance) consists purely of zero-vectors and is always stored explicitly.
The neighbors of the positions are listed along the dual dimension(s) of the matrix, and vectors point <em>towards</em> the neighbors.</p>
<p>This function can operate in <em>dense</em> mode or <em>sparse</em> mode, depending on <code>format</code>.
If <code>format=='dense'</code> or a dense <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, all possible pair-wise distances are considered and a full-rank tensor is returned.
The value of <code>method</code> is ignored in that case.</p>
<p>Otherwise, if <code>format</code> is a sparse format identifier or sparse <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, only a subset of distances is considered, depending on <code>method</code>.
In this case, the result is a sparse matrix with the same dimensionos as the dense tensor would have had.</p>
<p><strong>JIT behavior:</strong> This function can be JIT compiled with all backends.
However, as the exact number of neighbors is unknown beforehand, all sparse methods rely on a variable-size buffer.
PyTorch and TensorFlow allow variable shapes and behave the same way with JIT compilation as without.
JAX, however, requires all tensor shapes to be known beforehand.
This function will guess the required buffer size based on <code>avg_neighbors</code> and track the actually required sizes.
When using <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>, this will automatically trigger a re-tracing when a buffer overflow is detected.
User calling <code>jax.jit</code> manually must retrieve these sizes from the buffer API and implement buffer overflow handling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>positions</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Channel dimensions are interpreted as position components.
Instance and spatial dimensions list nodes.</dd>
<dt><strong><code>max_distance</code></strong></dt>
<dd>Scalar or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> specifying a max_radius for each point separately.
Can contain additional batch dimensions but spatial/instance dimensions must match <code>positions</code> if present.
If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.</dd>
<dt><strong><code>format</code></strong></dt>
<dd>Matrix format as <code>str</code> or concrete sparsity pattern as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Allowed strings are <code>'dense'',</code>'sparse'<code>, </code>'csr'<code>, </code>'coo'<code>, </code>'csc'`.
When a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> is passed, it needs to have all instance and spatial dims as <code>positions</code> as well as corresponding dual dimensions.
The distances will be evaluated at all stored entries of the <code>format</code> tensor.</dd>
<dt><strong><code>domain</code></strong></dt>
<dd>Lower and upper corner of the bounding box. All positions must lie within this box.
This must be specified to use with periodic boundaries.</dd>
<dt><strong><code>periodic</code></strong></dt>
<dd>Which domain boundaries should be treated as periodic, i.e. particles on opposite sides are neighbors.
Can be specified as a <code>bool</code> for all sides or as a vector-valued boolean <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to specify periodicity by direction.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value for distances greater than <code>max_distance</code>. Only for dense distance matrices.</dd>
<dt><strong><code>method</code></strong></dt>
<dd>
<p>Neighbor search algorithm; only used if <code>format</code> is a sparse format or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
The default, <code>'auto'</code> lets the runtime decide on the best method. Supported methods:</p>
<ul>
<li><code>'sparse'</code>: GPU-supported hash grid implementation with fully sparse connectivity.</li>
<li><code>'scipy-kd'</code>: SciPy's <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query_ball_point.html#scipy.spatial.KDTree.query_ball_point">kd-tree</a> implementation.</li>
</ul>
</dd>
<dt><strong><code>avg_neighbors</code></strong></dt>
<dd>Expected average number of neighbors. This is only relevant for hash grid searches, where it influences the default buffer sizes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Distance matrix as sparse or dense <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, depending on <code>format</code>.
For each spatial/instance dimension in <code>positions</code>, the matrix also contains a dual dimension of the same name and size.
The matrix also contains all batch dimensions of <code>positions</code> and the channel dimension of <code>positions</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))
&gt;&gt;&gt; dx = pairwise_differences(pos, format='dense', max_distance=2)
&gt;&gt;&gt; dx.particles[0]
(x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (~particlesᵈ=3, vectorᶜ=x,y)
</code></pre></div>
</dd>
<dt id="phiml.math.pairwise_distances"><code class="name flex">
<span>def <span class="ident">pairwise_distances</span></span>(<span>positions: phiml.math._tensors.Tensor, max_distance: Union[float, phiml.math._tensors.Tensor] = None, format: Union[str, phiml.math._tensors.Tensor] = 'dense', domain: Optional[Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]] = None, periodic: Union[bool, phiml.math._tensors.Tensor] = False, method: str = 'auto', default: float = nan, avg_neighbors=8.0) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the distance matrix containing the pairwise position differences between each pair of points.
The matrix will consist of the channel and batch dimension of <code>positions</code> and the primal dimensions plus their dual counterparts, spanning the matrix.
Points that are further apart than <code>max_distance</code> (if specified) are assigned an invalid value given by <code>default</code>.
The diagonal of the matrix (self-distance) consists purely of zero-vectors and is always stored explicitly.
The neighbors of the positions are listed along the dual dimension(s) of the matrix, and vectors point <em>towards</em> the neighbors.</p>
<p>This function can operate in <em>dense</em> mode or <em>sparse</em> mode, depending on <code>format</code>.
If <code>format=='dense'</code> or a dense <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, all possible pair-wise distances are considered and a full-rank tensor is returned.
The value of <code>method</code> is ignored in that case.</p>
<p>Otherwise, if <code>format</code> is a sparse format identifier or sparse <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, only a subset of distances is considered, depending on <code>method</code>.
In this case, the result is a sparse matrix with the same dimensionos as the dense tensor would have had.</p>
<p><strong>JIT behavior:</strong> This function can be JIT compiled with all backends.
However, as the exact number of neighbors is unknown beforehand, all sparse methods rely on a variable-size buffer.
PyTorch and TensorFlow allow variable shapes and behave the same way with JIT compilation as without.
JAX, however, requires all tensor shapes to be known beforehand.
This function will guess the required buffer size based on <code>avg_neighbors</code> and track the actually required sizes.
When using <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>, this will automatically trigger a re-tracing when a buffer overflow is detected.
User calling <code>jax.jit</code> manually must retrieve these sizes from the buffer API and implement buffer overflow handling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>positions</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Channel dimensions are interpreted as position components.
Instance and spatial dimensions list nodes.</dd>
<dt><strong><code>max_distance</code></strong></dt>
<dd>Scalar or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> specifying a max_radius for each point separately.
Can contain additional batch dimensions but spatial/instance dimensions must match <code>positions</code> if present.
If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.</dd>
<dt><strong><code>format</code></strong></dt>
<dd>Matrix format as <code>str</code> or concrete sparsity pattern as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
Allowed strings are <code>'dense'',</code>'sparse'<code>, </code>'csr'<code>, </code>'coo'<code>, </code>'csc'`.
When a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> is passed, it needs to have all instance and spatial dims as <code>positions</code> as well as corresponding dual dimensions.
The distances will be evaluated at all stored entries of the <code>format</code> tensor.</dd>
<dt><strong><code>domain</code></strong></dt>
<dd>Lower and upper corner of the bounding box. All positions must lie within this box.
This must be specified to use with periodic boundaries.</dd>
<dt><strong><code>periodic</code></strong></dt>
<dd>Which domain boundaries should be treated as periodic, i.e. particles on opposite sides are neighbors.
Can be specified as a <code>bool</code> for all sides or as a vector-valued boolean <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to specify periodicity by direction.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value for distances greater than <code>max_distance</code>. Only for dense distance matrices.</dd>
<dt><strong><code>method</code></strong></dt>
<dd>
<p>Neighbor search algorithm; only used if <code>format</code> is a sparse format or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
The default, <code>'auto'</code> lets the runtime decide on the best method. Supported methods:</p>
<ul>
<li><code>'sparse'</code>: GPU-supported hash grid implementation with fully sparse connectivity.</li>
<li><code>'scipy-kd'</code>: SciPy's <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query_ball_point.html#scipy.spatial.KDTree.query_ball_point">kd-tree</a> implementation.</li>
</ul>
</dd>
<dt><strong><code>avg_neighbors</code></strong></dt>
<dd>Expected average number of neighbors. This is only relevant for hash grid searches, where it influences the default buffer sizes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Distance matrix as sparse or dense <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, depending on <code>format</code>.
For each spatial/instance dimension in <code>positions</code>, the matrix also contains a dual dimension of the same name and size.
The matrix also contains all batch dimensions of <code>positions</code> and the channel dimension of <code>positions</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))
&gt;&gt;&gt; dx = pairwise_differences(pos, format='dense', max_distance=2)
&gt;&gt;&gt; dx.particles[0]
(x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (~particlesᵈ=3, vectorᶜ=x,y)
</code></pre></div>
</dd>
<dt id="phiml.math.perf_counter"><code class="name flex">
<span>def <span class="ident">perf_counter</span></span>(<span>wait_for_tensor, *wait_for_tensors: phiml.math._tensors.Tensor) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Get the time (<code>time.perf_counter()</code>) at which all <code>wait_for_tensors</code> are computed.
If all tensors are already available, returns the current <code>time.perf_counter()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>wait_for_tensor</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> that need to be computed before the time is measured.</dd>
<dt><strong><code>*wait_for_tensors</code></strong></dt>
<dd>Additional tensors that need to be computed before the time is measured.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Time at which all <code>wait_for_tensors</code> are ready as a scalar <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p></div>
</dd>
<dt id="phiml.math.pick_random"><code class="name flex">
<span>def <span class="ident">pick_random</span></span>(<span>value: ~TensorOrTree, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], count: Union[int, phiml.math._shape.Shape, None] = 1, weight: Optional[phiml.math._tensors.Tensor] = None) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Pick one or multiple random entries from <code>value</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor or tree. When containing multiple tensors, the corresponding entries are picked on all tensors that have <code>dim</code>.
You can pass <code><a title="phiml.math.range" href="#phiml.math.range">arange()</a></code> (the type) to retrieve the picked indices.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension along which to pick random entries. <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with one dim.</dd>
<dt><strong><code>count</code></strong></dt>
<dd>Number of entries to pick. When specified as a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, lists picked values along <code>count</code> instead of <code>dim</code>.</dd>
<dt><strong><code>weight</code></strong></dt>
<dd>Probability weight of each item along <code>dim</code>. Will be normalized to sum to 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree equal to <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phiml.math.set_global_precision" href="#phiml.math.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
</dd>
<dt id="phiml.math.primal"><code class="name flex">
<span>def <span class="ident">primal</span></span>(<span>obj) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the instance, spatial and channel dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or object with a valid <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>obj: Union[phiml.math._tensors.Tensor, <a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a>, numbers.Number, tuple, list, None] = None, name: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.</p>
<p>Unlike NumPy's array printing, the dimensions are sorted.
Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
Typically, this means x right, y up.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>tensor-like</dd>
<dt><strong><code>name</code></strong></dt>
<dd>name of the tensor</dd>
</dl>
<p>Returns:</p></div>
</dd>
<dt id="phiml.math.print_gradient"><code class="name flex">
<span>def <span class="ident">print_gradient</span></span>(<span>value: phiml.math._tensors.Tensor, name='', detailed=False) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Prints the gradient vector of <code>value</code> when computed.
The gradient at <code>value</code> is the vector-Jacobian product of all operations between the output of this function and the loss value.</p>
<p>The gradient is not printed in jit mode, see <code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python">def f(x):
    x = math.print_gradient(x, 'dx')
    return math.l1_loss(x)

math.jacobian(f)(math.ones(x=6))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> for which the gradient may be computed later.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>(Optional) Name to print along with the gradient values</dd>
<dt><strong><code>detailed</code></strong></dt>
<dd>If <code>False</code>, prints a short summary of the gradient tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.identity" href="#phiml.math.identity">identity()</a>(value)</code> which when differentiated, prints the gradient vector.</p></div>
</dd>
<dt id="phiml.math.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.quantile"><code class="name flex">
<span>def <span class="ident">quantile</span></span>(<span>value: phiml.math._tensors.Tensor, quantiles: Union[float, phiml.math._tensors.Tensor, tuple, list], dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the q-th quantile of <code>value</code> along <code>dim</code> for each q in <code>quantiles</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.quantile.html"><code>quantile</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"><code>quantile</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile"><code>tfp.stats.percentile</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html"><code>quantile</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>quantiles</code></strong></dt>
<dd>Single quantile or tensor of quantiles to compute.
Must be of type <code>float</code>, <code>tuple</code>, <code>list</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to reduce the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with dimensions of <code>quantiles</code> and non-reduced dimensions of <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.radians_to_degrees"><code class="name flex">
<span>def <span class="ident">radians_to_degrees</span></span>(<span>rad: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Convert degrees to radians.</p></div>
</dd>
<dt id="phiml.math.rand"><code class="name flex">
<span>def <span class="ident">rand</span></span>(<span>*shape: phiml.math._shape.Shape, low: Union[float, phiml.math._tensors.Tensor] = 0, high: Union[float, phiml.math._tensors.Tensor] = 1, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a uniform distribution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code> or <code>(kind, bits)</code>.
The dtype kind must be one of <code>float</code>, <code>int</code>, <code>complex</code>.
If not specified, a <code>float</code> tensor with the current default precision is created, see <code><a title="phiml.math.get_precision" href="#phiml.math.get_precision">get_precision()</a></code>.</dd>
<dt><strong><code>low</code></strong></dt>
<dd>Minimum value, included.</dd>
<dt><strong><code>high</code></strong></dt>
<dd>Maximum value, excluded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.randn"><code class="name flex">
<span>def <span class="ident">randn</span></span>(<span>*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a normal / Gaussian distribution.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html"><code>numpy.random.standard_normal</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.randn.html"><code>torch.randn</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/random/normal"><code>tf.random.normal</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html"><code>jax.random.normal</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) floating point <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code>. If <code>None</code>, a float tensor with the current default precision is created, see <code><a title="phiml.math.get_precision" href="#phiml.math.get_precision">get_precision()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a normal / Gaussian distribution.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html"><code>numpy.random.standard_normal</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.randn.html"><code>torch.randn</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/random/normal"><code>tf.random.normal</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html"><code>jax.random.normal</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) floating point <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code>. If <code>None</code>, a float tensor with the current default precision is created, see <code><a title="phiml.math.get_precision" href="#phiml.math.get_precision">get_precision()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.random_permutation"><code class="name flex">
<span>def <span class="ident">random_permutation</span></span>(<span>*shape: Union[phiml.math._shape.Shape, Any], dims=&lt;function non_batch&gt;, index_dim=(indexᶜ=None)) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate random permutations of the integers between 0 and the size of <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code>.</p>
<p>When multiple dims are given, the permutation is randomized across all of them and tensor of multi-indices is returned.</p>
<p>Batch dims result in batches of permutations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> of the result tensor, including <code>dims</code> and batches.</dd>
<dt><strong><code>*dims</code></strong></dt>
<dd>Sequence dims for an individual permutation. The total <code><a title="phiml.math.Shape.volume" href="#phiml.math.Shape.volume">Shape.volume</a></code> defines the maximum integer.
All other dims from <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> are treated as batch.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>*shape: phiml.math._shape.Shape, low: Union[float, phiml.math._tensors.Tensor] = 0, high: Union[float, phiml.math._tensors.Tensor] = 1, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a uniform distribution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code> or <code>(kind, bits)</code>.
The dtype kind must be one of <code>float</code>, <code>int</code>, <code>complex</code>.
If not specified, a <code>float</code> tensor with the current default precision is created, see <code><a title="phiml.math.get_precision" href="#phiml.math.get_precision">get_precision()</a></code>.</dd>
<dt><strong><code>low</code></strong></dt>
<dd>Minimum value, included.</dd>
<dt><strong><code>high</code></strong></dt>
<dd>Maximum value, excluded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.range"><code class="name flex">
<span>def <span class="ident">range</span></span>(<span>dim: phiml.math._shape.Shape, start_or_stop: Optional[int] = None, stop: Optional[int] = None, step=1, backend=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns evenly spaced values between <code>start</code> and <code>stop</code>.
If only one limit is given, <code>0</code> is used for the start.</p>
<p>See Also:
<code><a title="phiml.math.range_tensor" href="#phiml.math.range_tensor">range_tensor()</a></code>, <code><a title="phiml.math.linspace" href="#phiml.math.linspace">linspace()</a></code>, <code><a title="phiml.math.meshgrid" href="#phiml.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension name and type as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.
The <code>size</code> of <code>dim</code> is interpreted as <code>stop</code> unless <code>start_or_stop</code> is specified.</dd>
<dt><strong><code>start_or_stop</code></strong></dt>
<dd>(Optional) <code>int</code>. Interpreted as <code>start</code> if <code>stop</code> is specified as well. Otherwise this is <code>stop</code>.</dd>
<dt><strong><code>stop</code></strong></dt>
<dd>(Optional) <code>int</code>. <code>stop</code> value.</dd>
<dt><strong><code>step</code></strong></dt>
<dd>Distance between values.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Backend to use for creating the tensor. If unspecified, uses the current default.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.range_tensor"><code class="name flex">
<span>def <span class="ident">range_tensor</span></span>(<span>*shape: phiml.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with given <code><a title="phiml.math.shape" href="#phiml.math.shape">shape()</a></code> containing the linear indices of each element.
For 1D tensors, this equivalent to <code><a title="phiml.math.arange" href="#phiml.math.arange">arange()</a></code> with <code>step=1</code>.</p>
<p>See Also:
<code><a title="phiml.math.arange" href="#phiml.math.arange">arange()</a></code>, <code><a title="phiml.math.meshgrid" href="#phiml.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>Tensor shape.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.ravel_index"><code class="name flex">
<span>def <span class="ident">ravel_index</span></span>(<span>index: phiml.math._tensors.Tensor, resolution: phiml.math._shape.Shape, dim=&lt;function channel&gt;, mode='undefined') ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes a scalar index from a vector index.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with one channel dim.</dd>
<dt><strong><code>resolution</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dd>
<dt><strong><code>mode</code></strong></dt>
<dd><code>'undefined'</code>, <code>'periodic'</code>, <code>'clamp'</code> or an <code>int</code> to use for all invalid indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phiml.math.imag" href="#phiml.math.imag">imag()</a></code>, <code><a title="phiml.math.conjugate" href="#phiml.math.conjugate">conjugate()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Real component of <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.rename_dims"><code class="name flex">
<span>def <span class="ident">rename_dims</span></span>(<span>value: ~PhiTreeNodeType, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], names: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], **kwargs) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the name and optionally the type of some dimensions of <code>value</code>.</p>
<p>Dimensions that are not present on value will be ignored. The corresponding new dimensions given by <code>names</code> will not be added.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>Shapable</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Existing dimensions of <code>value</code> as comma-separated <code>str</code>, <code>tuple</code>, <code>list</code>, <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or filter function.</dd>
<dt><strong><code>names</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li>Sequence of names matching <code>dims</code> as <code>tuple</code>, <code>list</code> or <code>str</code>. This replaces only the dimension names but leaves the types untouched.</li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> matching <code>dims</code> to replace names and types.</li>
<li>Dimension type function to replace only types.</li>
</ul>
</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.replace"><code class="name flex">
<span>def <span class="ident">replace</span></span>(<span>obj: ~PhiTreeNodeType, **updates) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a copy of the given <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> with updated values as specified in <code>updates</code>.</p>
<p>If <code>obj</code> overrides <code>__with_attrs__</code>, the copy will be created via that specific implementation.
Otherwise, the <code><a title="phiml.math.copy" href="#phiml.math.copy">copy()</a></code> module and <code>setattr</code> will be used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code></dd>
<dt><strong><code>**updates</code></strong></dt>
<dd>Values to be replaced.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>obj</code> with updated values.</p></div>
</dd>
<dt id="phiml.math.replace_dims"><code class="name flex">
<span>def <span class="ident">replace_dims</span></span>(<span>value: ~PhiTreeNodeType, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], names: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], **kwargs) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the name and optionally the type of some dimensions of <code>value</code>.</p>
<p>Dimensions that are not present on value will be ignored. The corresponding new dimensions given by <code>names</code> will not be added.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>Shapable</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Existing dimensions of <code>value</code> as comma-separated <code>str</code>, <code>tuple</code>, <code>list</code>, <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or filter function.</dd>
<dt><strong><code>names</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li>Sequence of names matching <code>dims</code> as <code>tuple</code>, <code>list</code> or <code>str</code>. This replaces only the dimension names but leaves the types untouched.</li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> matching <code>dims</code> to replace names and types.</li>
<li>Dimension type function to replace only types.</li>
</ul>
</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.reshaped_native"><code class="name flex">
<span>def <span class="ident">reshaped_native</span></span>(<span>value: phiml.math._tensors.Tensor, groups: Union[tuple, list], force_expand: Any = True, to_numpy=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native representation of <code>value</code> where dimensions are laid out according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phiml.math.native" href="#phiml.math.native">native()</a></code>, <code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims()</a></code>, <code><a title="phiml.math.reshaped_tensor" href="#phiml.math.reshaped_tensor">reshaped_tensor()</a></code>, <code><a title="phiml.math.reshaped_numpy" href="#phiml.math.reshaped_numpy">reshaped_numpy()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>groups</code></strong></dt>
<dd>
<p><code>tuple</code> or <code>list</code> of dimensions to be packed into one native dimension. Each entry must be one of the following:</p>
<ul>
<li><code>str</code>: the name of one dimension that is present on <code>value</code>.</li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>: Dimensions to be packed. If <code>force_expand</code>, missing dimensions are first added, otherwise they are ignored.</li>
<li>Filter function: Packs all dimensions of this type that are present on <code>value</code>.</li>
<li>Ellipsis <code>&hellip;</code>: Packs all remaining dimensions into this slot. Can only be passed once.</li>
<li><code>None</code> or <code>()</code>: Adds a singleton dimension.</li>
</ul>
<p>Collections of or comma-separated dims may also be used but only if all dims are present on <code>value</code>.</p>
</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd><code>bool</code> or sequence of dimensions.
If <code>True</code>, repeats the tensor along missing dimensions.
If <code>False</code>, puts singleton dimensions where possible.
If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>If True, converts the native tensor to a <code>numpy.ndarray</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor with dimensions matching <code>groups</code>.</p></div>
</dd>
<dt id="phiml.math.reshaped_numpy"><code class="name flex">
<span>def <span class="ident">reshaped_numpy</span></span>(<span>value: phiml.math._tensors.Tensor, groups: Union[tuple, list], force_expand: Any = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the NumPy representation of <code>value</code> where dimensions are laid out according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phiml.math.numpy" href="#phiml.math.numpy">numpy_()</a></code>, <code><a title="phiml.math.reshaped_native" href="#phiml.math.reshaped_native">reshaped_native()</a></code>, <code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims()</a></code>, <code><a title="phiml.math.reshaped_tensor" href="#phiml.math.reshaped_tensor">reshaped_tensor()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension names as <code>str</code> or groups of dimensions to be packed_dim as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd><code>bool</code> or sequence of dimensions.
If <code>True</code>, repeats the tensor along missing dimensions.
If <code>False</code>, puts singleton dimensions where possible.
If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy <code>ndarray</code> with dimensions matching <code>groups</code>.</p></div>
</dd>
<dt id="phiml.math.reshaped_tensor"><code class="name flex">
<span>def <span class="ident">reshaped_tensor</span></span>(<span>value: Any, groups: Union[tuple, list], check_sizes=False, convert=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> from a native tensor or tensor-like whereby the dimensions of <code>value</code> are split according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.reshaped_native" href="#phiml.math.reshaped_native">reshaped_native()</a></code>, <code><a title="phiml.math.unpack_dim" href="#phiml.math.unpack_dim">unpack_dim()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Native tensor or tensor-like.</dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension groups to be packed_dim as <code>tuple[<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>]</code> or <code>list[<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>]</code>.</dd>
<dt><strong><code>check_sizes</code></strong></dt>
<dd>If True, group sizes must match the sizes of <code>value</code> exactly. Otherwise, allows singleton dimensions.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with all dimensions from <code>groups</code></p></div>
</dd>
<dt id="phiml.math.rotate_vector"><code class="name flex">
<span>def <span class="ident">rotate_vector</span></span>(<span>vector: phiml.math._tensors.Tensor, angle: Union[float, phiml.math._tensors.Tensor, None], invert=False, dim='vector') ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Rotates <code>vector</code> around the origin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vector</code></strong></dt>
<dd>n-dimensional vector with exactly one channel dimension</dd>
<dt><strong><code>angle</code></strong></dt>
<dd>Euler angle(s) or rotation matrix.
<code>None</code> is interpreted as no rotation.</dd>
<dt><strong><code>invert</code></strong></dt>
<dd>Whether to apply the inverse rotation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Rotated vector as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.rotation_matrix"><code class="name flex">
<span>def <span class="ident">rotation_matrix</span></span>(<span>x: Union[float, phiml.math._tensors.Tensor, None], matrix_dim=(vectorᶜ=None)) ‑> Optional[phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Create a 2D or 3D rotation matrix from the corresponding angle(s).</p>
<h2 id="args">Args</h2>
<dl>
<dt>x:</dt>
<dt>2D: scalar angle</dt>
<dt>3D: Either vector pointing along the rotation axis with rotation angle as length or Euler angles.</dt>
<dt>Euler angles need to be laid out along a <code><a title="phiml.math.angle" href="#phiml.math.angle">angle()</a></code> channel dimension with dimension names listing the spatial dimensions.</dt>
<dt>E.g. a 90° rotation about the z-axis is represented by <code>vec('angles', x=0, y=0, z=PI/2)</code>.</dt>
<dt>If a rotation matrix is passed for <code><a title="phiml.math.angle" href="#phiml.math.angle">angle()</a></code>, it is returned without modification.</dt>
<dt><strong><code>matrix_dim</code></strong></dt>
<dd>Matrix dimension for 2D rotations. In 3D, the channel dimension of angle is used.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Matrix containing <code>matrix_dim</code> in primal and dual form as well as all non-channel dimensions of <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Rounds the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code> to the closest integer.</p></div>
</dd>
<dt id="phiml.math.s2b"><code class="name flex">
<span>def <span class="ident">s2b</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>spatial</em> dimensions of <code>value</code> to <em>batch</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.safe_div"><code class="name flex">
<span>def <span class="ident">safe_div</span></span>(<span>x: Union[numbers.Number, phiml.math._tensors.Tensor], y: Union[numbers.Number, phiml.math._tensors.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>x/y</em> with the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>s <code>x</code> and <code>y</code> but returns 0 where <em>y=0</em>.</p></div>
</dd>
<dt id="phiml.math.safe_mul"><code class="name flex">
<span>def <span class="ident">safe_mul</span></span>(<span>x: Union[numbers.Number, phiml.math._tensors.Tensor], y: Union[numbers.Number, phiml.math._tensors.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplication for tensors with non-finite values.
Computes <em>x·y</em> in the forward pass but drops gradient contributions from infinite and <code>NaN</code> values.</p></div>
</dd>
<dt id="phiml.math.sample_subgrid"><code class="name flex">
<span>def <span class="ident">sample_subgrid</span></span>(<span>grid: phiml.math._tensors.Tensor, start: phiml.math._tensors.Tensor, size: phiml.math._shape.Shape) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a sub-grid from <code>grid</code> with equal distance between sampling points.
The values at the new sample points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to be resampled. Values are assumed to be sampled at cell centers.</dd>
<dt><strong><code>start</code></strong></dt>
<dd>Origin point of sub-grid within <code>grid</code>, measured in number of cells.
Must have a single dimension called <code>vector</code>.
Example: <code>start=(1, 0.5)</code> would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
The order of dims must be equal to <code>size</code> and <code>grid.shape.spatial</code>.</dd>
<dt><strong><code>size</code></strong></dt>
<dd>Resolution of the sub-grid. Must not be larger than the resolution of <code>grid</code>.
The order of dims must be equal to <code>start</code> and <code>grid.shape.spatial</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sub-grid as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>file: str, obj)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree using NumPy.
This function converts all tensors contained in <code>obj</code> to NumPy tensors before storing.
Each tensor is given a name corresponding to its path within <code>obj</code>, allowing reading only specific arrays from the file later on.
Pickle is used for structures, but no reference to <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or its sub-classes is included.</p>
<p>See Also:
<code><a title="phiml.math.load" href="#phiml.math.load">load()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file</code></strong></dt>
<dd>Target file, will be stored as <code>.npz</code>.</dd>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or tree to store.</dd>
</dl></div>
</dd>
<dt id="phiml.math.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>base_grid: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape], indices: Union[phiml.math._tensors.Tensor, dict], values: Union[float, phiml.math._tensors.Tensor], mode: Union[str, Callable] = 'update', outside_handling: str = 'check', indices_gradient=False, default=None, treat_as_batch=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Scatters <code>values</code> into <code>base_grid</code> at <code>indices</code>.
instance dimensions of <code>indices</code> and/or <code>values</code> are reduced during scattering.
Depending on <code>mode</code>, this method has one of the following effects:</p>
<ul>
<li><code>mode='update'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by <code>values</code>. The result is undefined if <code>indices</code> contains duplicates.</li>
<li><code>mode='add'</code>: Adds <code>values</code> to <code>base_grid</code> at <code>indices</code>. The values corresponding to duplicate indices are accumulated.</li>
<li><code>mode='mean'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by the mean of all <code>values</code> with the same index.</li>
</ul>
<p>Implementations:</p>
<ul>
<li>NumPy: Slice assignment / <code>numpy.add.at</code></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.scatter.html"><code>torch.scatter</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.scatter_add.html"><code>torch.scatter_add</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add"><code>tf.tensor_scatter_nd_add</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code>tf.tensor_scatter_nd_update</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html"><code>jax.lax.scatter_add</code></a>, <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html"><code>jax.lax.scatter</code></a></li>
</ul>
<p>See Also:
<code><a title="phiml.math.gather" href="#phiml.math.gather">gather()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> into which <code>values</code> are scattered.</dd>
<dt><strong><code>indices</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of n-dimensional indices at which to place <code>values</code>.
Must have a single channel dimension with size matching the number of spatial dimensions of <code>base_grid</code>.
This dimension is optional if the spatial rank is 1.
Must also contain all <code>scatter_dims</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of values to scatter at <code>indices</code>.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Scatter mode as <code>str</code> or function.
Supported modes are 'add', 'mean', 'update', 'max', 'min', 'prod', 'any', 'all'.
The corresponding functions are the built-in <code><a title="phiml.math.sum" href="#phiml.math.sum">sum_()</a></code>, <code>max´,</code>min<code>, as well as the reduce functions in </code>phiml.math`.</dd>
<dt><strong><code>outside_handling</code></strong></dt>
<dd>
<p>Defines how indices lying outside the bounds of <code>base_grid</code> are handled.</p>
<ul>
<li><code>'check'</code>: Raise an error if any index is out of bounds.</li>
<li><code>'discard'</code>: Outside indices are ignored.</li>
<li><code>'clamp'</code>: Outside indices are projected onto the closest point inside the grid.</li>
<li><code>'undefined'</code>: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.</li>
</ul>
</dd>
<dt><strong><code>indices_gradient</code></strong></dt>
<dd>Whether to allow the gradient of this operation to be backpropagated through <code>indices</code>.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Default value to use for bins into which no value is scattered.
By default, <code>NaN</code> is used for the modes <code>update</code> and <code><a title="phiml.math.mean" href="#phiml.math.mean">mean()</a></code>, <code>0</code> for <code><a title="phiml.math.sum" href="#phiml.math.sum">sum_()</a></code>, <code>inf</code> for min and <code>-inf</code> for max.
This will upgrade the data type to <code>float</code> if necessary.</dd>
<dt><strong><code>treat_as_batch</code></strong></dt>
<dd>Dimensions which should be treated like dims by this operation.
This can be used for scattering vectors along instance dims into a grid.
Normally, instance dims on <code>values</code> and <code>indices</code> would not be matched to <code>base_grid</code> but when treated as batch, they will be.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>base_grid</code> with updated values at <code>indices</code>.</p></div>
</dd>
<dt id="phiml.math.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>seed: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the current seed of all backends and the built-in <code>random</code> package.</p>
<p>Calling this function with a fixed value at the start of an application yields reproducible results
as long as the same backend is used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>seed</code></strong></dt>
<dd>Seed to use.</dd>
</dl></div>
</dd>
<dt id="phiml.math.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
</dd>
<dt id="phiml.math.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>obj, allow_unshaped=False) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>If <code>obj</code> is a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.Shaped" href="magic.html#phiml.math.magic.Shaped">Shaped</a></code>, returns its shape.
If <code>obj</code> is a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, returns <code>obj</code>.</p>
<p>This function can be passed as a <code>dim</code> argument to an operation to specify that it should act upon all dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code>Shaped</code></dd>
<dt><strong><code>allow_unshaped</code></strong></dt>
<dd>If <code>True</code>, returns an empty shape for unsupported objects, else raises a <code>ValueError</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>x: phiml.math._tensors.Tensor, offsets: Sequence[int], dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = zero-gradient, stack_dim: Union[str, phiml.math._shape.Shape, None] = (shiftᶜ=None), extend_bounds: Union[int, tuple] = 0, padding_kwargs: dict = None) ‑> List[phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Shift the tensor <code>x</code> by a fixed offset, using <code>padding</code> for edge values.</p>
<p>This is similar to <code>numpy.roll()</code> but with major differences:</p>
<ul>
<li>Values shifted in from the boundary are defined by <code>padding</code>.</li>
<li>Positive offsets represent negative shifts.</li>
<li>Support for multi-dimensional shifts</li>
</ul>
<p>See Also:
<code><a title="phiml.math.index_shift" href="#phiml.math.index_shift">index_shift()</a></code>, <code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input grid-like <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd><code>tuple</code> listing shifts to compute, each must be an <code>int</code>. One <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> will be returned for each entry.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to shift, defaults to all <em>spatial</em> dims of <code>x</code>.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding to be performed at the boundary so that the shifted versions have the same size as <code>x</code>.
Must be one of the following: <code>Extrapolation</code>, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or number for constant extrapolation, name of extrapolation as <code>str</code>.
Can be set to <code>None</code> to disable padding. Then the result tensors will be smaller than <code>x</code>.</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>Dimension along which the components corresponding to each dim in <code>dims</code> should be stacked.
This can be set to <code>None</code> only if <code>dims</code> is a single dimension.</dd>
<dt><strong><code>extend_bounds</code></strong></dt>
<dd>Number of cells by which to pad the tensors in addition to the number required to maintain the size of <code>x</code>.
Can only be used with a valid <code>padding</code>.</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>list</code> of shifted tensors. The number of return tensors is equal to the number of <code>offsets</code>.</p></div>
</dd>
<dt id="phiml.math.si2d"><code class="name flex">
<span>def <span class="ident">si2d</span></span>(<span>value: ~PhiTreeNodeType) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Change the type of all <em>spatial</em> and <em>instance</em> dimensions of <code>value</code> to <em>dual</em> dimensions. See <code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims()</a></code>.</p></div>
</dd>
<dt id="phiml.math.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the sigmoid function of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>The sign of positive numbers is 1 and -1 for negative numbers.
The sign of 0 is undefined.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> matching <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>sin(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.sinh"><code class="name flex">
<span>def <span class="ident">sinh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>sinh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.slice"><code class="name flex">
<span>def <span class="ident">slice</span></span>(<span>value: ~PhiTreeNodeType, slices: Union[<a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a>[str, Union[int, <a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, str, tuple, list, Any]], Any]) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Slices a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> along named dimensions.</p>
<p>See Also:
<code><a title="phiml.math.unstack" href="#phiml.math.unstack">unstack()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or <code>Number</code> or <code>None</code>.</dd>
<dt><strong><code>slices</code></strong></dt>
<dd>
<p><code>dict</code> mapping dimension names to slices. A slice can be one of the following:</p>
<ul>
<li>An index (<code>int</code>)</li>
<li>A range (<code><a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a></code>)</li>
<li>An item name (<code>str</code>)</li>
<li>Multiple item names (comma-separated <code>str</code>)</li>
<li>Multiple indices or item names (<code>tuple</code> or <code>list</code>)</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> of the same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; math.slice([vec(x=0, y=1), vec(x=2, y=3)], {'vector': 'y'})
[1, 3]
</code></pre></div>
</dd>
<dt id="phiml.math.slice_off"><code class="name flex">
<span>def <span class="ident">slice_off</span></span>(<span>x, *slices: <a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a>[str, Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, int, str]])</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Any instance of <code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code></dd>
</dl>
<p>*slices:
Returns:</p></div>
</dd>
<dt id="phiml.math.soft_plus"><code class="name flex">
<span>def <span class="ident">soft_plus</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>softplus(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.softmax"><code class="name flex">
<span>def <span class="ident">softmax</span></span>(<span>x, reduce: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None])</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the softmax of <code>x</code> over any dimension. The softmax is e^x / ∑ e^x .</p></div>
</dd>
<dt id="phiml.math.solve_linear"><code class="name flex">
<span>def <span class="ident">solve_linear</span></span>(<span>f: Union[Callable[[~X], ~Y], phiml.math._tensors.Tensor], y: ~Y, solve: phiml.math._optimize.Solve[~X, ~Y], *f_args, grad_for_f=False, f_kwargs: dict = None, **f_kwargs_) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the system of linear equations <em>f(x) = y</em> and returns <em>x</em>.
This method will use the solver specified in <code>solve</code>.
The following method identifiers are supported by all backends:</p>
<ul>
<li><code>'auto'</code>: Automatically choose a solver</li>
<li><code>'CG'</code>: Conjugate gradient, only for symmetric and positive definite matrices.</li>
<li><code>'CG-adaptive'</code>: Conjugate gradient with adaptive step size, only for symmetric and positive definite matrices.</li>
<li><code>'biCG'</code> or <code>'biCG-stab(0)'</code>: Biconjugate gradient</li>
<li><code>'biCG-stab'</code> or <code>'biCG-stab(1)'</code>: Biconjugate gradient stabilized, first order</li>
<li><code>'biCG-stab(2)'</code>, <code>'biCG-stab(4)'</code>, &hellip;: Biconjugate gradient stabilized, second or higher order</li>
<li><code>'scipy-direct'</code>: SciPy direct solve always run oh the CPU using <code>scipy.sparse.linalg.spsolve</code>.</li>
<li><code>'scipy-CG'</code>, <code>'scipy-GMres'</code>, <code>'scipy-biCG'</code>, <code>'scipy-biCG-stab'</code>, <code>'scipy-CGS'</code>, <code>'scipy-QMR'</code>, <code>'scipy-GCrotMK'</code>: SciPy iterative solvers always run oh the CPU, both in eager execution and JIT mode.</li>
</ul>
<p>For maximum performance, compile <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> using <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code> beforehand.
Then, an optimized representation of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> (such as a sparse matrix) will be used to solve the linear system.</p>
<p><strong>Caution:</strong> The matrix construction may potentially be performed each time <code><a title="phiml.math.solve_linear" href="#phiml.math.solve_linear">solve_linear()</a></code> is called if auxiliary arguments change.
To prevent this, jit-compile the function that makes the call to <code><a title="phiml.math.solve_linear" href="#phiml.math.solve_linear">solve_linear()</a></code>.</p>
<p>To obtain additional information about the performed solve, perform the solve within a <code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code> context.
The used implementation can be obtained as <code><a title="phiml.math.SolveInfo.method" href="#phiml.math.SolveInfo.method">SolveInfo.method</a></code>.</p>
<p>The gradient of this operation will perform another linear solve with the parameters specified by <code><a title="phiml.math.Solve.gradient_solve" href="#phiml.math.Solve.gradient_solve">Solve.gradient_solve</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.solve_nonlinear" href="#phiml.math.solve_nonlinear">solve_nonlinear()</a></code>, <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>
<p>One of the following:</p>
<ul>
<li>Linear function with <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> first parameter and return value. <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> can have additional auxiliary arguments and return auxiliary values.</li>
<li>Dense matrix (<code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with at least one dual dimension)</li>
<li>Sparse matrix (Sparse <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with at least one dual dimension)</li>
<li>Native tensor (not yet supported)</li>
</ul>
</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code><a title="phiml.math.f" href="#phiml.math.f">f</a>(x)</code> as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
<dt><strong><code>*f_args</code></strong></dt>
<dd>Positional arguments to be passed to <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> after <code>solve.x0</code>. These arguments will not be solved for.
Supports vararg mode or pass all arguments as a <code>tuple</code>.</dd>
<dt><strong><code>f_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code>.
These arguments are treated as auxiliary arguments and can be of any type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution of the linear system of equations <code>f(x) = y</code> as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
</dd>
<dt id="phiml.math.solve_nonlinear"><code class="name flex">
<span>def <span class="ident">solve_nonlinear</span></span>(<span>f: Callable, y, solve: phiml.math._optimize.Solve) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the non-linear equation <em>f(x) = y</em> by minimizing the norm of the residual.</p>
<p>This method is limited to backends that support <code><a title="phiml.math.jacobian" href="#phiml.math.jacobian">jacobian()</a></code>, currently PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.minimize" href="#phiml.math.minimize">minimize()</a></code>, <code><a title="phiml.math.solve_linear" href="#phiml.math.solve_linear">solve_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is optimized to match <code>y</code>.
All positional arguments of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> are optimized and must be <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.
The output of <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> must match <code>y</code>.</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code><a title="phiml.math.f" href="#phiml.math.f">f</a>(x)</code> as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>Solution fulfilling <code>f(x) = y</code> within specified tolerance as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
</dd>
<dt id="phiml.math.sort"><code class="name flex">
<span>def <span class="ident">sort</span></span>(<span>x: phiml.math._tensors.Tensor, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Sort the values of <code>x</code> along <code>dim</code>.
In order to sort a flattened array, use <code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims()</a></code> first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension to sort. If not present, sorting will be skipped. Defaults to non-batch dim.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sorted <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>x</code> if <code>x</code> is constant along <code>dims</code>.</p></div>
</dd>
<dt id="phiml.math.spack"><code class="name flex">
<span>def <span class="ident">spack</span></span>(<span>value, packed_dim: Union[str, phiml.math._shape.Shape], pos: Optional[int] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Short for `pack_dims(&hellip;, dims=spatial)</p></div>
</dd>
<dt id="phiml.math.sparse_tensor"><code class="name flex">
<span>def <span class="ident">sparse_tensor</span></span>(<span>indices: Optional[phiml.math._tensors.Tensor], values: Union[numbers.Number, phiml.math._tensors.Tensor], dense_shape: phiml.math._shape.Shape, can_contain_double_entries=True, indices_sorted=False, format=None, indices_constant: bool = True) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Construct a sparse tensor that stores <code>values</code> at the corresponding <code>indices</code> and is 0 everywhere else.
In addition to the sparse dimensions indexed by <code>indices</code>, the tensor inherits all batch and channel dimensions from <code>values</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> encoding the positions of stored values. It can either list the individual stored indices (COO format) or encode only part of the index while containing other dimensions directly (compact format).</p>
<p>For COO, it has the following dimensions:</p>
<ul>
<li>One instance dimension exactly matching the instance dimension on <code>values</code>.
It enumerates the positions of stored entries.</li>
<li>One channel dimension.
Its item names must match the dimension names of <code>dense_shape</code> but the order can be arbitrary.</li>
<li>Any number of batch dimensions</li>
</ul>
<p>You may pass <code>None</code> to create a sparse tensor with no entries.</p>
</dd>
<dt><strong><code>values</code></strong></dt>
<dd>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing the stored values at positions given by <code>indices</code>. It has the following dimensions:</p>
<ul>
<li>One instance dimension exactly matching the instance dimension on <code>indices</code>.
It enumerates the values of stored entries.</li>
<li>Any number of channel dimensions if multiple values are stored at each index.</li>
<li>Any number of batch dimensions</li>
</ul>
</dd>
<dt><strong><code>dense_shape</code></strong></dt>
<dd>Dimensions listed in <code>indices</code>.
The order can differ from the item names of <code>indices</code>.</dd>
<dt><strong><code>can_contain_double_entries</code></strong></dt>
<dd>Whether some indices might occur more than once.
If so, values at the same index will be summed.</dd>
<dt><strong><code>indices_sorted</code></strong></dt>
<dd>Whether the indices are sorted in ascending order given the dimension order of the item names of <code>indices</code>.</dd>
<dt><strong><code>indices_constant</code></strong></dt>
<dd>Whether the positions of the non-zero values are fixed.
If <code>True</code>, JIT compilation will not create a placeholder for <code>indices</code>.</dd>
<dt><strong><code>format</code></strong></dt>
<dd>Sparse format in which to store the data, such as <code>'coo'</code> or <code>'csr'</code>. See <code><a title="phiml.math.get_format" href="#phiml.math.get_format">get_format()</a></code>.
If <code>None</code>, uses the format in which the indices were given.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sparse <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with the specified <code>format</code>.</p></div>
</dd>
<dt id="phiml.math.spatial"><code class="name flex">
<span>def <span class="ident">spatial</span></span>(<span>*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('<a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the spatial dimensions of an existing <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or creates a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only spatial dimensions.</p>
<p>Usage for filtering spatial dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; spatial_dims = spatial(shape)
&gt;&gt;&gt; spatial_dims = spatial(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with only spatial dimensions:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; spatial_shape = spatial('undef', x=2, y=3)
(x=2, y=3, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> and <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> containing only dimensions of type spatial.</p></div>
</dd>
<dt id="phiml.math.spatial_gradient"><code class="name flex">
<span>def <span class="ident">spatial_gradient</span></span>(<span>grid: phiml.math._tensors.Tensor, dx: Union[float, phiml.math._tensors.Tensor] = 1, difference: str = 'central', padding: Union[<a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a>, float, phiml.math._tensors.Tensor, str, None] = zero-gradient, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, stack_dim: Union[str, phiml.math._shape.Shape, None] = (gradientᶜ=None), pad=0, padding_kwargs: dict = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the spatial_gradient of a scalar channel from finite differences.
The spatial_gradient vectors are in reverse order, lowest dimension first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid values</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>(Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Physical distance between grid points, <code>float</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.
When passing a vector-valued <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, the dx values should be listed along <code>stack_dim</code>, matching <code>dims</code>.</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>type of difference, one of ('forward', 'backward', 'central') (default 'forward')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Padding mode.
Must be one of the following: <code>Extrapolation</code>, <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or number for constant extrapolation, name of extrapolation as <code>str</code>.</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>name of the new vector dimension listing the spatial_gradient w.r.t. the various axes</dd>
<dt><strong><code>pad</code></strong></dt>
<dd>How many cells to extend the result compared to <code>grid</code>.
This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>sqrt(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.squared_norm"><code class="name flex">
<span>def <span class="ident">squared_norm</span></span>(<span>vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the squared norm of <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code>. If <code>vec_dim</code> is None, the combined channel dimensions of <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code> are interpreted as a vector.</p></div>
</dd>
<dt id="phiml.math.squeeze"><code class="name flex">
<span>def <span class="ident">squeeze</span></span>(<span>x: ~PhiTreeNodeType, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None]) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Remove specific singleton (volume=1) dims from <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor or composite type / tree.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Singleton dims to remove.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.stack"><code class="name flex">
<span>def <span class="ident">stack</span></span>(<span>values: Union[Sequence[~PhiTreeNodeType], <a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a>[str, ~PhiTreeNodeType]], dim: Union[str, phiml.math._shape.Shape], expand_values=False, simplify=False, layout_non_matching=False, **kwargs) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Stacks <code>values</code> along the new dimension <code>dim</code>.
All values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.
Batch dimensions will be added as needed.</p>
<p>Stacking tensors is performed lazily, i.e. the memory is allocated only when needed.
This makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Collection of <code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>
If a <code>dict</code>, keys must be of type <code>str</code> and are used as item names along <code>dim</code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with a least one dimension. None of these dimensions can be present with any of the <code>values</code>.
If <code>dim</code> is a single-dimension shape, its size is determined from <code>len(values)</code> and can be left undefined (<code>None</code>).
If <code>dim</code> is a multi-dimension shape, its volume must be equal to <code>len(values)</code>.</dd>
<dt><strong><code>expand_values</code></strong></dt>
<dd>If <code>True</code>, will first add missing dimensions to all values, not just batch dimensions.
This allows tensors with different dimensions to be stacked.
The resulting tensor will have all dimensions that are present in <code>values</code>.
If <code>False</code>, this may return a non-numeric object instead.</dd>
<dt><strong><code>simplify</code></strong></dt>
<dd>If <code>True</code> and all values are equal, returns one value without adding the dimension.</dd>
<dt><strong><code>layout_non_matching</code></strong></dt>
<dd>If non-matching values should be stacked using a Layout object, i.e. should be put into a named list instead.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing <code>values</code> stacked along <code>dim</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; stack({'x': 0, 'y': 1}, channel('vector'))
(x=0, y=1)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c='x,y'))
(x=0.000, y=1.000); (x=0.000, y=1.000) (bᵇ=2, cᶜ=x,y)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; stack([vec(x=1, y=0), vec(x=2, y=3.)], batch('b'))
(x=1.000, y=0.000); (x=2.000, y=3.000) (bᵇ=2, vectorᶜ=x,y)
</code></pre></div>
</dd>
<dt id="phiml.math.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the standard deviation over <code>values</code> along the specified dimensions.</p>
<p><em>Warning</em>: The standard deviation of non-uniform tensors along the stack dimension is undefined.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Disables gradients for the given tensor.
This may switch off the gradients for <code>x</code> itself or create a copy of <code>x</code> with disabled gradients.</p>
<p>Implementations:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach"><code>x.detach()</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient"><code>tf.stop_gradient</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html"><code>jax.lax.stop_gradient</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> for which gradients should be disabled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.stored_indices"><code class="name flex">
<span>def <span class="ident">stored_indices</span></span>(<span>x: phiml.math._tensors.Tensor, list_dim=(entriesⁱ=None), index_dim=(indexᶜ=None), invalid='discard') ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the indices of the stored values for a given `Tensor``.
For sparse tensors, this will return the stored indices tensor.
For collapsed tensors, only the stored dimensions will be returned.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimension along which stored indices should be laid out.</dd>
<dt><strong><code>invalid</code></strong></dt>
<dd>One of <code>'discard'</code>, <code>'clamp'</code>, <code>'keep'</code> Filter result by valid indices.
Internally, invalid indices may be stored for performance reasons.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> representing all indices of stored values.</p></div>
</dd>
<dt id="phiml.math.stored_values"><code class="name flex">
<span>def <span class="ident">stored_values</span></span>(<span>x: phiml.math._tensors.Tensor, list_dim=(entriesⁱ=None), invalid='discard') ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the stored values for a given `Tensor``.</p>
<p>For sparse tensors, this will return only the stored entries.</p>
<p>Dense tensors are reshaped so that all non-batch dimensions are packed into <code>list_dim</code>. Batch dimensions are preserved.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimension along which stored values should be laid out.</dd>
<dt><strong><code>invalid</code></strong></dt>
<dd>One of <code>'discard'</code>, <code>'clamp'</code>, <code>'keep'</code> Filter result by valid indices.
Internally, invalid indices may be stored for performance reasons.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> representing all values stored to represent <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>value: ~TensorOrTree, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function non_batch&gt;) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Sums <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>(Sparse) <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
</dd>
<dt id="phiml.math.svd"><code class="name flex">
<span>def <span class="ident">svd</span></span>(<span>x: phiml.math._tensors.Tensor, feature_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, list_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = None, latent_dim=(singularᶜ=None), full_matrices=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Singular value decomposition.</p>
<p>The original matrix is approximated by <code>(latent_to_value * singular.T) @ latents</code> or <code>latent_to_value @ (singular * latents)</code>.</p>
<p><strong>Warning:</strong> Even for well-defined SVDs, different backend use different sign conventions, causing results to differ.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Matrix containing <code>feature_dim</code> and <code>list_dim</code>.</dd>
<dt><strong><code>feature_dim</code></strong></dt>
<dd>Dimensions that list the features (columns).</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimensions that list the data points (rows).</dd>
<dt><strong><code>latent_dim</code></strong></dt>
<dd>Latent dimension. If a size is specified, truncates the SVD to this size.</dd>
<dt><strong><code>full_matrices</code></strong></dt>
<dd>If <code>True</code>, return full-sized (square) matrices for latent_by_example and latent_to_value. These may not match the singular values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>latents</code></dt>
<dd>Latent vectors of each item listed. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with <code>list_dim</code> and <code>latent_dim</code>.</dd>
<dt><code>singular</code></dt>
<dd>List of singular values. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with <code>latent_dim</code>.</dd>
<dt><code>features</code></dt>
<dd>Stacked normalized features / trends. This matrix can be used to compute the original value from a latent vector. <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with <code>latent_dim</code> and <code>feature_dim</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.math.swap_axes"><code class="name flex">
<span>def <span class="ident">swap_axes</span></span>(<span>x, axes)</span>
</code></dt>
<dd>
<div class="desc"><p>Swap the dimension order of <code>x</code>.
This operation is generally not necessary for <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>s because tensors will be reshaped under the hood or when getting the native/numpy representations.
It can be used to transpose native tensors.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.transpose.html"><code>numpy.transpose</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"><code>x.permute</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/transpose"><code>tf.transpose</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html"><code>jax.numpy.transpose</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or native tensor or <code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>.</dd>
<dt><strong><code>axes</code></strong></dt>
<dd><code>tuple</code> or <code>list</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or native tensor, depending on <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.tan"><code class="name flex">
<span>def <span class="ident">tan</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>tan(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.tanh"><code class="name flex">
<span>def <span class="ident">tanh</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>tanh(x)</em> of the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.tcat"><code class="name flex">
<span>def <span class="ident">tcat</span></span>(<span>values: Sequence[~PhiTreeNodeType], dim_type: Callable, expand_values=False, default_name='tcat') ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenate values by dim type.
This function first packs all dimensions of <code>dim_type</code> into one dim, then concatenates all <code>values</code>.
Values that do not have a dim of <code>dim_type</code> are considered a size-1 slice.</p>
<p>The name of the first matching dim of <code>dim_type</code> is used as the concatenated output dim name.
If no value has a matching dim, <code>default_name</code> is used instead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Values to be concatenated.</dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>Dimension type along which to concatenate.</dd>
<dt><strong><code>expand_values</code></strong></dt>
<dd>Whether to add missing other non-batch dims to values as needed.</dd>
<dt><strong><code>default_name</code></strong></dt>
<dd>Concatenation dim name if none of the values have a matching dim.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as any value.</p></div>
</dd>
<dt id="phiml.math.tensor"><code class="name flex">
<span>def <span class="ident">tensor</span></span>(<span>data, *shape: Union[phiml.math._shape.Shape, str, list], convert: bool = True, default_list_dim=(vectorᶜ=None)) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a Tensor from the specified <code>data</code>.
If <code>convert=True</code>, converts <code>data</code> to the preferred format of the default backend.</p>
<p><code>data</code> must be one of the following:</p>
<ul>
<li>Number: returns a dimensionless Tensor.</li>
<li>Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.</li>
<li><code>tuple</code> or <code>list</code> of numbers: backs the Tensor with native tensor.</li>
<li><code>tuple</code> or <code>list</code> of non-numbers: creates tensors for the items and stacks them.</li>
<li>Tensor: renames dimensions and dimension types if <code>names</code> is specified. Converts all internal native values of the tensor if <code>convert=True</code>.</li>
<li>Shape: creates a 1D tensor listing the dimension sizes.</li>
</ul>
<p>While specifying <code>names</code> is optional in some cases, it is recommended to always specify them.</p>
<p>Dimension types are always inferred from the dimension names if specified.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html"><code>numpy.array</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html"><code>torch.tensor</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.from_numpy.html"><code>torch.from_numpy</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"><code>tf.convert_to_tensor</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html"><code>jax.numpy.array</code></a></li>
</ul>
<p>See Also:
<code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code> which uses <code>convert=False</code>, <code><a title="phiml.math.layout" href="#phiml.math.layout">layout()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>native tensor, sparse COO / CSR / CSC matrix, scalar, sequence, <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Ordered dimensions and types. If sizes are defined, they will be checked against <code>data</code>.<code>You may also pass a single &lt;code&gt;str&lt;/code&gt; specifying dimension in the format</code>name:t<code>or</code>name:t=(item_names)` where <code>t</code> refers to the type letter, one of s,i,c,d,b.
Alternatively, you can pass a <code>list</code> of shapes which will call <code><a title="phiml.math.reshaped_tensor" href="#phiml.math.reshaped_tensor">reshaped_tensor()</a></code>.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>if dimension names are not provided and cannot automatically be inferred</dd>
<dt><code>ValueError</code></dt>
<dd>if <code>data</code> is not tensor-like</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor containing same values as data</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; tensor([1, 2, 3], channel(vector='x,y,z'))
(x=1, y=2, z=3)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tensor([1., 2, 3], channel(vector='x,y,z'))
(x=1.000, y=2.000, z=3.000) float64
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tensor(numpy.zeros([10, 8, 6, 2]), batch('batch'), spatial('x,y'), channel(vector='x,y'))
(batchᵇ=10, xˢ=8, yˢ=6, vectorᶜ=x,y) float64 const 0.0
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tensor([(0, 1), (0, 2), (1, 3)], instance('particles'), channel(vector='x,y'))
(x=0, y=1); (x=0, y=2); (x=1, y=3) (particlesⁱ=3, vectorᶜ=x,y)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tensor(numpy.random.randn(10))
(vectorᶜ=10) float64 -0.128 ± 1.197 (-2e+00...2e+00)
</code></pre></div>
</dd>
<dt id="phiml.math.tensor_like"><code class="name flex">
<span>def <span class="ident">tensor_like</span></span>(<span>existing_tensor: phiml.math._tensors.Tensor, values: Union[numbers.Number, phiml.math._tensors.Tensor, bool], value_order: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a tensor with the same format and shape as <code>existing_tensor</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>existing_tensor</code></strong></dt>
<dd>Any <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, sparse or dense.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>New values to replace the existing values by.
If <code>existing_tensor</code> is sparse, <code>values</code> must broadcast to the instance dimension listing the stored indices.</dd>
<dt><strong><code>value_order</code></strong></dt>
<dd>Order of <code>values</code> compared to <code>existing_tensor</code>, only relevant if <code>existing_tensor</code> is sparse.
If <code>'original'</code>, the values are ordered like the values that was used to create the first tensor with this sparsity pattern.
If <code>'as existing'</code>, the values match the current order of <code>existing_tensor</code>.
Note that the order of values may be changed upon creating a sparse tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to complex floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiML/Data_Types.html">https://tum-pbs.github.io/PhiML/Data_Types.html</a></p>
<p>See Also:
<code><a title="phiml.math.cast" href="#phiml.math.cast">cast()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>values to convert</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of same shape as <code>x</code></p></div>
</dd>
<dt id="phiml.math.to_device"><code class="name flex">
<span>def <span class="ident">to_device</span></span>(<span>value, device: phiml.backend._backend.ComputeDevice, convert=True, use_dlpack=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Allocates the tensors of <code>value</code> on <code>device</code>.
If the value already exists on that device, this function may either create a copy of <code>value</code> or return <code>value</code> directly.</p>
<p>See Also:
<code>to_cpu()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> or native tensor.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Device to allocate value on.
Either <code>ComputeDevice</code> or category <code>str</code>, such as <code>'CPU'</code> or <code>'GPU'</code>.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>Whether to convert tensors that do not belong to the corresponding backend to compatible native tensors.
If <code>False</code>, this function has no effect on numpy tensors.</dd>
<dt><strong><code>use_dlpack</code></strong></dt>
<dd>Only if <code>convert==True</code>.
Whether to use the DLPack library to convert from one GPU-enabled backend to another.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
</dd>
<dt id="phiml.math.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>value: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a serializable form of a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.
The result can be written to a JSON file, for example.</p>
<p>See Also:
<code><a title="phiml.math.from_dict" href="#phiml.math.from_dict">from_dict()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Serializable Python tree of primitives</p></div>
</dd>
<dt id="phiml.math.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiML/Data_Types.html">https://tum-pbs.github.io/PhiML/Data_Types.html</a></p>
<p>See Also:
<code><a title="phiml.math.cast" href="#phiml.math.cast">cast()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> to convert</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> matching <code>x</code>.</p></div>
</dd>
<dt id="phiml.math.to_format"><code class="name flex">
<span>def <span class="ident">to_format</span></span>(<span>x: phiml.math._tensors.Tensor, format: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to the specified sparse format or to a dense tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Sparse or dense <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>format</code></strong></dt>
<dd>Target format. One of <code>'dense'</code>, <code>'coo'</code>, <code>'csr'</code>, or <code>'csc'</code>.
Additionally, <code>'sparse'</code> can be passed to convert dense matrices to a sparse format, decided based on the backend for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> of the specified format.</p></div>
</dd>
<dt id="phiml.math.to_int32"><code class="name flex">
<span>def <span class="ident">to_int32</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code> to 32-bit integer.</p></div>
</dd>
<dt id="phiml.math.to_int64"><code class="name flex">
<span>def <span class="ident">to_int64</span></span>(<span>x: ~TensorOrTree) ‑> ~TensorOrTree</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> <code>x</code> to 64-bit integer.</p></div>
</dd>
<dt id="phiml.math.trace_check"><code class="name flex">
<span>def <span class="ident">trace_check</span></span>(<span>traced_function, *args, **kwargs) ‑> Tuple[bool, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if <code>f(*args, **kwargs)</code> has already been traced for arguments compatible with <code>args</code> and <code>kwargs</code>.
If true, jit-compiled functions are very fast since the Python function is not actually called anymore.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>traced_function</code></strong></dt>
<dd>Transformed Function, e.g. jit-compiled or linear function.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Hypothetical arguments to be passed to <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code></dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Hypothetical keyword arguments to be passed to <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd><code>True</code> if there is an existing trace that can be used, <code>False</code> if <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> would have to be re-traced.</dd>
<dt><code>message</code></dt>
<dd>A <code>str</code> that, if <code>result == False</code>, gives hints as to why <code><a title="phiml.math.f" href="#phiml.math.f">f</a></code> needs to be re-traced given <code>args</code> and <code>kwargs</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.math.unpack_dim"><code class="name flex">
<span>def <span class="ident">unpack_dim</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None], *unpacked_dims: Union[phiml.math._shape.Shape, Sequence[phiml.math._shape.Shape]], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses a dimension by unstacking the elements along it.
This function replaces the traditional <code>reshape</code> for these cases.
The compressed dimension <code>dim</code> is assumed to contain elements laid out according to the order of <code>unpacked_dims</code>.</p>
<p>If <code>dim</code> does not exist on <code>value</code>, this function will return <code>value</code> as-is. This includes primitive types.</p>
<p>See Also:
<code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, for which one dimension should be split.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Single dimension to be decompressed.</dd>
<dt><strong><code>*unpacked_dims</code></strong></dt>
<dd>Either vararg <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, ordered dimensions to replace <code>dim</code>, fulfilling <code>unpacked_dims.volume == shape(self)[dim].rank</code>.
This results in a single tensor output.
Alternatively, pass a <code>tuple</code> or <code>list</code> of shapes to unpack a dim into multiple tensors whose combined volumes match <code>dim.size</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))
(xˢ=4, yˢ=3) const 0.0
</code></pre></div>
</dd>
<dt id="phiml.math.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>value, dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None]) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Un-stacks a <code>Sliceable</code> along one or multiple dimensions.</p>
<p>If multiple dimensions are given, the order of elements will be according to the dimension order in <code>dim</code>, i.e. elements along the last dimension will be neighbors in the returned <code>tuple</code>.
If no dimension is given or none of the given dimensions exists on <code>value</code>, returns a list containing only <code>value</code>.</p>
<p>See Also:
<code><a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phiml.math.magic.Shapable" href="magic.html#phiml.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions as <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or comma-separated <code>str</code> or dimension type, i.e. <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of objects matching the type of <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; unstack(expand(0, spatial(x=5)), 'x')
(0.0, 0.0, 0.0, 0.0, 0.0)
</code></pre></div>
</dd>
<dt id="phiml.math.upsample2x"><code class="name flex">
<span>def <span class="ident">upsample2x</span></span>(<span>grid: phiml.math._tensors.Tensor, padding: <a title="phiml.math.extrapolation.Extrapolation" href="extrapolation.html#phiml.math.extrapolation.Extrapolation">Extrapolation</a> = zero-gradient, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function spatial&gt;, padding_kwargs: dict = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to double the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>half-size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which up-sampling is applied. If None, up-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor:</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
<dt><strong><code>padding_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code><a title="phiml.math.pad" href="#phiml.math.pad">pad()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>double-size grid</p></div>
</dd>
<dt id="phiml.math.use"><code class="name flex">
<span>def <span class="ident">use</span></span>(<span>backend: Union[str, phiml.backend._backend.Backend]) ‑> phiml.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the given backend as default.
This setting can be overridden using <code>with backend:</code>.</p>
<p>See <code>default_backend()</code>, <code><a title="phiml.math.choose_backend" href="#phiml.math.choose_backend">choose_backend_t()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>backend</code></strong></dt>
<dd><code>Backend</code> or backend name to set as default.
Possible names are <code>'torch'</code>, <code>'tensorflow'</code>, <code>'jax'</code>, <code>'numpy'</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The chosen backend as a `Backend´ instance.</p></div>
</dd>
<dt id="phiml.math.vec"><code class="name flex">
<span>def <span class="ident">vec</span></span>(<span>name: Union[str, phiml.math._shape.Shape] = 'vector', *sequence, tuple_dim=(sequenceˢ=None), list_dim=(sequenceⁱ=None), **components) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Lay out the given values along a channel dimension without converting them to the current backend.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Dimension name.</dd>
<dt><strong><code>*sequence</code></strong></dt>
<dd>Component values that will also be used as item names.
If specified, <code>components</code> must be empty.</dd>
<dt><strong><code>**components</code></strong></dt>
<dd>Values by component name.
If specified, no additional positional arguments must be given.</dd>
<dt><strong><code>tuple_dim</code></strong></dt>
<dd>Dimension for <code>tuple</code> values passed as components, e.g. <code>vec(x=(0, 1), ...)</code></dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimension for <code>list</code> values passed as components, e.g. <code>vec(x=[0, 1], ...)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; vec(x=1, y=0, z=-1)
(x=1, y=0, z=-1)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; vec(x=1., z=0)
(x=1.000, z=0.000)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; vec(x=tensor([1, 2, 3], instance('particles')), y=0)
(x=1, y=0); (x=2, y=0); (x=3, y=0) (particlesⁱ=3, vectorᶜ=x,y)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; vec(x=0, y=[0, 1])
(x=0, y=0); (x=0, y=1) (vectorᶜ=x,y, sequenceⁱ=2)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; vec(x=0, y=(0, 1))
(x=0, y=0); (x=0, y=1) (sequenceˢ=2, vectorᶜ=x,y)
</code></pre></div>
</dd>
<dt id="phiml.math.vec_length"><code class="name flex">
<span>def <span class="ident">vec_length</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Deprecated. Use <code><a title="phiml.math.norm" href="#phiml.math.norm">norm()</a></code> instead.</p></div>
</dd>
<dt id="phiml.math.vec_normalize"><code class="name flex">
<span>def <span class="ident">vec_normalize</span></span>(<span>vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = &lt;function channel&gt;, epsilon=None, allow_infinite=False, allow_zero=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes the vectors in <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code>. If <code>vec_dim</code> is None, the combined channel dimensions of <code><a title="phiml.math.vec" href="#phiml.math.vec">vec()</a></code> are interpreted as a vector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> to normalize.</dd>
<dt><strong><code>vec_dim</code></strong></dt>
<dd>Dimensions to normalize over. By default, all channel dimensions are used to compute the vector length.</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>(Optional) Zero-length threshold. Vectors shorter than this length yield the unit vector (1, 0, 0, &hellip;).
If not specified, the zero-vector yields <code>NaN</code> as it cannot be normalized.</dd>
<dt><strong><code>allow_infinite</code></strong></dt>
<dd>Allow infinite components in vectors. These vectors will then only points towards the infinite components.</dd>
<dt><strong><code>allow_zero</code></strong></dt>
<dd>Whether to return zero vectors for inputs smaller <code>epsilon</code> instead of a unit vector.</dd>
</dl></div>
</dd>
<dt id="phiml.math.vec_squared"><code class="name flex">
<span>def <span class="ident">vec_squared</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Deprecated. Use <code><a title="phiml.math.squared_norm" href="#phiml.math.squared_norm">squared_norm()</a></code> instead.</p></div>
</dd>
<dt id="phiml.math.when_available"><code class="name flex">
<span>def <span class="ident">when_available</span></span>(<span>runnable: Callable, *tensor_args: phiml.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>runnable(*tensor_args)</code> once the concrete values of all tensors are available.
In eager mode, <code>runnable</code> is called immediately.
When jit-compiled, <code>runnable</code> is called after the jit-compiled function has returned.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>runnable</code></strong></dt>
<dd>Function to call as <code>runnable(*tensor_args)</code>. This can be a <code>lambda</code> function.</dd>
<dt><strong><code>*tensor_args</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> values to pass to <code>runnable</code> with concrete values.</dd>
</dl></div>
</dd>
<dt id="phiml.math.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>condition: Union[phiml.math._tensors.Tensor, float, int], value_true: Union[phiml.math._tensors.Tensor, float, int, Any] = None, value_false: Union[phiml.math._tensors.Tensor, float, int, Any] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a tensor by choosing either values from <code>value_true</code> or <code>value_false</code> depending on <code>condition</code>.
If <code>condition</code> is not of type boolean, non-zero values are interpreted as True.</p>
<p>This function requires non-None values for <code>value_true</code> and <code>value_false</code>.
To get the indices of True / non-zero values, use :func:<code><a title="phiml.math.nonzero" href="#phiml.math.nonzero">nonzero()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong></dt>
<dd>determines where to choose values from value_true or from value_false</dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>Values to pick where <code>condition != 0 / True</code></dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>Values to pick where <code>condition == 0 / False</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing dimensions of all inputs.</p></div>
</dd>
<dt id="phiml.math.with_diagonal"><code class="name flex">
<span>def <span class="ident">with_diagonal</span></span>(<span>matrix: phiml.math._tensors.Tensor, values: Union[float, phiml.math._tensors.Tensor], check_square=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a copy of <code>matrix</code>, replacing the diagonal elements.
If <code>matrix</code> is sparse, diagonal zeros (and possibly other explicitly stored zeros) will be dropped from the sparse matrix.</p>
<p>This function currently only supports sparse COO,CSR,CSC SciPy matrices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> with at least one dual dim.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Diagonal values</dd>
<dt><strong><code>check_square</code></strong></dt>
<dd>If <code>True</code> allow this function only for square matrices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.wrap"><code class="name flex">
<span>def <span class="ident">wrap</span></span>(<span>data, *shape: Union[phiml.math._shape.Shape, str, list], default_list_dim=(vectorᶜ=None)) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Short for <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code> with <code>convert=False</code>.</p></div>
</dd>
<dt id="phiml.math.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) ‑> phiml.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value <code>0.0</code> / <code>0</code> / <code>False</code> everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<p>See Also:
<code><a title="phiml.math.zeros_like" href="#phiml.math.zeros_like">zeros_like()</a></code>, <code><a title="phiml.math.ones" href="#phiml.math.ones">ones()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type as <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code> object. Defaults to <code>float</code> matching the current precision setting.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></p></div>
</dd>
<dt id="phiml.math.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>obj: Union[phiml.math._tensors.Tensor, <a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a>]) ‑> Union[phiml.math._tensors.Tensor, <a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Create a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> containing only <code>0.0</code> / <code>0</code> / <code>False</code> with the same shape and dtype as <code>obj</code>.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phiml.math.ConvergenceException"><code class="flex name class">
<span>class <span class="ident">ConvergenceException</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for exceptions raised when a solve does not converge.</p>
<p>See Also:
<code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code>, <code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvergenceException(RuntimeError):
    &#34;&#34;&#34;
    Base class for exceptions raised when a solve does not converge.

    See Also:
        `Diverged`, `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        RuntimeError.__init__(self, result.msg)
        self.result: SolveInfo = result
        &#34;&#34;&#34; `SolveInfo` holding information about the solve. &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phiml.math._optimize.Diverged</li>
<li>phiml.math._optimize.NotConverged</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.ConvergenceException.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.SolveInfo" href="#phiml.math.SolveInfo">SolveInfo</a></code> holding information about the solve.</p></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.DType"><code class="flex name class">
<span>class <span class="ident">DType</span></span>
<span>(</span><span>kind: type, bits: int = None, precision: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instances of <code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code> represent the kind and size of data elements.
The data type of tensors can be obtained via <code><a title="phiml.math.Tensor.dtype" href="#phiml.math.Tensor.dtype">Tensor.dtype</a></code>.</p>
<p>The following kinds of data types are supported:</p>
<ul>
<li><code>float</code> with 32 / 64 bits</li>
<li><code>complex</code> with 64 / 128 bits</li>
<li><code>int</code> with 8 / 16 / 32 / 64 bits</li>
<li><code>bool</code> with 8 bits</li>
<li><code>str</code> with 8<em>n</em> bits</li>
</ul>
<p>Unlike with many computing libraries, there are no global variables corresponding to the available types.
Instead, data types can simply be instantiated as needed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong></dt>
<dd>Python type, one of <code>(bool, int, float, complex, str)</code></dd>
<dt><strong><code>bits</code></strong></dt>
<dd>number of bits per element, a multiple of 8.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DType:
    &#34;&#34;&#34;
    Instances of `DType` represent the kind and size of data elements.
    The data type of tensors can be obtained via `Tensor.dtype`.

    The following kinds of data types are supported:

    * `float` with 32 / 64 bits
    * `complex` with 64 / 128 bits
    * `int` with 8 / 16 / 32 / 64 bits
    * `bool` with 8 bits
    * `str` with 8*n* bits

    Unlike with many computing libraries, there are no global variables corresponding to the available types.
    Instead, data types can simply be instantiated as needed.
    &#34;&#34;&#34;

    def __init__(self, kind: type, bits: int = None, precision: int = None):
        &#34;&#34;&#34;
        Args:
            kind: Python type, one of `(bool, int, float, complex, str)`
            bits: number of bits per element, a multiple of 8.
        &#34;&#34;&#34;
        assert kind in (bool, int, float, complex, str, object)
        if kind is bool:
            assert bits is None, &#34;Bits may not be set for bool or object&#34;
            assert precision is None, f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            bits = 8
        elif kind == object:
            assert bits is None, &#34;bits may not be set for bool or object&#34;
            assert precision is None, f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            bits = int(np.round(np.log2(sys.maxsize))) + 1
        elif precision is not None:
            assert bits is None, &#34;Specify either bits or precision when creating a DType but not both.&#34;
            assert kind in [float, complex], f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            if kind == float:
                bits = precision
            else:
                bits = precision * 2
        else:
            assert isinstance(bits, int), f&#34;bits must be an int but got {type(bits)}&#34;
        self.kind = kind
        &#34;&#34;&#34; Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str) &#34;&#34;&#34;
        self.bits = bits
        &#34;&#34;&#34; Number of bits used to store a single value of this type. See `DType.itemsize`. &#34;&#34;&#34;

    @property
    def precision(self):
        &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
        if self.kind == float:
            return self.bits
        if self.kind == complex:
            return self.bits // 2
        else:
            return None

    @property
    def itemsize(self):
        &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
        assert self.bits % 8 == 0
        return self.bits // 8

    def __eq__(self, other):
        if isinstance(other, DType):
            return self.kind == other.kind and self.bits == other.bits
        elif other in {bool, int, float, complex, object}:
            return self.kind == other
        else:
            return False

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self.kind) + hash(self.bits)

    def __repr__(self):
        return f&#34;{self.kind.__name__}{self.bits}&#34;

    @staticmethod
    def as_dtype(value: Union[&#39;DType&#39;, tuple, type, None]) -&gt; Union[&#39;DType&#39;, None]:
        if isinstance(value, DType):
            return value
        elif value is int:
            return DType(int, 32)
        elif value is float:
            from . import get_precision
            return DType(float, get_precision())
        elif value is complex:
            from . import get_precision
            return DType(complex, 2 * get_precision())
        elif value is None:
            return None
        elif isinstance(value, tuple):
            return DType(*value)
        elif value is str:
            raise ValueError(&#34;str DTypes must specify precision&#34;)
        else:
            return DType(value)  # bool, object</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="phiml.math.DType.as_dtype"><code class="name flex">
<span>def <span class="ident">as_dtype</span></span>(<span>value: Union[ForwardRef('<a title="phiml.math.DType" href="#phiml.math.DType">DType</a>'), tuple, type, None]) ‑> Optional[phiml.backend._dtype.DType]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.DType.bits"><code class="name">var <span class="ident">bits</span></code></dt>
<dd>
<div class="desc"><p>Number of bits used to store a single value of this type. See <code><a title="phiml.math.DType.itemsize" href="#phiml.math.DType.itemsize">DType.itemsize</a></code>.</p></div>
</dd>
<dt id="phiml.math.DType.itemsize"><code class="name">prop <span class="ident">itemsize</span></code></dt>
<dd>
<div class="desc"><p>Number of bytes used to storea single value of this type. See <code><a title="phiml.math.DType.bits" href="#phiml.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def itemsize(self):
    &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
    assert self.bits % 8 == 0
    return self.bits // 8</code></pre>
</details>
</dd>
<dt id="phiml.math.DType.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"><p>Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str)</p></div>
</dd>
<dt id="phiml.math.DType.precision"><code class="name">prop <span class="ident">precision</span></code></dt>
<dd>
<div class="desc"><p>Floating point precision. Only defined if <code>kind in (float, complex)</code>. For complex values, returns half of <code><a title="phiml.math.DType.bits" href="#phiml.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self):
    &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
    if self.kind == float:
        return self.bits
    if self.kind == complex:
        return self.bits // 2
    else:
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phiml.math.Dict"><code class="flex name class">
<span>class <span class="ident">Dict</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Dictionary of <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> values.
Dicts are not themselves tensors and do not have a shape.
Use <code><a title="phiml.math.layout" href="#phiml.math.layout">layout()</a></code> to treat <code>dict</code> instances like tensors.</p>
<p>In addition to dictionary functions, supports mathematical operators with other <code><a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a></code>s and lookup via <code>.key</code> syntax.
<code><a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a></code> implements <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code> so instances can be passed to math operations like <code><a title="phiml.math.sin" href="#phiml.math.sin">sin()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dict(dict):
    &#34;&#34;&#34;
    Dictionary of `Tensor` or `phiml.math.magic.PhiTreeNode` values.
    Dicts are not themselves tensors and do not have a shape.
    Use `layout()` to treat `dict` instances like tensors.

    In addition to dictionary functions, supports mathematical operators with other `Dict`s and lookup via `.key` syntax.
    `Dict` implements `phiml.math.magic.PhiTreeNode` so instances can be passed to math operations like `sin`.
    &#34;&#34;&#34;

    def __value_attrs__(self):
        return tuple(self.keys())
    
    # --- Dict[key] ---

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError as k:
            raise AttributeError(k)

    def __setattr__(self, key, value):
        self[key] = value

    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError as k:
            raise AttributeError(k)
        
    # --- operators ---
    
    def __neg__(self):
        return Dict({k: -v for k, v in self.items()})
    
    def __invert__(self):
        return Dict({k: ~v for k, v in self.items()})
    
    def __abs__(self):
        return Dict({k: abs(v) for k, v in self.items()})
    
    def __round__(self, n=None):
        return Dict({k: round(v) for k, v in self.items()})

    def __add__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val + other[key] for key, val in self.items()})
        else:
            return Dict({key: val + other for key, val in self.items()})

    def __radd__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] + val for key, val in self.items()})
        else:
            return Dict({key: other + val for key, val in self.items()})

    def __sub__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val - other[key] for key, val in self.items()})
        else:
            return Dict({key: val - other for key, val in self.items()})

    def __rsub__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] - val for key, val in self.items()})
        else:
            return Dict({key: other - val for key, val in self.items()})

    def __mul__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val * other[key] for key, val in self.items()})
        else:
            return Dict({key: val * other for key, val in self.items()})

    def __rmul__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] * val for key, val in self.items()})
        else:
            return Dict({key: other * val for key, val in self.items()})

    def __truediv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val / other[key] for key, val in self.items()})
        else:
            return Dict({key: val / other for key, val in self.items()})

    def __rtruediv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] / val for key, val in self.items()})
        else:
            return Dict({key: other / val for key, val in self.items()})

    def __floordiv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val // other[key] for key, val in self.items()})
        else:
            return Dict({key: val // other for key, val in self.items()})

    def __rfloordiv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] // val for key, val in self.items()})
        else:
            return Dict({key: other // val for key, val in self.items()})

    def __pow__(self, power, modulo=None):
        assert modulo is None
        if isinstance(power, Dict):
            return Dict({key: val ** power[key] for key, val in self.items()})
        else:
            return Dict({key: val ** power for key, val in self.items()})

    def __rpow__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] ** val for key, val in self.items()})
        else:
            return Dict({key: other ** val for key, val in self.items()})

    def __mod__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val % other[key] for key, val in self.items()})
        else:
            return Dict({key: val % other for key, val in self.items()})

    def __rmod__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] % val for key, val in self.items()})
        else:
            return Dict({key: other % val for key, val in self.items()})

    def __eq__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val == other[key] for key, val in self.items()})
        else:
            return Dict({key: val == other for key, val in self.items()})

    def __ne__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val != other[key] for key, val in self.items()})
        else:
            return Dict({key: val != other for key, val in self.items()})

    def __lt__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &lt; other[key] for key, val in self.items()})
        else:
            return Dict({key: val &lt; other for key, val in self.items()})

    def __le__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &lt;= other[key] for key, val in self.items()})
        else:
            return Dict({key: val &lt;= other for key, val in self.items()})

    def __gt__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &gt; other[key] for key, val in self.items()})
        else:
            return Dict({key: val &gt; other for key, val in self.items()})

    def __ge__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &gt;= other[key] for key, val in self.items()})
        else:
            return Dict({key: val &gt;= other for key, val in self.items()})

    # --- overridden methods ---

    def copy(self):
        return Dict(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.Dict.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>D.copy() -&gt; a shallow copy of D</p></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.Diverged"><code class="flex name class">
<span>class <span class="ident">Diverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised if the optimization was stopped prematurely and cannot continue.
This may indicate that no solution exists.</p>
<p>The values of the last estimate <code>x</code> may or may not be finite.</p>
<p>This exception inherits from <code><a title="phiml.math.ConvergenceException" href="#phiml.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Diverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised if the optimization was stopped prematurely and cannot continue.
    This may indicate that no solution exists.

    The values of the last estimate `x` may or may not be finite.

    This exception inherits from `ConvergenceException`.

    See Also:
        `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phiml.math._optimize.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phiml.math.IncompatibleShapes"><code class="flex name class">
<span>class <span class="ident">IncompatibleShapes</span></span>
<span>(</span><span>message, *shapes: phiml.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Raised when the shape of a tensor does not match the other arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IncompatibleShapes(Exception):
    &#34;&#34;&#34;
    Raised when the shape of a tensor does not match the other arguments.
    &#34;&#34;&#34;
    def __init__(self, message, *shapes: Shape):
        Exception.__init__(self, message)
        self.shapes = shapes</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phiml.math.LinearFunction"><code class="flex name class">
<span>class <span class="ident">LinearFunction</span></span>
</code></dt>
<dd>
<div class="desc"><p>Just-in-time compiled linear function of <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> arguments and return values.</p>
<p>Use <code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear()</a></code> to create a linear function representation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFunction(Generic[X, Y], Callable[[X], Y]):
    &#34;&#34;&#34;
    Just-in-time compiled linear function of `Tensor` arguments and return values.

    Use `jit_compile_linear()` to create a linear function representation.
    &#34;&#34;&#34;

    def __init__(self, f, auxiliary_args: Set[str], forget_traces: bool):
        self.f = f
        self.f_params = function_parameters(f)
        self.auxiliary_args = auxiliary_args
        self.forget_traces = forget_traces
        self.matrices_and_biases: Dict[SignatureKey, Tuple[SparseCoordinateTensor, Tensor, Tuple]] = {}
        self.nl_jit = JitFunction(f, self.auxiliary_args, forget_traces)  # for backends that do not support sparse matrices

    def _get_or_trace(self, key: SignatureKey, args: tuple, f_kwargs: dict):
        if not key.tracing and key in self.matrices_and_biases:
            return self.matrices_and_biases[key]
        else:
            if self.forget_traces:
                self.matrices_and_biases.clear()
            _TRACING_LINEAR.append(self)
            try:
                matrix, bias, raw_out = matrix_from_function(self.f, *args, **f_kwargs, auto_compress=True, _return_raw_output=True)
            finally:
                assert _TRACING_LINEAR.pop(-1) is self
            if not key.tracing:
                self.matrices_and_biases[key] = matrix, bias, raw_out
                if len(self.matrices_and_biases) &gt;= 4:
                    warnings.warn(f&#34;&#34;&#34;Φ-ML-lin: The compiled linear function &#39;{f_name(self.f)}&#39; was traced {len(self.matrices_and_biases)} times.
Performing many traces may be slow and cause memory leaks.
Tensors in auxiliary arguments (all except the first parameter unless specified otherwise) are compared by reference, not by tensor values.
Auxiliary arguments: {key.auxiliary_kwargs}
Multiple linear traces can be avoided by jit-compiling the code that calls the linear function or setting forget_traces=True.&#34;&#34;&#34;, RuntimeWarning, stacklevel=3)
            return matrix, bias, raw_out

    def __call__(self, *args: X, **kwargs) -&gt; Y:
        try:
            key, tensors, natives, x, aux_kwargs = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        except LinearTraceInProgress:
            return self.f(*args, **kwargs)
        assert tensors, &#34;Linear function requires at least one argument&#34;
        if any(isinstance(t, ShiftLinTracer) for t in tensors):
            # TODO: if t is identity, use cached ShiftLinTracer, otherwise multiply two ShiftLinTracers
            return self.f(*args, **kwargs)
        if not key.backend.supports(Backend.sparse_coo_tensor):  # This might be called inside a Jax linear solve
            # warnings.warn(f&#34;Sparse matrices are not supported by {backend}. Falling back to regular jit compilation.&#34;, RuntimeWarning)
            if not math.all_available(*tensors):  # avoid nested tracing, Typical case jax.scipy.sparse.cg(LinearFunction). Nested traces cannot be reused which results in lots of traces per cg.
                ML_LOGGER.debug(f&#34;Φ-ML-lin: Running &#39;{f_name(self.f)}&#39; as-is with {key.backend} because it is being traced.&#34;)
                return self.f(*args, **kwargs)
            else:
                return self.nl_jit(*args, **kwargs)
        matrix, bias, (out_tree, out_tensors) = self._get_or_trace(key, args, aux_kwargs)
        result = matrix @ tensors[0] + bias
        out_tensors = list(out_tensors)
        out_tensors[0] = result
        return assemble_tree(out_tree, out_tensors)

    def sparse_matrix(self, *args, **kwargs):
        &#34;&#34;&#34;
        Create an explicit representation of this linear function as a sparse matrix.

        See Also:
            `sparse_matrix_and_bias()`.

        Args:
            *args: Function arguments. This determines the size of the matrix.
            **kwargs: Additional keyword arguments for the linear function.

        Returns:
            Sparse matrix representation with `values` property and `native()` method.
        &#34;&#34;&#34;
        key, *_, aux_kwargs = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        matrix, bias, *_ = self._get_or_trace(key, args, aux_kwargs)
        assert math.close(bias, 0), &#34;This is an affine function and cannot be represented by a single matrix. Use sparse_matrix_and_bias() instead.&#34;
        return matrix

    def sparse_matrix_and_bias(self, *args, **kwargs):
        &#34;&#34;&#34;
        Create an explicit representation of this affine function as a sparse matrix and a bias vector.

        Args:
            *args: Positional arguments to the linear function.
                This determines the size of the matrix.
            **kwargs: Additional keyword arguments for the linear function.

        Returns:
            matrix: Sparse matrix representation with `values` property and `native()` method.
            bias: `Tensor`
        &#34;&#34;&#34;
        key, *_, aux_kwargs = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        return self._get_or_trace(key, args, aux_kwargs)[:2]

    def __repr__(self):
        return f&#34;lin({f_name(self.f)})&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>collections.abc.Callable</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.LinearFunction.sparse_matrix"><code class="name flex">
<span>def <span class="ident">sparse_matrix</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create an explicit representation of this linear function as a sparse matrix.</p>
<p>See Also:
<code>sparse_matrix_and_bias()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Function arguments. This determines the size of the matrix.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments for the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sparse matrix representation with <code>values</code> property and <code><a title="phiml.math.native" href="#phiml.math.native">native()</a></code> method.</p></div>
</dd>
<dt id="phiml.math.LinearFunction.sparse_matrix_and_bias"><code class="name flex">
<span>def <span class="ident">sparse_matrix_and_bias</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create an explicit representation of this affine function as a sparse matrix and a bias vector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Positional arguments to the linear function.
This determines the size of the matrix.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments for the linear function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matrix</code></dt>
<dd>Sparse matrix representation with <code>values</code> property and <code><a title="phiml.math.native" href="#phiml.math.native">native()</a></code> method.</dd>
<dt><code>bias</code></dt>
<dd><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.NotConverged"><code class="flex name class">
<span>class <span class="ident">NotConverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.</p>
<p>This exception inherits from <code><a title="phiml.math.ConvergenceException" href="#phiml.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NotConverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.

    This exception inherits from `ConvergenceException`.

    See Also:
        `Diverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phiml.math._optimize.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phiml.math.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
</code></dt>
<dd>
<div class="desc"><p>Shapes enumerate dimensions, each consisting of a name, size and type.</p>
<p>There are five types of dimensions: <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.dual" href="#phiml.math.dual">dual()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, and <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>.</p>
<p>To construct a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>, use <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.dual" href="#phiml.math.dual">dual()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> or <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, depending on the desired dimension type.
To create a shape with multiple types, use <code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>The <code>__init__</code> constructor is for internal use only.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape:
    &#34;&#34;&#34;
    Shapes enumerate dimensions, each consisting of a name, size and type.

    There are five types of dimensions: `batch`, `dual`, `spatial`, `channel`, and `instance`.
    &#34;&#34;&#34;

    def __init__(self, sizes: tuple, names: tuple, types: tuple, item_names: tuple):
        &#34;&#34;&#34;
        To construct a `Shape`, use `batch`, `dual`, `spatial`, `channel` or `instance`, depending on the desired dimension type.
        To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

        The `__init__` constructor is for internal use only.
        &#34;&#34;&#34;
        if len(sizes) &gt; 0 and any(s is not None and not isinstance(s, int) for s in sizes):
            from ._tensors import Tensor
            sizes = tuple([s if isinstance(s, Tensor) or s is None else int(s) for s in sizes])  # TODO replace this by an assert
        self.sizes: tuple = sizes
        &#34;&#34;&#34;
        Ordered dimension sizes as `tuple`.
        The size of a dimension can be an `int` or a `Tensor` for [non-uniform shapes](https://tum-pbs.github.io/PhiML/Non_Uniform.html).
        
        See Also:
            `Shape.get_size()`, `Shape.size`, `Shape.shape`.
        &#34;&#34;&#34;
        self.names: Tuple[str, ...] = names
        &#34;&#34;&#34;
        Ordered dimension names as `tuple[str]`.
        
        See Also:
            `Shape.name`.
        &#34;&#34;&#34;
        self.types: Tuple[str, ...] = types  # undocumented, may be private
        self.item_names: Tuple[Optional[Tuple[str, ...]], ...] = (None,) * len(sizes) if item_names is None else item_names  # undocumented
        if DEBUG_CHECKS:
            assert len(sizes) == len(names) == len(types) == len(item_names), f&#34;sizes={sizes}, names={names}, types={types}, item_names={item_names}&#34;
            assert len(set(names)) == len(names), f&#34;Duplicate dimension names: {names}&#34;
            assert all(isinstance(n, str) for n in names), f&#34;All names must be of type string but got {names}&#34;
            assert isinstance(self.item_names, tuple)
            assert all([items is None or isinstance(items, tuple) for items in self.item_names])
            assert all([items is None or all([isinstance(n, str) for n in items]) for items in self.item_names])
            from ._tensors import Tensor
            for name, size in zip(names, sizes):
                if isinstance(size, Tensor):
                    assert size.rank &gt; 0
            for name, size, item_names in zip(self.names, self.sizes, self.item_names):
                if item_names is not None:
                    try:
                        int(size)
                    except Exception:
                        raise AssertionError(f&#34;When item names are present, the size must be an integer type&#34;)
                    assert len(item_names) == size, f&#34;Number of item names ({len(item_names)}) does not match size {size}&#34;
                    for item_name in item_names:
                        assert item_name, f&#34;Empty item name&#34;
                    assert len(set(item_names)) == len(item_names), f&#34;Duplicate item names in shape {self} at dim &#39;{name}&#39;: {item_names}&#34;
            for name, type in zip(names, types):
                if type == DUAL_DIM:
                    assert name.startswith(&#39;~&#39;), f&#34;Dual dimensions must start with &#39;~&#39; but got &#39;{name}&#39; in {self}&#34;

    def _check_is_valid_tensor_shape(self):
        if DEBUG_CHECKS:
            from ._tensors import Tensor
            for name, size in zip(self.names, self.sizes):
                if size is not None and isinstance(size, Tensor):
                    assert size.rank &gt; 0
                    for dim in size.shape.names:
                        assert dim in self.names, f&#34;Dimension {name} varies along {dim} but {dim} is not part of the Shape {self}&#34;

    def _to_dict(self, include_sizes=True):
        result = dict(names=self.names, types=self.types, item_names=self.item_names)
        if include_sizes:
            if not all([isinstance(s, int)] for s in self.sizes):
                raise NotImplementedError()
            result[&#39;sizes&#39;] = self.sizes
        return result

    @staticmethod
    def _from_dict(dict_: dict):
        names = tuple(dict_[&#39;names&#39;])
        sizes = list(dict_[&#39;sizes&#39;]) if &#39;sizes&#39; in dict_ else [None] * len(names)
        item_names = tuple([None if n is None else tuple(n) for n in dict_[&#39;item_names&#39;]])
        for i, n in enumerate(item_names):
            if n and sizes[i] is None:
                sizes[i] = len(n)
        return Shape(tuple(sizes), names, tuple(dict_[&#39;types&#39;]), item_names)

    @property
    def name_list(self):
        return list(self.names)

    @property
    def _named_sizes(self):
        return zip(self.names, self.sizes)

    @property
    def _dimensions(self):
        return zip(self.sizes, self.names, self.types, self.item_names)

    @property
    def untyped_dict(self):
        &#34;&#34;&#34;
        Returns:
            `dict` containing dimension names as keys.
                The values are either the item names as `tuple` if available, otherwise the size.
        &#34;&#34;&#34;
        return {name: self.get_item_names(i) or self.get_size(i) for i, name in enumerate(self.names)}

    def __len__(self):
        return len(self.sizes)

    def __contains__(self, item):
        if isinstance(item, (str, tuple, list)):
            dims = parse_dim_order(item)
            return all(dim in self.names for dim in dims)
        elif isinstance(item, Shape):
            return all([d in self.names for d in item.names])
        else:
            raise ValueError(item)

    def isdisjoint(self, other: Union[&#39;Shape&#39;, tuple, list, str]):
        &#34;&#34;&#34; Shapes are disjoint if all dimension names of one shape do not occur in the other shape. &#34;&#34;&#34;
        other = parse_dim_order(other)
        return not any(dim in self.names for dim in other)

    def __iter__(self):
        return iter(self[i] for i in range(self.rank))

    def index(self, dim: Union[str, &#39;Shape&#39;, None]) -&gt; int:
        &#34;&#34;&#34;
        Finds the index of the dimension within this `Shape`.

        See Also:
            `Shape.indices()`.

        Args:
            dim: Dimension name or single-dimension `Shape`.

        Returns:
            Index as `int`.
        &#34;&#34;&#34;
        if dim is None:
            return None
        elif isinstance(dim, str):
            if dim not in self.names:
                raise ValueError(f&#34;Shape {self} has no dimension &#39;{dim}&#39;&#34;)
            return self.names.index(dim)
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;index() requires a single dimension as input but got {dim}. Use indices() for multiple dimensions.&#34;
            return self.names.index(dim.name)
        else:
            raise ValueError(f&#34;index() requires a single dimension as input but got {dim}&#34;)

    def indices(self, dims: Union[tuple, list, &#39;Shape&#39;]) -&gt; Tuple[int]:
        &#34;&#34;&#34;
        Finds the indices of the given dimensions within this `Shape`.

        See Also:
            `Shape.index()`.

        Args:
            dims: Sequence of dimensions as `tuple`, `list` or `Shape`.

        Returns:
            Indices as `tuple[int]`.
        &#34;&#34;&#34;
        if isinstance(dims, (list, tuple, set)):
            return tuple([self.index(n) for n in dims if n in self.names])
        elif isinstance(dims, Shape):
            return tuple([self.index(n) for n in dims.names if n in self.names])
        else:
            raise ValueError(f&#34;indices() requires a sequence of dimensions but got {dims}&#34;)

    def get_size(self, dim: Union[str, &#39;Shape&#39;, int], default=None):
        &#34;&#34;&#34;
        See Also:
            `Shape.get_sizes()`, `Shape.size`

        Args:
            dim: Dimension, either as name `str` or single-dimension `Shape` or index `int`.
            default: (Optional) If the dim does not exist, return this value instead of raising an error.

        Returns:
            Size associated with `dim` as `int` or `Tensor`.
        &#34;&#34;&#34;
        if isinstance(dim, int):
            assert default is None, &#34;Cannot use a default value when passing an int for dim&#34;
            return self.sizes[dim]
        if isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.&#34;
            dim = dim.name
        if isinstance(dim, str):
            if dim not in self.names:
                if default is None:
                    raise KeyError(f&#34;get_size() failed because &#39;{dim}&#39; is not part of Shape {self} and no default value was provided&#34;)
                else:
                    return default
            return self.sizes[self.names.index(dim)]
        else:
            raise ValueError(f&#34;get_size() requires a single dim name but got {dim}. Use indices() to get multiple sizes.&#34;)

    def get_sizes(self, dims: Union[tuple, list, &#39;Shape&#39;]) -&gt; tuple:
        &#34;&#34;&#34;
        See Also:
            `Shape.get_size()`

        Args:
            dims: Dimensions as `tuple`, `list` or `Shape`.

        Returns:
            `tuple`
        &#34;&#34;&#34;
        assert isinstance(dims, (tuple, list, Shape)), f&#34;get_sizes() requires a sequence of dimensions but got {dims}&#34;
        return tuple([self.get_size(dim) for dim in dims])

    def get_type(self, dim: Union[str, &#39;Shape&#39;]) -&gt; str:
        # undocumented, use get_dim_type() instead.
        if isinstance(dim, str):
            return self.types[self.names.index(dim)]
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
            return self.types[self.names.index(dim.name)]
        else:
            raise ValueError(dim)

    def get_dim_type(self, dim: Union[str, &#39;Shape&#39;]) -&gt; Callable:
        &#34;&#34;&#34;
        Args:
            dim: Dimension, either as name `str` or single-dimension `Shape`.

        Returns:
            Dimension type, one of `batch`, `spatial`, `instance`, `channel`.
        &#34;&#34;&#34;
        return DIM_FUNCTIONS[self.get_type(dim)]

    def get_types(self, dims: Union[tuple, list, &#39;Shape&#39;]) -&gt; tuple:
        # undocumented, do not use
        if isinstance(dims, (tuple, list)):
            return tuple(self.get_type(n) for n in dims)
        elif isinstance(dims, Shape):
            return tuple(self.get_type(n) for n in dims.names)
        else:
            raise ValueError(dims)

    def get_item_names(self, dim: Union[str, &#39;Shape&#39;, int], fallback_spatial=False) -&gt; Union[tuple, None]:
        &#34;&#34;&#34;
        Args:
            fallback_spatial: If `True` and no item names are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as item names along `dim` in the order they are listed in this `Shape`.
            dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.

        Returns:
            Item names as `tuple` or `None` if not defined.
        &#34;&#34;&#34;
        if isinstance(dim, int):
            result = self.item_names[dim]
        elif isinstance(dim, str):
            result = self.item_names[self.index(dim)]
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
            result = self.item_names[self.names.index(dim.name)]
        else:
            raise ValueError(dim)
        if result is not None:
            return result
        elif fallback_spatial and self.spatial_rank == self.get_size(dim) and self.get_type(dim) == CHANNEL_DIM:
            return self.spatial.names
        else:
            return None

    def flipped(self, dims: Union[List[str], Tuple[str]]):
        item_names = list(self.item_names)
        for dim in dims:
            if dim in self.names:
                dim_i_n = self.get_item_names(dim)
                if dim_i_n is not None:
                    item_names[self.index(dim)] = tuple(reversed(dim_i_n))
        return Shape(self.sizes, self.names, self.types, tuple(item_names))

    def __getitem__(self, selection):
        if isinstance(selection, int):
            return Shape((self.sizes[selection],), (self.names[selection],), (self.types[selection],), (self.item_names[selection],))
        elif isinstance(selection, slice):
            return Shape(self.sizes[selection], self.names[selection], self.types[selection], self.item_names[selection])
        elif isinstance(selection, str):
            if &#39;,&#39; in selection:
                selection = [self.index(s.strip()) for s in selection.split(&#39;,&#39;)]
            else:
                selection = self.index(selection)
            return self[selection]
        elif isinstance(selection, Shape):
            selection = selection.names
        if isinstance(selection, (tuple, list)):
            selection = [self.index(s) if isinstance(s, str) else s for s in selection]
            return Shape(tuple([self.sizes[i] for i in selection]), tuple([self.names[i] for i in selection]), tuple([self.types[i] for i in selection]), tuple([self.item_names[i] for i in selection]))
        raise AssertionError(&#34;Can only access shape elements as shape[int], shape[str], shape[slice], shape[Sequence] or shape[Shape]&#34;)

    @property
    def reversed(self):
        return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))

    @property
    def batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]

    @property
    def non_batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]

    @property
    def spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def instance(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the instance dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]

    @property
    def non_instance(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-instance dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]

    @property
    def channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]

    @property
    def non_channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]

    @property
    def dual(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dual dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == DUAL_DIM]]

    @property
    def non_dual(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-dual dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != DUAL_DIM]]

    @property
    def primal(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dual dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t not in [DUAL_DIM, BATCH_DIM]]]

    @property
    def non_primal(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only batch and dual dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t in [DUAL_DIM, BATCH_DIM]]]

    @property
    def transposed(self):
        if self.channel_rank &gt; 0:
            replacement = {DUAL_DIM: CHANNEL_DIM, CHANNEL_DIM: DUAL_DIM}
        elif self.instance_rank &gt; 0:
            replacement = {DUAL_DIM: INSTANCE_DIM, INSTANCE_DIM: DUAL_DIM}
        elif self.spatial_rank &gt; 0:
            replacement = {DUAL_DIM: SPATIAL_DIM, SPATIAL_DIM: DUAL_DIM}
        elif self.dual_rank &gt; 0:
            warnings.warn(f&#34;Transposing {self} is ill-defined because there are not primal dims. Replacing dual dims by channel dims.&#34;, SyntaxWarning)
            replacement = {DUAL_DIM: CHANNEL_DIM}
        else:
            raise ValueError(f&#34;Cannot transpose shape {self} as it has no channel or instance or spatial dims.&#34;)
        return self._with_types(tuple([replacement.get(t, t) for t in self.types]))

    def transpose(self, dims: DimFilter):
        if callable(dims) and dims in TYPE_BY_FUNCTION:
            dims = TYPE_BY_FUNCTION[dims]
            replacement = {DUAL_DIM: dims, dims: DUAL_DIM}
            return self._with_types(tuple([replacement.get(t, t) for t in self.types]))
        dims = self.only(dims)
        return self.replace(dims, dims.transposed)

    @property
    def non_singleton(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
        Dimensions are singleton if their size is exactly `1`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]

    @property
    def singleton(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only singleton dimensions as a new `Shape` object.
        Dimensions are singleton if their size is exactly `1`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, s in enumerate(self.sizes) if _size_equal(s, 1)]]

    def assert_all_sizes_defined(self):
        &#34;&#34;&#34;
        Filters this shape, returning only singleton dimensions as a new `Shape` object.
        Dimensions are singleton if their size is exactly `1`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        for n, s in zip(self.names, self.sizes):
            assert s is not None, f&#34;All sizes must be defined but dim &#39;{n}&#39; is undefined in shape {self}&#34;

    def as_channel(self):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of type *channel*.&#34;&#34;&#34;
        return channel(**self.untyped_dict)

    def as_batch(self):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of type *batch*.&#34;&#34;&#34;
        return batch(**self.untyped_dict)

    def as_spatial(self):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of type *spatial*.&#34;&#34;&#34;
        return spatial(**self.untyped_dict)

    def as_instance(self):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of type *instance*.&#34;&#34;&#34;
        return instance(**self.untyped_dict)

    def as_dual(self):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of type *dual*.&#34;&#34;&#34;
        return dual(**self.untyped_dict)

    def as_type(self, new_type: Callable):
        &#34;&#34;&#34;Returns a copy of this `Shape` with all dimensions of the given type, either `batch`, `dual`, `spatial`, `instance`, or `channel` .&#34;&#34;&#34;
        return new_type(**self.untyped_dict)

    def _more_dual(self):
        return Shape(self.sizes, tuple(&#39;~&#39; + n for n in self.names), (DUAL_DIM,) * len(self.names), self.item_names)

    def _less_dual(self, default_type=&#39;unknown_primal&#39;):
        names = tuple(n[1:] if n.startswith(&#39;~&#39;) else n for n in self.names)
        types = [t if t != DUAL_DIM else (DUAL_DIM if n.startswith(&#39;~~&#39;) else default_type) for n, t in zip(self.names, self.types)]
        return Shape(self.sizes, names, tuple(types), self.item_names)

    def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
        &#34;&#34;&#34;
        Slices this `Shape` along a dimension.
        The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
        https://tum-pbs.github.io/PhiML/Non_Uniform.html

        Args:
            dim: dimension to unstack

        Returns:
            slices of this shape
        &#34;&#34;&#34;
        if dim == &#39;dims&#39;:
            return tuple(Shape((self.sizes[i],), (self.names[i],), (self.types[i],), (self.item_names[i],)) for i in range(self.rank))
        if dim not in self and self.is_uniform:
            return tuple([self])
        from ._tensors import Tensor
        if dim in self:
            inner = self.without(dim)
            dim_size = self.get_size(dim)
        else:
            inner = self
            dim_size = self.shape.get_size(dim)
        sizes = []
        for size in inner.sizes:
            if isinstance(size, Tensor) and dim in size.shape:
                sizes.append(size._unstack(dim))
                dim_size = size.shape.get_size(dim)
            else:
                sizes.append(size)
        assert isinstance(dim_size, int)
        shapes = tuple(Shape(tuple([int(size[i]) if isinstance(size, tuple) else size for size in sizes]), inner.names, inner.types, inner.item_names) for i in range(dim_size))
        return shapes

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the name of the dimension.

        See Also:
            `Shape.names`.
        &#34;&#34;&#34;
        assert self.rank == 1, f&#34;Shape.name is only defined for shapes of rank 1. shape={self}&#34;
        return self.names[0]

    @property
    def size(self):
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the size of the dimension.

        See Also:
            `Shape.sizes`, `Shape.get_size()`.
        &#34;&#34;&#34;
        assert self.rank == 1, f&#34;Shape.size is only defined for shapes of rank 1 but has dims {self}&#34;
        return self.sizes[0]

    @property
    def type(self) -&gt; str:
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the type of the dimension.

        See Also:
            `Shape.get_type()`.
        &#34;&#34;&#34;
        assert self.rank == 1, &#34;Shape.type is only defined for shapes of rank 1.&#34;
        return self.types[0]

    @property
    def dim_type(self):
        types = set(self.types)
        assert len(types) == 1, f&#34;Shape contains multiple types: {self}&#34;
        return DIM_FUNCTIONS[next(iter(types))]

    def __int__(self):
        assert self.rank == 1, &#34;int(Shape) is only defined for shapes of rank 1.&#34;
        return self.sizes[0]

    def mask(self, names: Union[tuple, list, set, &#39;Shape&#39;]):
        &#34;&#34;&#34;
        Returns a binary sequence corresponding to the names of this Shape.
        A value of 1 means that a dimension of this Shape is contained in `names`.

        Args:
          names: instance of dimension
          names: tuple or list or set: 

        Returns:
          binary sequence

        &#34;&#34;&#34;
        if isinstance(names, str):
            names = [names]
        elif isinstance(names, Shape):
            names = names.names
        mask = [1 if name in names else 0 for name in self.names]
        return tuple(mask)

    def __repr__(self):
        def size_repr(size, items):
            if items is not None:
                items_str = &#34;,&#34;.join(items)
                return items_str if len(items_str) &lt;= 12 else f&#34;{size}:{items[0][:5]}...&#34;
            return size

        strings = [f&#34;{name}{SUPERSCRIPT.get(dim_type, &#39;?&#39;)}={size_repr(size, items)}&#34; for size, name, dim_type, items in self._dimensions]
        return &#39;(&#39; + &#39;, &#39;.join(strings) + &#39;)&#39;

    def __eq__(self, other):
        if not isinstance(other, Shape):
            return False
        if self.names != other.names or self.types != other.types:
            return False
        for size1, size2 in zip(self.sizes, other.sizes):
            equal = size1 == size2
            assert isinstance(equal, (bool, math.Tensor))
            if isinstance(equal, math.Tensor):
                equal = equal.all
            if not equal:
                return False
        for names1, names2 in zip(self.item_names, other.item_names):
            if names1 != names2:
                return False
        return True

    def __ne__(self, other):
        return not self == other

    def __bool__(self):
        return self.rank &gt; 0

    def _reorder(self, names: Union[tuple, list, &#39;Shape&#39;]) -&gt; &#39;Shape&#39;:
        assert len(names) == self.rank
        if isinstance(names, Shape):
            names = names.names
        order = [self.index(n) for n in names]
        return self[order]

    def _order_group(self, names: Union[tuple, list, &#39;Shape&#39;]) -&gt; list:
        &#34;&#34;&#34; Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. &#34;&#34;&#34;
        if isinstance(names, Shape):
            names = names.names
        result = []
        for dim in self.names:
            if dim not in result:
                if dim in names:
                    result.extend(names)
                else:
                    result.append(dim)
        return result

    def __and__(self, other):
        if other is dual:
            return concat_shapes(self, self.primal.as_dual())
        return merge_shapes(self, other)

    def __rand__(self, other):
        if other is dual:
            return concat_shapes(self.primal.as_dual(), self)
        return merge_shapes(self, other)

    def _expand(self, dim: &#39;Shape&#39;, pos=None) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;**Deprecated.** Use `phiml.math.merge_shapes()` or `phiml.math.concat_shapes()` instead. &#34;&#34;&#34;
        if not dim:
            return self
        assert dim.name not in self, f&#34;Cannot expand shape {self} by {dim} because dimension already exists.&#34;
        assert isinstance(dim, Shape) and dim.rank == 1, f&#34;Shape.expand() requires a single dimension as a Shape but got {dim}&#34;
        if pos is None:
            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim.type]]
            if len(same_type_dims) &gt; 0:
                pos = self.index(same_type_dims.names[0])
            else:
                pos = {
                    BATCH_DIM: 0,
                    DUAL_DIM: self.batch_rank,
                    INSTANCE_DIM: self.batch_rank + self.dual_rank,
                    SPATIAL_DIM: self.batch.rank + self.dual_rank + self.instance_rank,
                    CHANNEL_DIM: self.rank + 1
                }[dim.type]
        elif pos &lt; 0:
            pos += self.rank + 1
        sizes = list(self.sizes)
        names = list(self.names)
        types = list(self.types)
        item_names = list(self.item_names)
        sizes.insert(pos, dim.size)
        names.insert(pos, dim.name)
        types.insert(pos, dim.type)
        item_names.insert(pos, dim.item_names[0])
        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))

    def without(self, dims: &#39;DimFilter&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Builds a new shape from this one that is missing all given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is `Shape.only()`.

        Args:
          dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)
          dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

        Returns:
          Shape without specified dimensions
        &#34;&#34;&#34;
        if dims is None:  # subtract none
            return self
        elif callable(dims):
            dims = dims(self)
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] not in parse_dim_order(dims)]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
        if isinstance(dims, (tuple, list, set)) and all([isinstance(d, str) for d in dims]):
            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
        elif isinstance(dims, (tuple, list, set)):
            result = self
            for wo in dims:
                result = result.without(wo)
            return result
        else:
            raise ValueError(dims)

    def only(self, dims: &#39;DimFilter&#39;, reorder=False):
        &#34;&#34;&#34;
        Builds a new shape from this one that only contains the given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.without`.

        Args:
          dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.
          reorder: If `False`, keeps the dimension order as defined in this shape.
            If `True`, reorders the dimensions of this shape to match the order of `dims`.

        Returns:
          Shape containing only specified dimensions

        &#34;&#34;&#34;
        if dims is None:  # keep none
            return EMPTY_SHAPE
        if callable(dims):
            dims = dims(self)
        if isinstance(dims, str):
            dims = parse_dim_order(dims)
        if isinstance(dims, Shape):
            dims = dims.names
        if isinstance(dims, (tuple, list, set)):
            if all(isinstance(d, int) for d in dims):
                if not reorder:
                    dims = tuple(sorted(dims))
                return self[dims]
            dim_names = []
            for d in dims:
                if callable(d):
                    d = d(self)
                if isinstance(d, str):
                    dim_names.append(d)
                elif isinstance(d, Shape):
                    dim_names.extend(d.names)
                else:
                    raise ValueError(f&#34;Format not understood for Shape.only(): {dims}&#34;)
            if reorder:
                dim_names = [d.name if isinstance(d, Shape) else d for d in dim_names]
                assert all(isinstance(d, str) for d in dim_names)
                return self[[self.names.index(d) for d in dim_names if d in self.names]]
            else:
                dim_names = [d.name if isinstance(d, Shape) else d for d in dim_names]
                assert all(isinstance(d, str) for d in dim_names)
                return self[[i for i in range(self.rank) if self.names[i] in dim_names]]
        raise ValueError(dims)

    def is_compatible(self, *others: &#39;Shape&#39;):
        &#34;&#34;&#34;
        Checks if this shape and the others can be broadcast.

        Args:
            others: Other shapes.

        Returns:
            `True` only if all shapes are compatible.
        &#34;&#34;&#34;
        try:
            merge_shapes(self, *others)
            return True
        except IncompatibleShapes:
            return False

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions.
        Equal to `len(shape)`.

        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
        &#34;&#34;&#34;
        return len(self.sizes)

    @property
    def batch_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
        return sum([1 for ty in self.types if ty == BATCH_DIM])

    @property
    def instance_rank(self) -&gt; int:
        return sum([1 for ty in self.types if ty == INSTANCE_DIM])

    @property
    def spatial_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        return sum([1 for ty in self.types if ty == SPATIAL_DIM])

    @property
    def dual_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        return sum([1 for ty in self.types if ty == DUAL_DIM])

    @property
    def channel_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
        return sum([1 for ty in self.types if ty == CHANNEL_DIM])

    @property
    def well_defined(self):
        &#34;&#34;&#34;
        Returns `True` if no dimension size is `None`.

        Shapes with undefined sizes may be used in `phiml.math.tensor()`, `phiml.math.wrap()`, `phiml.math.stack()` or `phiml.math.concat()`.

        To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
        with positional `str` arguments, e.g. `spatial(&#39;x&#39;)`.
        &#34;&#34;&#34;
        for size in self.sizes:
            if size is None:
                return False
        return True

    @property
    def defined(self):
        return self[[i for i, size in enumerate(self.sizes) if size is not None]]

    @property
    def undefined(self):
        return self[[i for i, size in enumerate(self.sizes) if size is None]]

    @property
    def shape(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Higher-order `Shape`.
        The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.

        For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiML/Non_Uniform.html)
        may contain additional dimensions.

        See Also:
            `Shape.is_uniform`.

        Returns:
            `Shape`.
        &#34;&#34;&#34;
        from . import Tensor
        shape = Shape((self.rank,), (&#39;dims&#39;,), (CHANNEL_DIM,), (self.names,))
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    @property
    def is_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is uniform if it all sizes have a single integer value.

        See Also:
            `Shape.is_non_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        from ._tensors import Tensor
        return all(not isinstance(s, Tensor) for s in self.sizes)

    @property
    def is_non_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is non-uniform if the size of any dimension varies along another dimension.

        See Also:
            `Shape.is_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        return not self.is_uniform

    @property
    def non_uniform(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns only the non-uniform dimensions of this shape, i.e. the dimensions whose size varies along another dimension.

        See Also
            `Shape.non_uniform_shape`
        &#34;&#34;&#34;
        from . import Tensor
        indices = [i for i, size in enumerate(self.sizes) if isinstance(size, Tensor) and size.rank &gt; 0]
        return self[indices]

    @property
    def non_uniform_shape(self):
        &#34;&#34;&#34;
        Returns the stack dimensions of non-uniform shapes.
        This is equal to `Shape.shape` excluding the `dims` dimension.

        For example, when stacking `(x=3)` and `(x=2)` along `vector`, the resulting shape is non_uniform.
        Its `non_uniform_shape` is `vector` and its `non_uniform` dimension is `x`.

        See Also
            `Shape.non_uniform`.
        &#34;&#34;&#34;
        from . import Tensor
        shape = EMPTY_SHAPE
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    def with_size(self, size: Union[int, Sequence[str]]):
        &#34;&#34;&#34;
        Only for single-dimension shapes.
        Returns a `Shape` representing this dimension but with a different size.

        See Also:
            `Shape.with_sizes()`.

        Args:
            size: Replacement size for this dimension.

        Returns:
            `Shape`
        &#34;&#34;&#34;
        assert self.rank == 1, &#34;Shape.with_size() is only defined for shapes of rank 1.&#34;
        return self.with_sizes([size])

    def with_sizes(self, sizes: Union[Sequence[int], Sequence[Tuple[str, ...]], &#39;Shape&#39;, int], keep_item_names=True):
        &#34;&#34;&#34;
        Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.

        See Also:
            `Shape.with_size()`.

        Args:
            sizes: One of

                * `tuple` / `list` of same length as `self` containing replacement sizes or replacement item names.
                * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.
                * `int`: new size for all dimensions

            keep_item_names: If `False`, forgets all item names.
                If `True`, keeps item names where the size does not change.

        Returns:
            `Shape` with same names and types as `self`.
        &#34;&#34;&#34;
        if isinstance(sizes, (int, str)):
            sizes = [sizes] * len(self.sizes)
        if isinstance(sizes, Shape):
            item_names = [sizes.get_item_names(dim) if dim in sizes else self.get_item_names(dim) for dim in self.names]
            sizes = [sizes.get_size(dim) if dim in sizes else s for dim, s in self._named_sizes]
            return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
        else:
            assert len(sizes) == len(self.sizes), f&#34;Failed to set sizes of Shape {self} to {sizes} because rank does not match.&#34;
            sizes_ = []
            item_names = []
            for i, obj in enumerate(sizes):
                new_size, new_item_names = Shape._size_and_item_names_from_obj(obj, self.sizes[i], self.item_names[i], keep_item_names)
                sizes_.append(new_size)
                item_names.append(new_item_names)
            return Shape(tuple(sizes_), self.names, self.types, tuple(item_names))

    @staticmethod
    def _size_and_item_names_from_obj(obj, prev_size, prev_item_names, keep_item_names=True):
        if isinstance(obj, str):
            obj = [s.strip() for s in obj.split(&#39;,&#39;)]
        if isinstance(obj, (tuple, list)):
            return len(obj), tuple(obj)
        elif isinstance(obj, Number):
            return obj, prev_item_names if keep_item_names and (prev_size is None or _size_equal(obj, prev_size)) else None
        elif isinstance(obj, math.Tensor) or obj is None:
            return obj, None
        elif isinstance(obj, Shape):
            return obj.rank, obj.names
        else:
            raise ValueError(f&#34;sizes can only contain int, str or Tensor but got {type(obj)}&#34;)

    def without_sizes(self):
        &#34;&#34;&#34;
        Returns:
            `Shape` with all sizes undefined (`None`)
        &#34;&#34;&#34;
        return Shape((None,) * self.rank, self.names, self.types, (None,) * self.rank)

    def _replace_single_size(self, dim: str, size: int, keep_item_names: bool = False):
        new_sizes = list(self.sizes)
        new_sizes[self.index(dim)] = size
        return self.with_sizes(new_sizes, keep_item_names=keep_item_names)

    def with_dim_size(self, dim: Union[str, &#39;Shape&#39;], size: Union[int, &#39;math.Tensor&#39;, str, tuple, list], keep_item_names=True):
        &#34;&#34;&#34;
        Returns a new `Shape` that has a different size for `dim`.

        Args:
            dim: Dimension for which to replace the size, `Shape` or `str`.
            size: New size, `int` or `Tensor`

        Returns:
            `Shape` with same names and types as `self`.
        &#34;&#34;&#34;
        if isinstance(dim, Shape):
            dim = dim.name
        assert isinstance(dim, str)
        new_size, new_item_names = Shape._size_and_item_names_from_obj(size, self.get_size(dim), self.get_item_names(dim), keep_item_names)
        return self.replace(dim, Shape((new_size,), (dim,), (self.get_type(dim),), (new_item_names,)), keep_item_names=keep_item_names)

    def _with_names(self, names: Union[str, tuple, list]):
        if isinstance(names, str):
            names = parse_dim_names(names, self.rank)
            names = [n if n is not None else o for n, o in zip(names, self.names)]
        return Shape(self.sizes, tuple(names), self.types, self.item_names)

    def _replace_names_and_types(self,
                                 dims: Union[&#39;Shape&#39;, str, tuple, list],
                                 new: Union[&#39;Shape&#39;, str, tuple, list]) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a copy of `self` with `dims` replaced by `new`.
        Dimensions that are not present in `self` are ignored.

        The dimension order is preserved.

        Args:
            dims: Dimensions to replace.
            new: New dimensions, must have same length as `dims`.
                If a `Shape` is given, replaces the dimension types and item names as well.

        Returns:
            `Shape` with same rank and dimension order as `self`.
        &#34;&#34;&#34;
        dims = parse_dim_order(dims)
        sizes = [math.rename_dims(s, dims, new) if isinstance(s, math.Tensor) else s for s in self.sizes]
        new = parse_dim_order(new) if isinstance(new, str) else new
        names = list(self.names)
        types = list(self.types)
        item_names = list(self.item_names)
        for old_name, new_dim in zip(dims, new):
            if old_name in self:
                if isinstance(new_dim, Shape):
                    names[self.index(old_name)] = new_dim.name
                    types[self.index(old_name)] = new_dim.type
                    item_names[self.index(old_name)] = new_dim.item_names[0]
                else:
                    names[self.index(old_name)] = _apply_prefix(new_dim, types[self.index(old_name)])
        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))

    def replace(self, dims: Union[&#39;Shape&#39;, str, tuple, list], new: &#39;Shape&#39;, keep_item_names=True, replace_item_names: DimFilter = None) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a copy of `self` with `dims` replaced by `new`.
        Dimensions that are not present in `self` are ignored.

        The dimension order is preserved.

        Args:
            dims: Dimensions to replace.
            new: New dimensions, must have same length as `dims`.
                If a `Shape` is given, replaces the dimension types and item names as well.
            keep_item_names: Keeps existing item names for dimensions where `new` does not specify item names if the new dimension has the same size.
            replace_item_names: For which dims the item names should be replaced as well.

        Returns:
            `Shape` with same rank and dimension order as `self`.
        &#34;&#34;&#34;
        dims = parse_dim_order(dims)
        assert isinstance(new, Shape), f&#34;new must be a Shape but got {new}&#34;
        names = list(self.names)
        sizes = list(self.sizes)
        types = list(self.types)
        item_names = list(self.item_names)
        for i in self.indices(self.only(replace_item_names)):
            if item_names[i]:
                if len(new) &gt; len(dims):
                    raise NotImplementedError
                else:
                    name_map = {d: n for d, n in zip(dims, new.names)}
                    item_names[i] = tuple([name_map.get(n, n) for n in item_names[i]])
        if len(new) &gt; len(dims):  # Put all in one spot
            assert len(dims) == 1, &#34;Cannot replace 2+ dims by more replacements&#34;
            index = self.index(dims[0])
            return concat_shapes(self[:index], new, self[index+1:])
        for old_name, new_dim in zip(dims, new):
            if old_name in self:
                names[self.index(old_name)] = new_dim.name
                types[self.index(old_name)] = new_dim.type
                if new_dim.item_names[0]:
                    item_names[self.index(old_name)] = new_dim.item_names[0]
                elif not _size_equal(new_dim.size, self.get_size(old_name)) or not keep_item_names:
                    item_names[self.index(old_name)] = None  # forget previous item names
                sizes[self.index(old_name)] = new_dim.size
        replaced = Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
        if len(new) == len(dims):
            return replaced
        to_remove = dims[-(len(dims) - len(new)):]
        return replaced.without(to_remove)

    def _with_types(self, types: Union[&#39;Shape&#39;, str, Tuple[str, ...], List[str]]):
        &#34;&#34;&#34;
        Only for internal use.
        Note: This method does not rename dimensions to comply with type requirements (e.g. ~ for dual dims).
        &#34;&#34;&#34;
        if isinstance(types, Shape):
            types = tuple([types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])
        elif isinstance(types, str):
            types = (types,) * self.rank
        elif isinstance(types, (tuple, list)):
            types = tuple(types)
        else:
            raise ValueError(types)
        names = tuple([_apply_prefix(name, t) for name, t in zip(self.names, types)])
        return Shape(self.sizes, names, types, self.item_names)

    def _with_item_names(self, item_names: tuple):
        return Shape(self.sizes, self.names, self.types, item_names)

    def _with_item_name(self, dim: str, item_name: tuple):
        if dim not in self:
            return self
        item_names = list(self.item_names)
        item_names[self.index(dim)] = item_name
        return Shape(self.sizes, self.names, self.types, tuple(item_names))

    def _perm(self, names: Tuple[str]) -&gt; List[int]:
        assert len(set(names)) == len(names), f&#34;No duplicates allowed but got {names}&#34;
        assert len(names) &gt;= len(self.names), f&#34;Cannot find permutation for {self} given {names} because names {set(self.names) - set(names)} are missing&#34;
        assert len(names) &lt;= len(self.names), f&#34;Cannot find permutation for {self} given {names} because too many names were passed: {names}&#34;
        perm = [self.names.index(name) for name in names]
        return perm

    @property
    def volume(self) -&gt; Union[int, None]:
        &#34;&#34;&#34;
        Returns the total number of values contained in a tensor of this shape.
        This is the product of all dimension sizes.

        Returns:
            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
        &#34;&#34;&#34;
        result = 1
        for size in self.sizes:
            if size is None:
                return None
            result *= size
        from ._tensors import Tensor
        if not isinstance(result, Tensor):
            return result
        result /= self.non_uniform_shape.volume  # We summed up the items -&gt; undo multiplication
        return int(result.sum)

    @property
    def is_empty(self) -&gt; bool:
        &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
        return len(self.sizes) == 0

    def after_pad(self, widths: dict) -&gt; &#39;Shape&#39;:
        sizes = list(self.sizes)
        item_names = list(self.item_names)
        for dim, (lo, up) in widths.items():
            if dim in self.names:
                sizes[self.index(dim)] += lo + up
                item_names[self.index(dim)] = None
        return Shape(tuple(sizes), self.names, self.types, tuple(item_names))

    def prepare_gather(self, dim: str, selection: Union[slice, int, &#39;Shape&#39;, str, tuple, list]) -&gt; Union[slice, List[int]]:
        &#34;&#34;&#34;
        Parse a slice object for a specific dimension.

        Args:
            dim: Name of dimension to slice.
            selection: Slice object.

        Returns:

        &#34;&#34;&#34;
        if isinstance(selection, Shape):
            selection = selection.name if selection.rank == 1 else selection.names
        if isinstance(selection, str) and &#39;,&#39; in selection:
            selection = parse_dim_order(selection)
        if isinstance(selection, str):  # single item name
            item_names = self.get_item_names(dim, fallback_spatial=True)
            assert item_names is not None, f&#34;No item names defined for dim &#39;{dim}&#39; in tensor {self.shape} and dimension size does not match spatial rank.&#34;
            assert selection in item_names, f&#34;Accessing tensor.{dim}[&#39;{selection}&#39;] failed. Item names are {item_names}.&#34;
            selection = item_names.index(selection)
        if isinstance(selection, (tuple, list)):
            selection = list(selection)
            if any([isinstance(s, str) for s in selection]):
                item_names = self.get_item_names(dim, fallback_spatial=True)
                for i, s in enumerate(selection):
                    if isinstance(s, str):
                        assert item_names is not None, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed because no item names are present on tensor {self.shape}&#34;
                        assert s in item_names, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed. Item names are {item_names}.&#34;
                        selection[i] = item_names.index(s)
            if not selection:  # empty
                selection = slice(0, 0)
        return selection

    def prepare_renaming_gather(self, dim: str, selection: Union[slice, int, &#39;Shape&#39;, str, tuple, list]):
        if isinstance(selection, str) and &#39;-&gt;&#39; in selection:
            selection, new_names = selection.split(&#39;-&gt;&#39;)
            if new_names == &#39;?&#39;:
                return self.prepare_gather(dim, selection), self[dim]._with_item_names((None,))
            else:
                return self.prepare_gather(dim, selection), self[dim].with_size(new_names)
        else:
            return self.prepare_gather(dim, selection), None

    def resolve_index(self, index: Dict[str, Union[slice, int, &#39;Shape&#39;, str, tuple, list]]) -&gt; Dict[str, Union[slice, int, tuple, list]]:
        &#34;&#34;&#34;
        Replaces item names by the corresponding indices.

        Args:
            index: n-dimensional index or slice.

        Returns:
            Same index but without any reference to item names.
        &#34;&#34;&#34;
        return {dim: self.prepare_gather(dim, s) for dim, s in index.items()}

    def after_gather(self, selection: dict) -&gt; &#39;Shape&#39;:
        from . import Tensor
        if self.is_non_uniform:
            sizes = [(s[selection] if isinstance(s, Tensor) else s) for s in self.sizes]
            sizes = [(int(s) if isinstance(s, Tensor) and s.rank == 0 else s) for s in sizes]
            result = self.with_sizes(sizes)
        else:
            result = self
        for sel_dim, sel in selection.items():
            if sel_dim not in self.names:
                continue
            sel = self.prepare_gather(sel_dim, sel)
            if isinstance(sel, int):
                result = result.without(sel_dim)
            elif isinstance(sel, slice):
                step = int(sel.step) if sel.step is not None else 1
                start = int(sel.start) if sel.start is not None else (0 if step &gt; 0 else self.get_size(sel_dim)-1)
                stop = int(sel.stop) if sel.stop is not None else (self.get_size(sel_dim) if step &gt; 0 else -1)
                if stop &lt; 0 and step &gt; 0:
                    stop += self.get_size(sel_dim)
                    assert stop &gt;= 0
                if start &lt; 0 and step &gt; 0:
                    start += self.get_size(sel_dim)
                    assert start &gt;= 0
                stop = min(stop, self.get_size(sel_dim))
                new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
                if new_size.rank == 0:
                    new_size = int(new_size)  # NumPy array not allowed because not hashable
                result = result._replace_single_size(sel_dim, new_size, keep_item_names=True)
                if step &lt; 0:
                    result = result.flipped([sel_dim])
                if self.get_item_names(sel_dim) is not None:
                    result = result._with_item_name(sel_dim, tuple(self.get_item_names(sel_dim)[sel]))
            elif isinstance(sel, (tuple, list)):
                result = result._replace_single_size(sel_dim, len(sel))
                if self.get_item_names(sel_dim) is not None:
                    result = result._with_item_name(sel_dim, tuple([self.get_item_names(sel_dim)[i] for i in sel]))
            elif isinstance(sel, Tensor):
                if sel.dtype.kind == bool:
                    raise NotImplementedError(&#34;Shape.after_gather(Tensor[bool]) not yet implemented&#34;)
                    # from ._ops import nonzero
                    # sel = nonzero(sel)
                if sel.dtype.kind == int:
                    assert len(selection) == 1, f&#34;When slicing a Shape with Tensor[int], only one sel item is allowed but got {sel}&#34;
                    sel_shape = shape(sel)
                    assert sel_shape.channel_rank == 1 and sel_shape.channel.item_names[0], f&#34;Shape.after_gather(Tensor[int]) requires indices to have a single channel dim with item names but got {sel}&#34;
                    indexed = sel_shape.channel.item_names[0]
                    assert indexed in self, f&#34;All indexed dims {indexed} must be part of sliced Shape {self}&#34;
                    from ._ops import slice_
                    sizes = [slice_(s, sel) for s in self.sizes]
                    return self.with_sizes(sizes).without(indexed) &amp; sel_shape.non_channel
            else:
                raise NotImplementedError(f&#34;{type(sel)} not supported. Only (int, slice) allowed.&#34;)
        return result

    def meshgrid(self, names=False):
        &#34;&#34;&#34;
        Builds a sequence containing all multi-indices within a tensor of this shape.
        All indices are returned as `dict` mapping dimension names to `int` indices.

        The corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.

        This function currently only supports uniform tensors.

        Args:
            names: If `True`, replace indices by their item names if available.

        Returns:
            `dict` iterator.
        &#34;&#34;&#34;
        assert self.is_uniform, f&#34;Shape.meshgrid() is currently not supported for non-uniform tensors, {self}&#34;
        indices = [0] * self.rank
        while True:
            if names:
                yield {dim: (names[index] if names is not None else index) for dim, index, names in zip(self.names, indices, self.item_names)}
            else:
                yield {dim: index for dim, index in zip(self.names, indices)}
            for i in range(self.rank-1, -1, -1):
                indices[i] = (indices[i] + 1) % self.sizes[i]
                if indices[i] != 0:
                    break
            else:
                return

    def first_index(self, names=False):
        return next(iter(self.meshgrid(names=names)))

    def are_adjacent(self, dims: Union[str, tuple, list, set, &#39;Shape&#39;]):
        indices = self.indices(dims)
        return (max(indices) - min(indices)) == len(dims) - 1

    def __add__(self, other):
        if isinstance(other, Shape) and self.isdisjoint(other):
            return concat_shapes(self, other)
        return self._op2(other, lambda s, o: s + o, 0)

    def __radd__(self, other):
        return self._op2(other, lambda s, o: o + s, 0)

    def __sub__(self, other):
        if isinstance(other, (str, Shape, tuple, list, set)) or callable(other):
            return self.without(other)
        return self._op2(other, lambda s, o: s - o, 0)

    def __rsub__(self, other):
        return self._op2(other, lambda s, o: o - s, 0)

    def __mul__(self, other):
        return self._op2(other, lambda s, o: s * o, 1)

    def __rmul__(self, other):
        return self._op2(other, lambda s, o: o * s, 1)

    def _op2(self, other, fun, default: int):
        if isinstance(other, int):
            return Shape(tuple([fun(s, other) for s in self.sizes]), self.names, self.types, (None,) * self.rank)
        else:
            return NotImplemented

    def __hash__(self):
        return hash(self.names)

    @staticmethod
    def __stack__(values, dim: &#39;Shape&#39;, **kwargs):
        return shape_stack(dim, *values)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.Shape.batch"><code class="name">prop <span class="ident">batch</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.batch_rank"><code class="name">prop <span class="ident">batch_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
    return sum([1 for ty in self.types if ty == BATCH_DIM])</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.channel"><code class="name">prop <span class="ident">channel</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the channel dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.channel_rank"><code class="name">prop <span class="ident">channel_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of channel dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
    return sum([1 for ty in self.types if ty == CHANNEL_DIM])</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.defined"><code class="name">prop <span class="ident">defined</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def defined(self):
    return self[[i for i, size in enumerate(self.sizes) if size is not None]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.dim_type"><code class="name">prop <span class="ident">dim_type</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dim_type(self):
    types = set(self.types)
    assert len(types) == 1, f&#34;Shape contains multiple types: {self}&#34;
    return DIM_FUNCTIONS[next(iter(types))]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.dual"><code class="name">prop <span class="ident">dual</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dual dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dual(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dual dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == DUAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.dual_rank"><code class="name">prop <span class="ident">dual_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dual_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    return sum([1 for ty in self.types if ty == DUAL_DIM])</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.instance"><code class="name">prop <span class="ident">instance</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the instance dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def instance(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the instance dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.instance_rank"><code class="name">prop <span class="ident">instance_rank</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def instance_rank(self) -&gt; int:
    return sum([1 for ty in self.types if ty == INSTANCE_DIM])</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.is_empty"><code class="name">prop <span class="ident">is_empty</span> : bool</code></dt>
<dd>
<div class="desc"><p>True if this shape has no dimensions. Equivalent to <code><a title="phiml.math.Shape.rank" href="#phiml.math.Shape.rank">Shape.rank</a></code> <code>== 0</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_empty(self) -&gt; bool:
    &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
    return len(self.sizes) == 0</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.is_non_uniform"><code class="name">prop <span class="ident">is_non_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is non-uniform if the size of any dimension varies along another dimension.</p>
<p>See Also:
<code><a title="phiml.math.Shape.is_uniform" href="#phiml.math.Shape.is_uniform">Shape.is_uniform</a></code>, <code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_non_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is non-uniform if the size of any dimension varies along another dimension.

    See Also:
        `Shape.is_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    return not self.is_uniform</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.is_uniform"><code class="name">prop <span class="ident">is_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is uniform if it all sizes have a single integer value.</p>
<p>See Also:
<code><a title="phiml.math.Shape.is_non_uniform" href="#phiml.math.Shape.is_non_uniform">Shape.is_non_uniform</a></code>, <code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is uniform if it all sizes have a single integer value.

    See Also:
        `Shape.is_non_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    from ._tensors import Tensor
    return all(not isinstance(s, Tensor) for s in self.sizes)</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.name"><code class="name">prop <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the name of the dimension.</p>
<p>See Also:
<code><a title="phiml.math.Shape.names" href="#phiml.math.Shape.names">Shape.names</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the name of the dimension.

    See Also:
        `Shape.names`.
    &#34;&#34;&#34;
    assert self.rank == 1, f&#34;Shape.name is only defined for shapes of rank 1. shape={self}&#34;
    return self.names[0]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.name_list"><code class="name">prop <span class="ident">name_list</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name_list(self):
    return list(self.names)</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension names as <code>tuple[str]</code>.</p>
<p>See Also:
<code><a title="phiml.math.Shape.name" href="#phiml.math.Shape.name">Shape.name</a></code>.</p></div>
</dd>
<dt id="phiml.math.Shape.non_batch"><code class="name">prop <span class="ident">non_batch</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-batch dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_channel"><code class="name">prop <span class="ident">non_channel</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-channel dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_dual"><code class="name">prop <span class="ident">non_dual</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-dual dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_dual(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-dual dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != DUAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_instance"><code class="name">prop <span class="ident">non_instance</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-instance dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_instance(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-instance dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_primal"><code class="name">prop <span class="ident">non_primal</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only batch and dual dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_primal(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only batch and dual dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t in [DUAL_DIM, BATCH_DIM]]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_singleton"><code class="name">prop <span class="ident">non_singleton</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only non-singleton dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.
Dimensions are singleton if their size is exactly <code>1</code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_singleton(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
    Dimensions are singleton if their size is exactly `1`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_spatial"><code class="name">prop <span class="ident">non_spatial</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-spatial dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_uniform"><code class="name">prop <span class="ident">non_uniform</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Returns only the non-uniform dimensions of this shape, i.e. the dimensions whose size varies along another dimension.</p>
<p>See Also
<code><a title="phiml.math.Shape.non_uniform_shape" href="#phiml.math.Shape.non_uniform_shape">Shape.non_uniform_shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_uniform(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns only the non-uniform dimensions of this shape, i.e. the dimensions whose size varies along another dimension.

    See Also
        `Shape.non_uniform_shape`
    &#34;&#34;&#34;
    from . import Tensor
    indices = [i for i, size in enumerate(self.sizes) if isinstance(size, Tensor) and size.rank &gt; 0]
    return self[indices]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.non_uniform_shape"><code class="name">prop <span class="ident">non_uniform_shape</span></code></dt>
<dd>
<div class="desc"><p>Returns the stack dimensions of non-uniform shapes.
This is equal to <code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">Shape.shape</a></code> excluding the <code>dims</code> dimension.</p>
<p>For example, when stacking <code>(x=3)</code> and <code>(x=2)</code> along <code>vector</code>, the resulting shape is non_uniform.
Its <code>non_uniform_shape</code> is <code>vector</code> and its <code>non_uniform</code> dimension is <code>x</code>.</p>
<p>See Also
<code><a title="phiml.math.Shape.non_uniform" href="#phiml.math.Shape.non_uniform">Shape.non_uniform</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_uniform_shape(self):
    &#34;&#34;&#34;
    Returns the stack dimensions of non-uniform shapes.
    This is equal to `Shape.shape` excluding the `dims` dimension.

    For example, when stacking `(x=3)` and `(x=2)` along `vector`, the resulting shape is non_uniform.
    Its `non_uniform_shape` is `vector` and its `non_uniform` dimension is `x`.

    See Also
        `Shape.non_uniform`.
    &#34;&#34;&#34;
    from . import Tensor
    shape = EMPTY_SHAPE
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.primal"><code class="name">prop <span class="ident">primal</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dual dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def primal(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dual dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t not in [DUAL_DIM, BATCH_DIM]]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.rank"><code class="name">prop <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the number of dimensions.
Equal to <code>len(<a title="phiml.math.shape" href="#phiml.math.shape">shape()</a>)</code>.</p>
<p>See <code><a title="phiml.math.Shape.is_empty" href="#phiml.math.Shape.is_empty">Shape.is_empty</a></code>, <code><a title="phiml.math.Shape.batch_rank" href="#phiml.math.Shape.batch_rank">Shape.batch_rank</a></code>, <code><a title="phiml.math.Shape.spatial_rank" href="#phiml.math.Shape.spatial_rank">Shape.spatial_rank</a></code>, <code><a title="phiml.math.Shape.channel_rank" href="#phiml.math.Shape.channel_rank">Shape.channel_rank</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions.
    Equal to `len(shape)`.

    See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
    &#34;&#34;&#34;
    return len(self.sizes)</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.reversed"><code class="name">prop <span class="ident">reversed</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reversed(self):
    return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.shape"><code class="name">prop <span class="ident">shape</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Higher-order <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.
The returned shape will always contain the channel dimension <code>dims</code> with a size equal to the <code><a title="phiml.math.Shape.rank" href="#phiml.math.Shape.rank">Shape.rank</a></code> of this shape.</p>
<p>For uniform shapes, <code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">Shape.shape</a></code> will only contain the dimension <code>dims</code> but the shapes of <a href="https://tum-pbs.github.io/PhiML/Non_Uniform.html">non-uniform shapes</a>
may contain additional dimensions.</p>
<p>See Also:
<code><a title="phiml.math.Shape.is_uniform" href="#phiml.math.Shape.is_uniform">Shape.is_uniform</a></code>.</p>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Higher-order `Shape`.
    The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.

    For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiML/Non_Uniform.html)
    may contain additional dimensions.

    See Also:
        `Shape.is_uniform`.

    Returns:
        `Shape`.
    &#34;&#34;&#34;
    from . import Tensor
    shape = Shape((self.rank,), (&#39;dims&#39;,), (CHANNEL_DIM,), (self.names,))
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.singleton"><code class="name">prop <span class="ident">singleton</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only singleton dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.
Dimensions are singleton if their size is exactly <code>1</code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def singleton(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only singleton dimensions as a new `Shape` object.
    Dimensions are singleton if their size is exactly `1`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, s in enumerate(self.sizes) if _size_equal(s, 1)]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.size"><code class="name">prop <span class="ident">size</span></code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the size of the dimension.</p>
<p>See Also:
<code><a title="phiml.math.Shape.sizes" href="#phiml.math.Shape.sizes">Shape.sizes</a></code>, <code><a title="phiml.math.Shape.get_size" href="#phiml.math.Shape.get_size">Shape.get_size()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self):
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the size of the dimension.

    See Also:
        `Shape.sizes`, `Shape.get_size()`.
    &#34;&#34;&#34;
    assert self.rank == 1, f&#34;Shape.size is only defined for shapes of rank 1 but has dims {self}&#34;
    return self.sizes[0]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.sizes"><code class="name">var <span class="ident">sizes</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension sizes as <code>tuple</code>.
The size of a dimension can be an <code>int</code> or a <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> for <a href="https://tum-pbs.github.io/PhiML/Non_Uniform.html">non-uniform shapes</a>.</p>
<p>See Also:
<code><a title="phiml.math.Shape.get_size" href="#phiml.math.Shape.get_size">Shape.get_size()</a></code>, <code><a title="phiml.math.Shape.size" href="#phiml.math.Shape.size">Shape.size</a></code>, <code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">Shape.shape</a></code>.</p></div>
</dd>
<dt id="phiml.math.Shape.spatial"><code class="name">prop <span class="ident">spatial</span> : <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">Shape.batch</a></code>, <code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">Shape.instance</a></code>, <code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">Shape.channel</a></code>, <code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">Shape.dual</a></code>, <code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">Shape.non_channel</a></code>, <code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">Shape.non_dual</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.dual`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`, `Shape.non_dual`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.spatial_rank"><code class="name">prop <span class="ident">spatial_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    return sum([1 for ty in self.types if ty == SPATIAL_DIM])</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.transposed"><code class="name">prop <span class="ident">transposed</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def transposed(self):
    if self.channel_rank &gt; 0:
        replacement = {DUAL_DIM: CHANNEL_DIM, CHANNEL_DIM: DUAL_DIM}
    elif self.instance_rank &gt; 0:
        replacement = {DUAL_DIM: INSTANCE_DIM, INSTANCE_DIM: DUAL_DIM}
    elif self.spatial_rank &gt; 0:
        replacement = {DUAL_DIM: SPATIAL_DIM, SPATIAL_DIM: DUAL_DIM}
    elif self.dual_rank &gt; 0:
        warnings.warn(f&#34;Transposing {self} is ill-defined because there are not primal dims. Replacing dual dims by channel dims.&#34;, SyntaxWarning)
        replacement = {DUAL_DIM: CHANNEL_DIM}
    else:
        raise ValueError(f&#34;Cannot transpose shape {self} as it has no channel or instance or spatial dims.&#34;)
    return self._with_types(tuple([replacement.get(t, t) for t in self.types]))</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.type"><code class="name">prop <span class="ident">type</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the type of the dimension.</p>
<p>See Also:
<code><a title="phiml.math.Shape.get_type" href="#phiml.math.Shape.get_type">Shape.get_type()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def type(self) -&gt; str:
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the type of the dimension.

    See Also:
        `Shape.get_type()`.
    &#34;&#34;&#34;
    assert self.rank == 1, &#34;Shape.type is only defined for shapes of rank 1.&#34;
    return self.types[0]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.undefined"><code class="name">prop <span class="ident">undefined</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def undefined(self):
    return self[[i for i, size in enumerate(self.sizes) if size is None]]</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.untyped_dict"><code class="name">prop <span class="ident">untyped_dict</span></code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p><code>dict</code> containing dimension names as keys.
The values are either the item names as <code>tuple</code> if available, otherwise the size.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def untyped_dict(self):
    &#34;&#34;&#34;
    Returns:
        `dict` containing dimension names as keys.
            The values are either the item names as `tuple` if available, otherwise the size.
    &#34;&#34;&#34;
    return {name: self.get_item_names(i) or self.get_size(i) for i, name in enumerate(self.names)}</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.volume"><code class="name">prop <span class="ident">volume</span> : Optional[int]</code></dt>
<dd>
<div class="desc"><p>Returns the total number of values contained in a tensor of this shape.
This is the product of all dimension sizes.</p>
<h2 id="returns">Returns</h2>
<p>volume as <code>int</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code>None</code> if the shape is not <code><a title="phiml.math.Shape.well_defined" href="#phiml.math.Shape.well_defined">Shape.well_defined</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def volume(self) -&gt; Union[int, None]:
    &#34;&#34;&#34;
    Returns the total number of values contained in a tensor of this shape.
    This is the product of all dimension sizes.

    Returns:
        volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
    &#34;&#34;&#34;
    result = 1
    for size in self.sizes:
        if size is None:
            return None
        result *= size
    from ._tensors import Tensor
    if not isinstance(result, Tensor):
        return result
    result /= self.non_uniform_shape.volume  # We summed up the items -&gt; undo multiplication
    return int(result.sum)</code></pre>
</details>
</dd>
<dt id="phiml.math.Shape.well_defined"><code class="name">prop <span class="ident">well_defined</span></code></dt>
<dd>
<div class="desc"><p>Returns <code>True</code> if no dimension size is <code>None</code>.</p>
<p>Shapes with undefined sizes may be used in <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code>, <code><a title="phiml.math.stack" href="#phiml.math.stack">stack()</a></code> or <code><a title="phiml.math.concat" href="#phiml.math.concat">concat()</a></code>.</p>
<p>To create an undefined size, call a constructor function (<code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>)
with positional <code>str</code> arguments, e.g. <code>spatial('x')</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def well_defined(self):
    &#34;&#34;&#34;
    Returns `True` if no dimension size is `None`.

    Shapes with undefined sizes may be used in `phiml.math.tensor()`, `phiml.math.wrap()`, `phiml.math.stack()` or `phiml.math.concat()`.

    To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
    with positional `str` arguments, e.g. `spatial(&#39;x&#39;)`.
    &#34;&#34;&#34;
    for size in self.sizes:
        if size is None:
            return False
    return True</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.Shape.after_gather"><code class="name flex">
<span>def <span class="ident">after_gather</span></span>(<span>self, selection: dict) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.after_pad"><code class="name flex">
<span>def <span class="ident">after_pad</span></span>(<span>self, widths: dict) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.are_adjacent"><code class="name flex">
<span>def <span class="ident">are_adjacent</span></span>(<span>self, dims: Union[str, tuple, list, set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')])</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.as_batch"><code class="name flex">
<span>def <span class="ident">as_batch</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of type <em>batch</em>.</p></div>
</dd>
<dt id="phiml.math.Shape.as_channel"><code class="name flex">
<span>def <span class="ident">as_channel</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of type <em>channel</em>.</p></div>
</dd>
<dt id="phiml.math.Shape.as_dual"><code class="name flex">
<span>def <span class="ident">as_dual</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of type <em>dual</em>.</p></div>
</dd>
<dt id="phiml.math.Shape.as_instance"><code class="name flex">
<span>def <span class="ident">as_instance</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of type <em>instance</em>.</p></div>
</dd>
<dt id="phiml.math.Shape.as_spatial"><code class="name flex">
<span>def <span class="ident">as_spatial</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of type <em>spatial</em>.</p></div>
</dd>
<dt id="phiml.math.Shape.as_type"><code class="name flex">
<span>def <span class="ident">as_type</span></span>(<span>self, new_type: Callable)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all dimensions of the given type, either <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.dual" href="#phiml.math.dual">dual()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, or <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code> .</p></div>
</dd>
<dt id="phiml.math.Shape.assert_all_sizes_defined"><code class="name flex">
<span>def <span class="ident">assert_all_sizes_defined</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only singleton dimensions as a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object.
Dimensions are singleton if their size is exactly <code>1</code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> object</p></div>
</dd>
<dt id="phiml.math.Shape.first_index"><code class="name flex">
<span>def <span class="ident">first_index</span></span>(<span>self, names=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.flipped"><code class="name flex">
<span>def <span class="ident">flipped</span></span>(<span>self, dims: Union[List[str], Tuple[str]])</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.get_dim_type"><code class="name flex">
<span>def <span class="ident">get_dim_type</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')]) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as name <code>str</code> or single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dimension type, one of <code><a title="phiml.math.batch" href="#phiml.math.batch">batch()</a></code>, <code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial()</a></code>, <code><a title="phiml.math.instance" href="#phiml.math.instance">instance()</a></code>, <code><a title="phiml.math.channel" href="#phiml.math.channel">channel()</a></code>.</p></div>
</dd>
<dt id="phiml.math.Shape.get_item_names"><code class="name flex">
<span>def <span class="ident">get_item_names</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), int], fallback_spatial=False) ‑> Optional[tuple]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>fallback_spatial</code></strong></dt>
<dd>If <code>True</code> and no item names are defined for <code>dim</code> and <code>dim</code> is a channel dimension, the spatial dimension names are interpreted as item names along <code>dim</code> in the order they are listed in this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as <code>int</code> index, <code>str</code> name or single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Item names as <code>tuple</code> or <code>None</code> if not defined.</p></div>
</dd>
<dt id="phiml.math.Shape.get_size"><code class="name flex">
<span>def <span class="ident">get_size</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), int], default=None)</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phiml.math.Shape.get_sizes" href="#phiml.math.Shape.get_sizes">Shape.get_sizes()</a></code>, <code><a title="phiml.math.Shape.size" href="#phiml.math.Shape.size">Shape.size</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as name <code>str</code> or single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or index <code>int</code>.</dd>
<dt><strong><code>default</code></strong></dt>
<dd>(Optional) If the dim does not exist, return this value instead of raising an error.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Size associated with <code>dim</code> as <code>int</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p></div>
</dd>
<dt id="phiml.math.Shape.get_sizes"><code class="name flex">
<span>def <span class="ident">get_sizes</span></span>(<span>self, dims: Union[tuple, list, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')]) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phiml.math.Shape.get_size" href="#phiml.math.Shape.get_size">Shape.get_size()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions as <code>tuple</code>, <code>list</code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code></p></div>
</dd>
<dt id="phiml.math.Shape.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')]) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.get_types"><code class="name flex">
<span>def <span class="ident">get_types</span></span>(<span>self, dims: Union[tuple, list, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')]) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), None]) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the index of the dimension within this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.Shape.indices" href="#phiml.math.Shape.indices">Shape.indices()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension name or single-dimension <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Index as <code>int</code>.</p></div>
</dd>
<dt id="phiml.math.Shape.indices"><code class="name flex">
<span>def <span class="ident">indices</span></span>(<span>self, dims: Union[tuple, list, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')]) ‑> Tuple[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the indices of the given dimensions within this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</p>
<p>See Also:
<code><a title="phiml.math.Shape.index" href="#phiml.math.Shape.index">Shape.index()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Sequence of dimensions as <code>tuple</code>, <code>list</code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Indices as <code>tuple[int]</code>.</p></div>
</dd>
<dt id="phiml.math.Shape.is_compatible"><code class="name flex">
<span>def <span class="ident">is_compatible</span></span>(<span>self, *others: <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if this shape and the others can be broadcast.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>others</code></strong></dt>
<dd>Other shapes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>True</code> only if all shapes are compatible.</p></div>
</dd>
<dt id="phiml.math.Shape.isdisjoint"><code class="name flex">
<span>def <span class="ident">isdisjoint</span></span>(<span>self, other: Union[ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), tuple, list, str])</span>
</code></dt>
<dd>
<div class="desc"><p>Shapes are disjoint if all dimension names of one shape do not occur in the other shape.</p></div>
</dd>
<dt id="phiml.math.Shape.mask"><code class="name flex">
<span>def <span class="ident">mask</span></span>(<span>self, names: Union[tuple, list, set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')])</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a binary sequence corresponding to the names of this Shape.
A value of 1 means that a dimension of this Shape is contained in <code>names</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>instance of dimension</dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list or set: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>binary sequence</p></div>
</dd>
<dt id="phiml.math.Shape.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self, names=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.
All indices are returned as <code>dict</code> mapping dimension names to <code>int</code> indices.</p>
<p>The corresponding values can be retrieved from Tensors and other Sliceables using <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a>[index]</code>.</p>
<p>This function currently only supports uniform tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>If <code>True</code>, replace indices by their item names if available.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>dict</code> iterator.</p></div>
</dd>
<dt id="phiml.math.Shape.only"><code class="name flex">
<span>def <span class="ident">only</span></span>(<span>self, dims: DimFilter, reorder=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that only contains the given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phiml.math.Shape.without" href="#phiml.math.Shape.without">Shape.without()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.</dd>
<dt><strong><code>reorder</code></strong></dt>
<dd>If <code>False</code>, keeps the dimension order as defined in this shape.
If <code>True</code>, reorders the dimensions of this shape to match the order of <code>dims</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only specified dimensions</p></div>
</dd>
<dt id="phiml.math.Shape.prepare_gather"><code class="name flex">
<span>def <span class="ident">prepare_gather</span></span>(<span>self, dim: str, selection: Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, int, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), str, tuple, list]) ‑> Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, List[int]]</span>
</code></dt>
<dd>
<div class="desc"><p>Parse a slice object for a specific dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Name of dimension to slice.</dd>
<dt><strong><code>selection</code></strong></dt>
<dd>Slice object.</dd>
</dl>
<p>Returns:</p></div>
</dd>
<dt id="phiml.math.Shape.prepare_renaming_gather"><code class="name flex">
<span>def <span class="ident">prepare_renaming_gather</span></span>(<span>self, dim: str, selection: Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, int, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), str, tuple, list])</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.replace"><code class="name flex">
<span>def <span class="ident">replace</span></span>(<span>self, dims: Union[ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), tuple, list, str], new: <a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>, keep_item_names=True, replace_item_names: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None] = None) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of <code>self</code> with <code>dims</code> replaced by <code>new</code>.
Dimensions that are not present in <code>self</code> are ignored.</p>
<p>The dimension order is preserved.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to replace.</dd>
<dt><strong><code>new</code></strong></dt>
<dd>New dimensions, must have same length as <code>dims</code>.
If a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> is given, replaces the dimension types and item names as well.</dd>
<dt><strong><code>keep_item_names</code></strong></dt>
<dd>Keeps existing item names for dimensions where <code>new</code> does not specify item names if the new dimension has the same size.</dd>
<dt><strong><code>replace_item_names</code></strong></dt>
<dd>For which dims the item names should be replaced as well.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with same rank and dimension order as <code>self</code>.</p></div>
</dd>
<dt id="phiml.math.Shape.resolve_index"><code class="name flex">
<span>def <span class="ident">resolve_index</span></span>(<span>self, index: <a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a>[str, Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, int, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), str, tuple, list]]) ‑> <a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a>[str, Union[<a title="phiml.math.slice" href="#phiml.math.slice">slice_()</a>, int, tuple, list]]</span>
</code></dt>
<dd>
<div class="desc"><p>Replaces item names by the corresponding indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index</code></strong></dt>
<dd>n-dimensional index or slice.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same index but without any reference to item names.</p></div>
</dd>
<dt id="phiml.math.Shape.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>self, dims: Union[str, Sequence[+T_co], set, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), Callable, None])</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Shape.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dim='dims') ‑> Tuple[phiml.math._shape.Shape]</span>
</code></dt>
<dd>
<div class="desc"><p>Slices this <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> along a dimension.
The dimension listing the sizes of the shape is referred to as <code>'dims'</code>.</p>
<p>Non-uniform tensor shapes may be unstacked along other dimensions as well, see
<a href="https://tum-pbs.github.io/PhiML/Non_Uniform.html">https://tum-pbs.github.io/PhiML/Non_Uniform.html</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension to unstack</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>slices of this shape</p></div>
</dd>
<dt id="phiml.math.Shape.with_dim_size"><code class="name flex">
<span>def <span class="ident">with_dim_size</span></span>(<span>self, dim: Union[str, ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>')], size: Union[int, ForwardRef('math.Tensor'), str, tuple, list], keep_item_names=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> that has a different size for <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension for which to replace the size, <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> or <code>str</code>.</dd>
<dt><strong><code>size</code></strong></dt>
<dd>New size, <code>int</code> or <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with same names and types as <code>self</code>.</p></div>
</dd>
<dt id="phiml.math.Shape.with_size"><code class="name flex">
<span>def <span class="ident">with_size</span></span>(<span>self, size: Union[int, Sequence[str]])</span>
</code></dt>
<dd>
<div class="desc"><p>Only for single-dimension shapes.
Returns a <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> representing this dimension but with a different size.</p>
<p>See Also:
<code><a title="phiml.math.Shape.with_sizes" href="#phiml.math.Shape.with_sizes">Shape.with_sizes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>Replacement size for this dimension.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></p></div>
</dd>
<dt id="phiml.math.Shape.with_sizes"><code class="name flex">
<span>def <span class="ident">with_sizes</span></span>(<span>self, sizes: Union[Sequence[int], Sequence[Tuple[str, ...]], ForwardRef('<a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a>'), int], keep_item_names=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> matching the dimension names and types of <code>self</code> but with different sizes.</p>
<p>See Also:
<code><a title="phiml.math.Shape.with_size" href="#phiml.math.Shape.with_size">Shape.with_size()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>
<p>One of</p>
<ul>
<li><code>tuple</code> / <code>list</code> of same length as <code>self</code> containing replacement sizes or replacement item names.</li>
<li><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> of any rank. Replaces sizes for dimensions shared by <code>sizes</code> and <code>self</code>.</li>
<li><code>int</code>: new size for all dimensions</li>
</ul>
</dd>
<dt><strong><code>keep_item_names</code></strong></dt>
<dd>If <code>False</code>, forgets all item names.
If <code>True</code>, keeps item names where the size does not change.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with same names and types as <code>self</code>.</p></div>
</dd>
<dt id="phiml.math.Shape.without"><code class="name flex">
<span>def <span class="ident">without</span></span>(<span>self, dims: DimFilter) ‑> phiml.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is <code><a title="phiml.math.Shape.only" href="#phiml.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Single dimension (str) or instance of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to exclude as <code>str</code> or <code>tuple</code> or <code>list</code> or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>. Dimensions that are not included in this shape are ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
</dd>
<dt id="phiml.math.Shape.without_sizes"><code class="name flex">
<span>def <span class="ident">without_sizes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> with all sizes undefined (<code>None</code>)</p></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.Solve"><code class="flex name class">
<span>class <span class="ident">Solve</span></span>
<span>(</span><span>method: Optional[str] = 'auto', rel_tol: Union[float, phiml.math._tensors.Tensor] = None, abs_tol: Union[float, phiml.math._tensors.Tensor] = None, x0: Union[~X, Any] = None, max_iterations: Union[int, phiml.math._tensors.Tensor] = 1000, suppress: Union[tuple, list] = (), preprocess_y: Callable = None, preprocess_y_args: tuple = (), preconditioner: Optional[str] = None, rank_deficiency: int = None, gradient_solve: Optional[ForwardRef('<a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a>[Y, X]')] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Specifies parameters and stopping criteria for solving a minimization problem or system of equations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solve(Generic[X, Y]):
    &#34;&#34;&#34;
    Specifies parameters and stopping criteria for solving a minimization problem or system of equations.
    &#34;&#34;&#34;

    def __init__(self,
                 method: Union[str, None] = &#39;auto&#39;,
                 rel_tol: Union[float, Tensor] = None,
                 abs_tol: Union[float, Tensor] = None,
                 x0: Union[X, Any] = None,
                 max_iterations: Union[int, Tensor] = 1000,
                 suppress: Union[tuple, list] = (),
                 preprocess_y: Callable = None,
                 preprocess_y_args: tuple = (),
                 preconditioner: Optional[str] = None,
                 rank_deficiency: int = None,
                 gradient_solve: Union[&#39;Solve[Y, X]&#39;, None] = None):
        method = method or &#39;auto&#39;
        assert isinstance(method, str)
        self.method: str = method
        &#34;&#34;&#34; Optimization method to use. Available solvers depend on the solve function that is used to perform the solve. &#34;&#34;&#34;
        self.rel_tol: Tensor = math.to_float(wrap(rel_tol)) if rel_tol is not None else None
        &#34;&#34;&#34;Relative tolerance for linear solves only, defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
        This must be unset or `0` for minimization problems.
        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. &#34;&#34;&#34;
        self.abs_tol: Tensor = math.to_float(wrap(abs_tol)) if abs_tol is not None else None
        &#34;&#34;&#34; Absolut tolerance for optimization problems and linear solves.
        Defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
        For systems of equations *f(x)=y*, the final tolerance is `max(rel_tol * norm(y), abs_tol)`. &#34;&#34;&#34;
        self.max_iterations: Tensor = math.to_int32(wrap(max_iterations))
        &#34;&#34;&#34; Maximum number of iterations to perform before raising a `NotConverged` error is raised. &#34;&#34;&#34;
        self.x0 = x0
        &#34;&#34;&#34; Initial guess for the method, of same type and dimensionality as the solve result.
         This property must be set to a value compatible with the solution `x` before running a method. &#34;&#34;&#34;
        self.preprocess_y: Callable = preprocess_y
        &#34;&#34;&#34; Function to be applied to the right-hand-side vector of an equation system before solving the system.
        This property is propagated to gradient solves by default. &#34;&#34;&#34;
        self.preprocess_y_args: tuple = preprocess_y_args
        assert all(issubclass(err, ConvergenceException) for err in suppress)
        self.suppress: tuple = tuple(suppress)
        &#34;&#34;&#34; Error types to suppress; `tuple` of `ConvergenceException` types. For these errors, the solve function will instead return the partial result without raising the error. &#34;&#34;&#34;
        self.preconditioner = preconditioner
        self.rank_deficiency: int = rank_deficiency
        &#34;&#34;&#34;Rank deficiency of matrix or linear function. If not specified, will be determined for (implicit or explicit) matrix solves and assumed 0 for function-based solves.&#34;&#34;&#34;
        self._gradient_solve: Solve[Y, X] = gradient_solve
        self.id = str(uuid.uuid4())  # not altered by copy_with(), so that the lookup SolveTape[Solve] works after solve has been copied

    @property
    def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
        &#34;&#34;&#34;
        Parameters to use for the gradient pass when an implicit gradient is computed.
        If `None`, a duplicate of this `Solve` is created for the gradient solve.

        In any case, the gradient solve information will be stored in `gradient_solve.result`.
        &#34;&#34;&#34;
        if self._gradient_solve is None:
            self._gradient_solve = Solve(self.method, self.rel_tol, self.abs_tol, None, self.max_iterations, self.suppress, self.preprocess_y, self.preprocess_y_args)
        return self._gradient_solve

    def __repr__(self):
        return f&#34;{self.method} with tolerance {self.rel_tol} (rel), {self.abs_tol} (abs), max_iterations={self.max_iterations}&#34; + (&#34; including preprocessing&#34; if self.preprocess_y else &#34;&#34;)

    def __eq__(self, other):
        if not isinstance(other, Solve):
            return False
        if self.method != other.method \
                or not math.equal(self.abs_tol, other.abs_tol) \
                or not math.equal(self.rel_tol, other.rel_tol) \
                or (self.max_iterations != other.max_iterations).any \
                or self.preprocess_y is not other.preprocess_y \
                or self.suppress != other.suppress:
            return False
        return self.x0 == other.x0

    def __variable_attrs__(self):
        return &#39;x0&#39;, &#39;rel_tol&#39;, &#39;abs_tol&#39;, &#39;max_iterations&#39;

    def __value_attrs__(self):
        return self.__variable_attrs__()

    def with_defaults(self, mode: str):
        assert mode in (&#39;solve&#39;, &#39;optimization&#39;)
        result = self
        if result.rel_tol is None:
            result = copy_with(result, rel_tol=_default_tolerance() if mode == &#39;solve&#39; else wrap(0.))
        if result.abs_tol is None:
            result = copy_with(result, abs_tol=_default_tolerance())
        return result

    def with_preprocessing(self, preprocess_y: Callable, *args) -&gt; &#39;Solve&#39;:
        &#34;&#34;&#34;
        Adds preprocessing to this `Solve` and all corresponding gradient solves.

        Args:
            preprocess_y: Preprocessing function.
            *args: Arguments for the preprocessing function.

        Returns:
            Copy of this `Solve` with given preprocessing.
        &#34;&#34;&#34;
        assert self.preprocess_y is None, f&#34;preprocessing for linear solve &#39;{self}&#39; already set&#34;
        gradient_solve = self._gradient_solve.with_preprocessing(preprocess_y, *args) if self._gradient_solve is not None else None
        return copy_with(self, preprocess_y=preprocess_y, preprocess_y_args=args, _gradient_solve=gradient_solve)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.Solve.abs_tol"><code class="name">var <span class="ident">abs_tol</span></code></dt>
<dd>
<div class="desc"><p>Absolut tolerance for optimization problems and linear solves.
Defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(rel_tol * norm(y), abs_tol)</code>.</p></div>
</dd>
<dt id="phiml.math.Solve.gradient_solve"><code class="name">prop <span class="ident">gradient_solve</span> : <a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a>[Y, X]</code></dt>
<dd>
<div class="desc"><p>Parameters to use for the gradient pass when an implicit gradient is computed.
If <code>None</code>, a duplicate of this <code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> is created for the gradient solve.</p>
<p>In any case, the gradient solve information will be stored in <code>gradient_solve.result</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
    &#34;&#34;&#34;
    Parameters to use for the gradient pass when an implicit gradient is computed.
    If `None`, a duplicate of this `Solve` is created for the gradient solve.

    In any case, the gradient solve information will be stored in `gradient_solve.result`.
    &#34;&#34;&#34;
    if self._gradient_solve is None:
        self._gradient_solve = Solve(self.method, self.rel_tol, self.abs_tol, None, self.max_iterations, self.suppress, self.preprocess_y, self.preprocess_y_args)
    return self._gradient_solve</code></pre>
</details>
</dd>
<dt id="phiml.math.Solve.max_iterations"><code class="name">var <span class="ident">max_iterations</span></code></dt>
<dd>
<div class="desc"><p>Maximum number of iterations to perform before raising a <code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code> error is raised.</p></div>
</dd>
<dt id="phiml.math.Solve.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p>Optimization method to use. Available solvers depend on the solve function that is used to perform the solve.</p></div>
</dd>
<dt id="phiml.math.Solve.preprocess_y"><code class="name">var <span class="ident">preprocess_y</span></code></dt>
<dd>
<div class="desc"><p>Function to be applied to the right-hand-side vector of an equation system before solving the system.
This property is propagated to gradient solves by default.</p></div>
</dd>
<dt id="phiml.math.Solve.rank_deficiency"><code class="name">var <span class="ident">rank_deficiency</span></code></dt>
<dd>
<div class="desc"><p>Rank deficiency of matrix or linear function. If not specified, will be determined for (implicit or explicit) matrix solves and assumed 0 for function-based solves.</p></div>
</dd>
<dt id="phiml.math.Solve.rel_tol"><code class="name">var <span class="ident">rel_tol</span></code></dt>
<dd>
<div class="desc"><p>Relative tolerance for linear solves only, defaults to 1e-5 for singe precision solves and 1e-12 for double precision solves.
This must be unset or <code>0</code> for minimization problems.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(rel_tol * norm(y), abs_tol)</code>.</p></div>
</dd>
<dt id="phiml.math.Solve.suppress"><code class="name">var <span class="ident">suppress</span></code></dt>
<dd>
<div class="desc"><p>Error types to suppress; <code>tuple</code> of <code><a title="phiml.math.ConvergenceException" href="#phiml.math.ConvergenceException">ConvergenceException</a></code> types. For these errors, the solve function will instead return the partial result without raising the error.</p></div>
</dd>
<dt id="phiml.math.Solve.x0"><code class="name">var <span class="ident">x0</span></code></dt>
<dd>
<div class="desc"><p>Initial guess for the method, of same type and dimensionality as the solve result.
This property must be set to a value compatible with the solution <code>x</code> before running a method.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.Solve.with_defaults"><code class="name flex">
<span>def <span class="ident">with_defaults</span></span>(<span>self, mode: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Solve.with_preprocessing"><code class="name flex">
<span>def <span class="ident">with_preprocessing</span></span>(<span>self, preprocess_y: Callable, *args) ‑> phiml.math._optimize.Solve</span>
</code></dt>
<dd>
<div class="desc"><p>Adds preprocessing to this <code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> and all corresponding gradient solves.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>preprocess_y</code></strong></dt>
<dd>Preprocessing function.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Arguments for the preprocessing function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of this <code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code> with given preprocessing.</p></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.SolveInfo"><code class="flex name class">
<span>class <span class="ident">SolveInfo</span></span>
</code></dt>
<dd>
<div class="desc"><p>Stores information about the solution or trajectory of a solve.</p>
<p>When representing the full optimization trajectory, all tracked quantities will have an additional <code>trajectory</code> batch dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveInfo(Generic[X, Y]):
    &#34;&#34;&#34;
    Stores information about the solution or trajectory of a solve.

    When representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.
    &#34;&#34;&#34;

    def __init__(self,
                 solve: Solve,
                 x: X,
                 residual: Union[Y, None],
                 iterations: Union[Tensor, None],
                 function_evaluations: Union[Tensor, None],
                 converged: Tensor,
                 diverged: Tensor,
                 method: str,
                 msg: Tensor,
                 solve_time: float):
        # tuple.__new__(SolveInfo, (x, residual, iterations, function_evaluations, converged, diverged))
        self.solve: Solve[X, Y] = solve
        &#34;&#34;&#34; `Solve`, Parameters specified for the solve. &#34;&#34;&#34;
        self.x: X = x
        &#34;&#34;&#34; `Tensor` or `phiml.math.magic.PhiTreeNode`, solution estimate. &#34;&#34;&#34;
        self.residual: Y = residual
        &#34;&#34;&#34; `Tensor` or `phiml.math.magic.PhiTreeNode`, residual vector for systems of equations or function value for minimization problems. &#34;&#34;&#34;
        self.iterations: Tensor = iterations
        &#34;&#34;&#34; `Tensor`, number of performed iterations to reach this state. &#34;&#34;&#34;
        self.function_evaluations: Tensor = function_evaluations
        &#34;&#34;&#34; `Tensor`, how often the function (or its gradient function) was called. &#34;&#34;&#34;
        self.converged: Tensor = converged
        &#34;&#34;&#34; `Tensor`, whether the residual is within the specified tolerance. &#34;&#34;&#34;
        self.diverged: Tensor = diverged
        &#34;&#34;&#34; `Tensor`, whether the solve has diverged at this point. &#34;&#34;&#34;
        self.method = method
        &#34;&#34;&#34; `str`, which method and implementation that was used. &#34;&#34;&#34;
        if all_available(diverged, converged, iterations):
            _, res_tensors = disassemble_tree(residual, cache=False)
            msg_fun = partial(_default_solve_info_msg, solve=solve)
            msg = map_(msg_fun, msg, converged.trajectory[-1], diverged.trajectory[-1], iterations.trajectory[-1], method=method, residual=res_tensors[0], dims=converged.shape.without(&#39;trajectory&#39;))
        self.msg = msg
        &#34;&#34;&#34; `str`, termination message &#34;&#34;&#34;
        self.solve_time = solve_time
        &#34;&#34;&#34; Time spent in Backend solve function (in seconds) &#34;&#34;&#34;

    def __repr__(self):
        return f&#34;{self.method}: {self.converged.trajectory[-1].sum} converged, {self.diverged.trajectory[-1].sum} diverged&#34;

    def snapshot(self, index):
        return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index],
                         self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg, self.solve_time)

    def convergence_check(self, only_warn: bool):
        if not all_available(self.diverged, self.converged):
            return
        if self.diverged.any:
            if Diverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg, ConvergenceWarning)
                else:
                    raise Diverged(self)
        if not self.converged.trajectory[-1].all:
            if NotConverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg, ConvergenceWarning)
                else:
                    raise NotConverged(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.SolveInfo.converged"><code class="name">var <span class="ident">converged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, whether the residual is within the specified tolerance.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.diverged"><code class="name">var <span class="ident">diverged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, whether the solve has diverged at this point.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.function_evaluations"><code class="name">var <span class="ident">function_evaluations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, how often the function (or its gradient function) was called.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.iterations"><code class="name">var <span class="ident">iterations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>, number of performed iterations to reach this state.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, which method and implementation that was used.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.msg"><code class="name">var <span class="ident">msg</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, termination message</p></div>
</dd>
<dt id="phiml.math.SolveInfo.residual"><code class="name">var <span class="ident">residual</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>, residual vector for systems of equations or function value for minimization problems.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.solve"><code class="name">var <span class="ident">solve</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code>, Parameters specified for the solve.</p></div>
</dd>
<dt id="phiml.math.SolveInfo.solve_time"><code class="name">var <span class="ident">solve_time</span></code></dt>
<dd>
<div class="desc"><p>Time spent in Backend solve function (in seconds)</p></div>
</dd>
<dt id="phiml.math.SolveInfo.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> or <code><a title="phiml.math.magic.PhiTreeNode" href="magic.html#phiml.math.magic.PhiTreeNode">PhiTreeNode</a></code>, solution estimate.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.SolveInfo.convergence_check"><code class="name flex">
<span>def <span class="ident">convergence_check</span></span>(<span>self, only_warn: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.SolveInfo.snapshot"><code class="name flex">
<span>def <span class="ident">snapshot</span></span>(<span>self, index)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.SolveTape"><code class="flex name class">
<span>class <span class="ident">SolveTape</span></span>
<span>(</span><span>*solves: phiml.math._optimize.Solve, record_trajectories=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Used to record additional information about solves invoked via <code><a title="phiml.math.solve_linear" href="#phiml.math.solve_linear">solve_linear()</a></code>, <code><a title="phiml.math.solve_nonlinear" href="#phiml.math.solve_nonlinear">solve_nonlinear()</a></code> or <code><a title="phiml.math.minimize" href="#phiml.math.minimize">minimize()</a></code>.
While a <code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code> is active, certain performance optimizations and algorithm implementations may be disabled.</p>
<p>To access a <code><a title="phiml.math.SolveInfo" href="#phiml.math.SolveInfo">SolveInfo</a></code> of a recorded solve, use</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; solve = Solve(method, ...)
&gt;&gt;&gt; with SolveTape() as solves:
&gt;&gt;&gt;     x = math.solve_linear(f, y, solve)
&gt;&gt;&gt; result: SolveInfo = solves[solve]  # get by Solve
&gt;&gt;&gt; result: SolveInfo = solves[0]  # get by index
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*solves</code></strong></dt>
<dd>(Optional) Select specific <code>solves</code> to be recorded.
If none is given, records all solves that occur within the scope of this <code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code>.</dd>
<dt><strong><code>record_trajectories</code></strong></dt>
<dd>When enabled, the entries of <code><a title="phiml.math.SolveInfo" href="#phiml.math.SolveInfo">SolveInfo</a></code> will contain an additional batch dimension named <code>trajectory</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveTape:
    &#34;&#34;&#34;
    Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.
    While a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.

    To access a `SolveInfo` of a recorded solve, use
    &gt;&gt;&gt; solve = Solve(method, ...)
    &gt;&gt;&gt; with SolveTape() as solves:
    &gt;&gt;&gt;     x = math.solve_linear(f, y, solve)
    &gt;&gt;&gt; result: SolveInfo = solves[solve]  # get by Solve
    &gt;&gt;&gt; result: SolveInfo = solves[0]  # get by index
    &#34;&#34;&#34;

    def __init__(self, *solves: Solve, record_trajectories=False):
        &#34;&#34;&#34;
        Args:
            *solves: (Optional) Select specific `solves` to be recorded.
                If none is given, records all solves that occur within the scope of this `SolveTape`.
            record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.
        &#34;&#34;&#34;
        self.record_only_ids = [s.id for s in solves]
        self.record_trajectories = record_trajectories
        self.solves: List[SolveInfo] = []

    def should_record_trajectory_for(self, solve: Solve):
        if not self.record_trajectories:
            return False
        if not self.record_only_ids:
            return True
        return solve.id in self.record_only_ids

    def __enter__(self):
        _SOLVE_TAPES.append(self)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        _SOLVE_TAPES.remove(self)

    def _add(self, solve: Solve, trj: bool, result: SolveInfo):
        if any(s.solve.id == solve.id for s in self.solves):
            warnings.warn(&#34;SolveTape contains two results for the same solve settings. SolveTape[solve] will return the first solve result.&#34;, RuntimeWarning)
        if self.record_only_ids and solve.id not in self.record_only_ids:
            return  # this solve should not be recorded
        if self.record_trajectories:
            assert trj, &#34;Solve did not record a trajectory.&#34;
            self.solves.append(result)
        elif trj:
            self.solves.append(result.snapshot(-1))
        else:
            self.solves.append(result)

    def __getitem__(self, item) -&gt; SolveInfo:
        if isinstance(item, int):
            return self.solves[item]
        else:
            assert isinstance(item, Solve)
            solves = [s for s in self.solves if s.solve.id == item.id]
            if len(solves) == 0:
                raise KeyError(f&#34;No solve recorded with key &#39;{item}&#39;.&#34;)
            assert len(solves) == 1
            return solves[0]

    def __iter__(self):
        return iter(self.solves)

    def __len__(self):
        return len(self.solves)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.SolveTape.should_record_trajectory_for"><code class="name flex">
<span>def <span class="ident">should_record_trajectory_for</span></span>(<span>self, solve: phiml.math._optimize.Solve)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="phiml.math.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class to represent structured data of one data type.
This class replaces the native tensor classes <code>numpy.ndarray</code>, <code>torch.Tensor</code>, <code>tensorflow.Tensor</code> or <code>jax.numpy.ndarray</code> as the main data container in Φ-ML.</p>
<p><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> instances are different from native tensors in two important ways:</p>
<ul>
<li>The dimensions of Tensors have <em>names</em> and <em>types</em>.</li>
<li>Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.</li>
</ul>
<p>To check whether a value is a tensor, use <code>isinstance(value, <a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a>)</code>.</p>
<p>To construct a Tensor, use <code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor()</a></code>, <code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap()</a></code> or one of the basic tensor creation functions,
see <a href="https://tum-pbs.github.io/PhiML/Tensors.html">https://tum-pbs.github.io/PhiML/Tensors.html</a> .</p>
<p>Tensors are not editable.
When backed by an editable native tensor, e.g. a <code>numpy.ndarray</code>, do not edit the underlying data structure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
    &#34;&#34;&#34;
    Abstract base class to represent structured data of one data type.
    This class replaces the native tensor classes `numpy.ndarray`, `torch.Tensor`, `tensorflow.Tensor` or `jax.numpy.ndarray` as the main data container in Φ-ML.

    `Tensor` instances are different from native tensors in two important ways:

    * The dimensions of Tensors have *names* and *types*.
    * Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.

    To check whether a value is a tensor, use `isinstance(value, Tensor)`.

    To construct a Tensor, use `phiml.math.tensor()`, `phiml.math.wrap()` or one of the basic tensor creation functions,
    see https://tum-pbs.github.io/PhiML/Tensors.html .

    Tensors are not editable.
    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
    &#34;&#34;&#34;

    def __init__(self):
        if DEBUG_CHECKS:
            self._init_stack = traceback.extract_stack()

    def native(self, order: Union[str, tuple, list, Shape] = None, force_expand=True, to_numpy=False):
        &#34;&#34;&#34;
        Returns a native tensor object with the dimensions ordered according to `order`.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Additionally, groups of dimensions can be specified to pack dims, see `phiml.math.reshaped_native()`.

        Args:
            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.
            force_expand: If `False`, dimensions along which values are guaranteed to be constant will not be expanded to their true size but returned as singleton dimensions.
            to_numpy: Whether to convert the tensor to a NumPy `ndarray`.

        Returns:
            Native tensor representation, such as PyTorch tensor or NumPy array.

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        if isinstance(order, (tuple, list)):
            return reshaped_native(self, order, force_expand=force_expand, to_numpy=to_numpy)
        elif order is None:
            assert self.rank &lt;= 1, f&#34;When calling Tensor.native() or Tensor.numpy(), the dimension order must be specified for Tensors with more than one dimension, e.g. &#39;{&#39;,&#39;.join(self._shape.names)}&#39;. The listed default dimension order can vary depending on the chosen backend. Consider using math.reshaped_native(Tensor) instead.&#34;
            order = self._shape.names
        else:
            order = parse_dim_order(order)
        native = self._transposed_native(order, force_expand)
        return choose_backend(native).numpy(native) if to_numpy else native

    def _transposed_native(self, order: Sequence[str], force_expand: bool):
        raise NotImplementedError(self.__class__)

    def numpy(self, order: Union[str, tuple, list, Shape] = None, force_expand=True) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
        
        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
        To get a differentiable tensor, use `Tensor.native()` instead.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.

        See Also:
            `phiml.math.numpy()`

        Args:
            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.

        Returns:
            NumPy representation

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        return self.native(order, force_expand, to_numpy=True)

    def __array__(self, dtype=None):  # NumPy conversion
        if self.rank &gt; 1:
            warnings.warn(&#34;Automatic conversion of Φ-ML tensors to NumPy can cause problems because the dimension order is not guaranteed.&#34;, SyntaxWarning, stacklevel=3)
        return self.numpy(self._shape)

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):  # NumPy interface
        if len(inputs) != 2:
            return NotImplemented
        if ufunc.__name__ == &#39;multiply&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), &#39;mul&#39;, &#39;*&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), &#39;rmul&#39;, &#39;*&#39;)
        if ufunc.__name__ == &#39;add&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), &#39;add&#39;, &#39;+&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), &#39;radd&#39;, &#39;+&#39;)
        if ufunc.__name__ == &#39;subtract&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), &#39;add&#39;, &#39;-&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), &#39;rsub&#39;, &#39;-&#39;)
        if ufunc.__name__ in [&#39;divide&#39;, &#39;true_divide&#39;]:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), &#39;true_divide&#39;, &#39;/&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), &#39;r_true_divide&#39;, &#39;/&#39;)
        if ufunc.__name__ == &#39;floor_divide&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), &#39;floor_divide&#39;, &#39;//&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), &#39;r_floor_divide&#39;, &#39;//&#39;)
        if ufunc.__name__ == &#39;remainder&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), &#39;remainder&#39;, &#39;%&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), &#39;r_remainder&#39;, &#39;%&#39;)
        if ufunc.__name__ == &#39;power&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), &#39;power&#39;, &#39;**&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), &#39;r_power&#39;, &#39;**&#39;)
        if ufunc.__name__ == &#39;equal&#39;:
            return self.__eq__(inputs[1] if self is inputs[0] else inputs[0])
        if ufunc.__name__ == &#39;not_equal&#39;:
            return self.__ne__(inputs[1] if self is inputs[0] else inputs[0])
        if ufunc.__name__ == &#39;greater&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;greater&#39;, &#39;&gt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &gt; x, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;r_greater&#39;, &#39;&gt;&#39;)
        if ufunc.__name__ == &#39;greater_equal&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;greater_equal&#39;, &#39;&gt;=&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &gt;= x, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;r_greater_equal&#39;, &#39;&gt;=&#39;)
        if ufunc.__name__ == &#39;less&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;less&#39;, &#39;&lt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &lt; x, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;r_less&#39;, &#39;&lt;&#39;)
        if ufunc.__name__ == &#39;less_equal&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;less_equal&#39;, &#39;&lt;=&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &lt;= x, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;r_less_equal&#39;, &#39;&lt;=&#39;)
        if ufunc.__name__ == &#39;left_shift&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &lt;&lt; y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), &#39;left_shift&#39;, &#39;&lt;&lt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &lt;&lt; x, lambda x, y: choose_backend(x, y).shift_bits_left(y, x), &#39;r_left_shift&#39;, &#39;&lt;&lt;&#39;)
        if ufunc.__name__ == &#39;right_shift&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &gt;&gt; y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), &#39;right_shift&#39;, &#39;&gt;&gt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &gt;&gt; x, lambda x, y: choose_backend(x, y).shift_bits_right(y, x), &#39;r_right_shift&#39;, &#39;&gt;&gt;&#39;)
        raise NotImplementedError(f&#34;NumPy function &#39;{ufunc.__name__}&#39; is not compatible with Φ-ML tensors.&#34;)

    @property
    def dtype(self) -&gt; DType:
        &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    @property
    def shape(self) -&gt; Shape:
        &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    @property
    def backend(self) -&gt; Backend:
        from ._ops import choose_backend_t
        return choose_backend_t(self)

    default_backend = backend

    def _with_shape_replaced(self, new_shape: Shape):
        raise NotImplementedError(self.__class__)

    def _with_natives_replaced(self, natives: list):
        &#34;&#34;&#34; Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
        This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
        [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
        [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
        [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
        &#34;&#34;&#34;
        return self.shape.rank

    @property
    def _is_tracer(self) -&gt; bool:
        &#34;&#34;&#34;
        Tracers store additional internal information.
        They should not be converted to `native()` in intermediate operations.
        
        TensorStack prevents performing the actual stack operation if one of its component tensors is special.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _to_dict(self):
        return cached(self)._to_dict()

    def __len__(self):
        return self.shape.volume if self.rank == 1 else NotImplemented

    def __bool__(self):
        assert self.rank == 0, f&#34;Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead.&#34;
        from ._ops import all_
        if not self.default_backend.supports(Backend.jit_compile):  # NumPy
            return bool(self.native()) if self.rank == 0 else bool(all_(self).native())
        else:
            # __bool__ does not work with TensorFlow tracing.
            # TensorFlow needs to see a tf.Tensor in loop conditions but won&#39;t allow bool() invocations.
            # However, this function must always return a Python bool.
            raise AssertionError(&#34;To evaluate the boolean value of a Tensor, use &#39;Tensor.all&#39;.&#34;)

    @property
    def all(self):
        &#34;&#34;&#34; Whether all values of this `Tensor` are `True` as a native bool. &#34;&#34;&#34;
        from ._ops import all_, cast
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return all_(self, dim=self.shape).native()

    @property
    def any(self):
        &#34;&#34;&#34; Whether this `Tensor` contains a `True` value as a native bool. &#34;&#34;&#34;
        from ._ops import any_, cast
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return any_(self, dim=self.shape).native()

    @property
    def mean(self):
        &#34;&#34;&#34; Mean value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import mean
        return mean(self, dim=self.shape).native()

    @property
    def finite_mean(self):
        &#34;&#34;&#34; Mean value of all finite values in this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_mean
        return finite_mean(self, dim=self.shape).native()

    @property
    def std(self):
        &#34;&#34;&#34; Standard deviation of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import std
        return std(self, dim=self.shape).native()

    @property
    def sum(self):
        &#34;&#34;&#34; Sum of all values of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import sum_
        return sum_(self, dim=self.shape).native()

    @property
    def finite_sum(self):
        &#34;&#34;&#34; Sum of all finite values of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_sum
        return finite_sum(self, dim=self.shape).native()

    @property
    def min(self):
        &#34;&#34;&#34; Minimum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import min_
        return min_(self, dim=self.shape).native()

    @property
    def finite_min(self):
        &#34;&#34;&#34; Minimum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_min
        return finite_min(self, dim=self.shape).native()

    @property
    def max(self):
        &#34;&#34;&#34; Maximum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import max_
        return max_(self, dim=self.shape).native()

    @property
    def finite_max(self):
        &#34;&#34;&#34; Maximum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_max
        return finite_max(self, dim=self.shape).native()

    @property
    def real(self) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Returns the real part of this tensor.

        See Also:
            `phiml.math.real()`
        &#34;&#34;&#34;
        from ._ops import real
        return real(self)

    @property
    def imag(self) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Returns the imaginary part of this tensor.
        If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.

        See Also:
            `phiml.math.imag()`
        &#34;&#34;&#34;
        from ._ops import imag
        return imag(self)

    @property
    def available(self) -&gt; bool:
        &#34;&#34;&#34;
        A tensor is available if it stores concrete values and these can currently be read.

        Tracers used inside jit compilation are typically not available.

        See Also:
            `phiml.math.jit_compile()`.
        &#34;&#34;&#34;
        if self._is_tracer:
            return False
        natives = self._natives()
        natives_available = [choose_backend(native).is_available(native) for native in natives]
        return all(natives_available)

    @property
    def device(self) -&gt; Union[ComputeDevice, None]:
        &#34;&#34;&#34;
        Returns the `ComputeDevice` that this tensor is allocated on.
        The device belongs to this tensor&#39;s `default_backend`.

        See Also:
            `Tensor.default_backend`.
        &#34;&#34;&#34;
        natives = self._natives()
        if not natives:
            return None
        return self.default_backend.get_device(natives[0])

    def __int__(self):
        return int(self.native()) if self.shape.volume == 1 else NotImplemented

    def __float__(self):
        return float(self.native()) if self.shape.volume == 1 else NotImplemented

    def __complex__(self):
        return complex(self.native()) if self.shape.volume == 1 else NotImplemented

    def __index__(self):
        assert self.shape.volume == 1, f&#34;Only scalar tensors can be converted to index but has shape {self.shape}&#34;
        assert self.dtype.kind == int, f&#34;Only int tensors can be converted to index but dtype is {self.dtype}&#34;
        return int(self.native())

    def __contains__(self, item):
        if isinstance(item, Shape):
            return item in self.shape
        elif isinstance(item, BoundDim):
            return item.name in self.shape
        elif isinstance(item, _BoundDims):
            return item.dims in self.shape
        elif isinstance(item, str):
            assert self.dtype.kind != object, &#34;str in Tensor not allowed for object-type Tensors&#34;
            return item in self.shape
        raise ValueError(f&#34;&#39;dim in Tensor&#39; requires dim to be a Shape or str but got {item}&#34;)

    def __repr__(self):
        return format_tensor(self, PrintOptions())

    def _repr_pretty_(self, printer, cycle):
        printer.text(format_tensor(self, PrintOptions(colors=DEFAULT_COLORS)))

    def print(self, layout=&#39;full&#39;, float_format=None, threshold=8, include_shape=None, include_dtype=None):
        print(format_tensor(self, PrintOptions(layout=layout, float_format=float_format, threshold=threshold, colors=DEFAULT_COLORS, include_shape=include_shape, include_dtype=include_dtype)))

    def __format__(self, format_spec: str):
        if BROADCAST_FORMATTER.values is not None:
            return BROADCAST_FORMATTER.register_formatted(self, format_spec)
        specs = format_spec.split(&#39;:&#39;)
        layout_ = &#39;auto&#39;
        for possible_layout in [&#39;summary&#39;, &#39;full&#39;, &#39;row&#39;, &#39;numpy&#39;]:
            if possible_layout in specs:
                assert layout_ == &#39;auto&#39;, f&#34;Two layout identifiers encountered in &#39;{format_spec}&#39;&#34;
                layout_ = possible_layout
        include_shape = &#39;shape&#39; in specs or (False if &#39;no-shape&#39; in specs else None)
        include_dtype = &#39;dtype&#39; in specs or (False if &#39;no-dtype&#39; in specs else None)
        color = &#39;color&#39; in specs or (False if &#39;no-color&#39; in specs else None)
        threshold = 8
        float_format = None
        for spec in specs:
            if spec.startswith(&#39;threshold=&#39;):
                threshold = int(spec[len(&#39;threshold=&#39;):])
            elif &#39;.&#39; in spec:
                float_format = spec
        result = format_tensor(self, PrintOptions(layout_, float_format, threshold, color, include_shape, include_dtype))
        return result

    def __getitem__(self, item) -&gt; &#39;Tensor&#39;:
        if isinstance(item, Tensor):
            if item.dtype.kind == bool:
                from ._ops import boolean_mask
                return boolean_mask(self, item.shape.non_batch or item.shape, item)
            elif item.dtype.kind == int:
                from ._ops import gather
                return gather(self, item)
            else:
                raise AssertionError(f&#34;Index tensor must be of dtype int (gather) or bool (boolean_mask) but got {item}&#34;)
        item = slicing_dict(self, item)
        selections = {}
        sliced = self
        for dim, selection in item.items():
            if dim not in self.shape:
                continue
            selection, new_dim = self.shape.prepare_renaming_gather(dim, selection)
            # Either handle slicing directly or add it to the dict
            if isinstance(selection, (tuple, list)):
                result = [sliced[{dim: i}] for i in selection]
                stack_dim = sliced.shape[dim].after_gather({dim: selection})
                sliced = stack(result, stack_dim)
                if new_dim is not None:
                    sliced = rename_dims(sliced, dim, new_dim)
            elif isinstance(selection, Tensor) and selection.dtype.kind == bool:
                from ._ops import boolean_mask
                sliced = boolean_mask(sliced, dim, selection)
            elif isinstance(selection, Tensor) and selection.dtype.kind == int:
                from ._ops import gather
                sliced = gather(sliced, selection, dims=dim)
            else:
                selections[dim] = selection
        return sliced._getitem(selections) if selections else sliced

    def _getitem(self, selection: dict) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Slice the tensor along specified dimensions.

        Args:
          selection: dim_name: str -&gt; Union[int, slice]
          selection: dict: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __setitem__(self, key, value):
        raise SyntaxError(&#34;Tensors are not editable to preserve the autodiff chain. This feature might be added in the future. To update part of a tensor, use math.where() or math.scatter()&#34;)

    def __unstack__(self, dims: Tuple[str, ...]) -&gt; Tuple[&#39;Tensor&#39;, ...]:  # from phiml.math.magic.Sliceable
        if len(dims) == 1:
            return self._unstack(dims[0])
        else:
            return NotImplemented

    def _unstack(self, dim: str):
        &#34;&#34;&#34;
        Splits this tensor along the specified dimension.
        The returned tensors have the same dimensions as this tensor save the unstacked dimension.

        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

        See Also:
            `TensorDim.unstack()`

        Args:
            dim: name of dimension to unstack

        Returns:
            tuple of tensors

        &#34;&#34;&#34;
        raise NotImplementedError()

    @staticmethod
    def __stack__(values: tuple, dim: Shape, **_kwargs) -&gt; &#39;Tensor&#39;:
        if any(isinstance(v, Layout) for v in values):
            layout_ = [v for v in values if isinstance(v, Layout)][0]
            return layout_.__stack__(values, dim, **_kwargs)
        from ._ops import stack_tensors
        return stack_tensors(values, dim)

    def __expand__(self, dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        return expand_tensor(self, dims)

    @staticmethod
    def __concat__(values: tuple, dim: str, **kwargs) -&gt; &#39;Tensor&#39;:
        from ._ops import concat_tensor
        return concat_tensor(values, dim)

    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        return self._with_shape_replaced(rename_dims(self.shape, dims, new_dims))

    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        if self.shape.is_uniform:
            native = self._transposed_native(self.shape.names, True)
            new_shape = self.shape.replace(dim, unpacked_dims)
            if not new_shape.well_defined:
                assert new_shape.undefined.rank &lt;= 1, f&#34;At most one dim can have an undefined size to be inferred during un-packing but got {new_shape}&#34;
                missing = self.shape.volume / new_shape.defined.volume
                sizes = [missing if s is None else s for s in new_shape.sizes]
                new_shape = new_shape.with_sizes(sizes)
            if new_shape.is_uniform:
                native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
                return NativeTensor(native_reshaped, new_shape)
            else:
                split_dim = new_shape.non_uniform_shape[-1]
                i = 0
                result = []
                for idx in split_dim.meshgrid():
                    s = new_shape.after_gather(idx).get_size(new_shape.non_uniform.name)
                    sliced = self[{dim: slice(i, i + s)}]
                    result.append(sliced._with_shape_replaced(sliced.shape.replace(dim, unpacked_dims - split_dim)))
                    i += s
                return stack(result, split_dim)
        else:
            tensors = self._tensors
            if dim == self._stack_dim.name:
                for udim in unpacked_dims:
                    tensors = [TensorStack(tensors[o::len(tensors)//udim.size], udim) for o in range(len(tensors)//udim.size)]
                assert len(tensors) == 1
                return tensors[0]
            raise NotImplementedError

    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: Union[int, None], **kwargs) -&gt; &#39;Tensor&#39;:
        order = self.shape._order_group(dims)
        if self.shape.is_uniform:
            native = self._transposed_native(order, force_expand=True)
            if pos is None:
                pos = min(self.shape.indices(dims))
            new_shape = self.shape.without(dims)._expand(packed_dim.with_sizes([self.shape.only(dims).volume]), pos)
            native = choose_backend(native).reshape(native, new_shape.sizes)
            return NativeTensor(native, new_shape)
        else:
            from ._ops import concat_tensor
            value = cached(self)
            assert isinstance(value, TensorStack)
            inner_packed = [pack_dims(t, dims, packed_dim) for t in value._tensors]
            return concat_tensor(inner_packed, packed_dim.name)

    def __cast__(self, dtype: DType):
        return self._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))

    def dimension(self, name: Union[str, Shape]) -&gt; &#39;TensorDim&#39;:
        &#34;&#34;&#34;
        Returns a reference to a specific dimension of this tensor.
        This is equivalent to the syntax `tensor.&lt;name&gt;`.

        The dimension need not be part of the `Tensor.shape` in which case its size is 1.

        Args:
            name: dimension name

        Returns:
            `TensorDim` corresponding to a dimension of this tensor
        &#34;&#34;&#34;
        if isinstance(name, str):
            return TensorDim(self, name)
        elif isinstance(name, Shape):
            return TensorDim(self, name.name)
        else:
            raise ValueError(name)

    def pack(self, dims, packed_dim):
        &#34;&#34;&#34; See `pack_dims()` &#34;&#34;&#34;
        from ._ops import pack_dims
        return pack_dims(self, dims, packed_dim)

    def unpack(self, dim, unpacked_dims):
        &#34;&#34;&#34; See `unpack_dim()` &#34;&#34;&#34;
        from ._ops import unpack_dim
        return unpack_dim(self, dim, unpacked_dims)

    @property
    def T(self):
        return self._with_shape_replaced(self.shape.transposed)

    @property
    def Ti(self):
        return self._with_shape_replaced(self.shape.transpose(instance))

    @property
    def Tc(self):
        return self._with_shape_replaced(self.shape.transpose(channel))

    @property
    def Ts(self):
        return self._with_shape_replaced(self.shape.transpose(channel))

    def map(self, function: Callable, dims=shape_, range=range, unwrap_scalars=True, **kwargs):
        from ._functional import map_
        return map_(function, self, dims=dims, range=range, unwrap_scalars=unwrap_scalars, **kwargs)

    def __getattr__(self, name):
        if name.startswith(&#39;__&#39;):  # called by hasattr in magic ops
            raise AttributeError
        if name.startswith(&#39;_&#39;):
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        if name == &#39;is_tensor_like&#39;:  # TensorFlow replaces abs() while tracing and checks for this attribute
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        assert name not in (&#39;shape&#39;, &#39;_shape&#39;, &#39;tensor&#39;), name
        return TensorDim(self, name)

    def __add__(self, other):
        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), &#39;add&#39;, &#39;+&#39;)

    def __radd__(self, other):
        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), &#39;radd&#39;, &#39;+&#39;)

    def __sub__(self, other):
            return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), &#39;sub&#39;, &#39;-&#39;)

    def __rsub__(self, other):
        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), &#39;rsub&#39;, &#39;-&#39;)

    def __and__(self, other):
        return self._op2(other, lambda x, y: x &amp; y, lambda x, y: choose_backend(x, y).and_(x, y), &#39;and&#39;, &#39;&amp;&#39;)

    def __rand__(self, other):
        return self._op2(other, lambda x, y: y &amp; x, lambda x, y: choose_backend(x, y).and_(y, x), &#39;rand&#39;, &#39;&amp;&#39;)

    def __or__(self, other):
        return self._op2(other, lambda x, y: x | y, lambda x, y: choose_backend(x, y).or_(x, y), &#39;or&#39;, &#39;|&#39;)

    def __ror__(self, other):
        return self._op2(other, lambda x, y: y | x, lambda x, y: choose_backend(x, y).or_(y, x), &#39;ror&#39;, &#39;|&#39;)

    def __xor__(self, other):
        return self._op2(other, lambda x, y: x ^ y, lambda x, y: choose_backend(x, y).xor(x, y), &#39;xor&#39;, &#39;^&#39;)

    def __rxor__(self, other):
        return self._op2(other, lambda x, y: y ^ x, lambda x, y: choose_backend(x, y).xor(y, x), &#39;rxor&#39;, &#39;^&#39;)

    def __mul__(self, other):
        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), &#39;mul&#39;, &#39;*&#39;)

    def __rmul__(self, other):
        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), &#39;rmul&#39;, &#39;*&#39;)

    def __truediv__(self, other):
        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), &#39;truediv&#39;, &#39;/&#39;)

    def __rtruediv__(self, other):
        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), &#39;rtruediv&#39;, &#39;/&#39;)

    def __divmod__(self, other):
        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y), &#39;divmod&#39;, &#39;divmod&#39;)

    def __rdivmod__(self, other):
        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x), &#39;rdivmod&#39;, &#39;divmod&#39;)

    def __floordiv__(self, other):
        return self._op2(other, lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), &#39;floordiv&#39;, &#39;//&#39;)

    def __rfloordiv__(self, other):
        return self._op2(other, lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), &#39;rfloordiv&#39;, &#39;//&#39;)

    def __pow__(self, power, modulo=None):
        assert modulo is None
        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), &#39;pow&#39;, &#39;**&#39;)

    def __rpow__(self, other):
        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), &#39;rpow&#39;, &#39;**&#39;)

    def __mod__(self, other):
        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), &#39;mod&#39;, &#39;%&#39;)

    def __rmod__(self, other):
        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), &#39;rmod&#39;, &#39;%&#39;)

    def __eq__(self, other) -&gt; &#39;Tensor&#39;:
        if self is other:
            return expand(True, self.shape)
        if _EQUALITY_REDUCE[-1][&#39;type&#39;] == &#39;ref&#39;:
            return wrap(self is other)
        elif _EQUALITY_REDUCE[-1][&#39;type&#39;] == &#39;shape_and_value&#39;:
            if set(self.shape) != set(other.shape):
                return wrap(False)
            from ._ops import close
            return wrap(close(self, other, rel_tolerance=_EQUALITY_REDUCE[-1][&#39;rel_tolerance&#39;], abs_tolerance=_EQUALITY_REDUCE[-1][&#39;abs_tolerance&#39;], equal_nan=_EQUALITY_REDUCE[-1][&#39;equal_nan&#39;]))
        if other is None:
            other = float(&#39;nan&#39;)
        if self.shape.is_compatible(shape(other)):
            return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), &#39;eq&#39;, &#39;==&#39;)
        else:
            return wrap(False)

    def __ne__(self, other) -&gt; &#39;Tensor&#39;:
        if _EQUALITY_REDUCE[-1][&#39;type&#39;] == &#39;ref&#39;:
            return wrap(self is not other)
        elif _EQUALITY_REDUCE[-1][&#39;type&#39;] == &#39;shape_and_value&#39;:
            if set(self.shape) != set(other.shape):
                return wrap(True)
            from ._ops import close
            return wrap(not close(self, other, rel_tolerance=_EQUALITY_REDUCE[-1][&#39;rel_tolerance&#39;], abs_tolerance=_EQUALITY_REDUCE[-1][&#39;abs_tolerance&#39;], equal_nan=_EQUALITY_REDUCE[-1][&#39;equal_nan&#39;]))
        if other is None:
            other = float(&#39;nan&#39;)
        if self.shape.is_compatible(shape(other)):
            return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), &#39;ne&#39;, &#39;!=&#39;)
        else:
            return wrap(True)

    def __lt__(self, other):
        return self._op2(other, lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;lt&#39;, &#39;&lt;&#39;)

    def __le__(self, other):
        return self._op2(other, lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;le&#39;, &#39;&lt;=&#39;)

    def __gt__(self, other):
        return self._op2(other, lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;gt&#39;, &#39;&gt;&#39;)

    def __ge__(self, other):
        return self._op2(other, lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;ge&#39;, &#39;&gt;=&#39;)

    def __lshift__(self, other):
        return self._op2(other, lambda x, y: x &lt;&lt; y, lambda x, y: choose_backend(x, y).shift_bits_left(x, y), &#39;lshift&#39;, &#39;&lt;&lt;&#39;)

    def __rlshift__(self, other):
        return self._op2(other, lambda y, x: x &lt;&lt; y, lambda y, x: choose_backend(x, y).shift_bits_left(x, y), &#39;lshift&#39;, &#39;&lt;&lt;&#39;)

    def __rshift__(self, other):
        return self._op2(other, lambda x, y: x &gt;&gt; y, lambda x, y: choose_backend(x, y).shift_bits_right(x, y), &#39;rshift&#39;, &#39;&gt;&gt;&#39;)

    def __rrshift__(self, other):
        return self._op2(other, lambda y, x: x &gt;&gt; y, lambda y, x: choose_backend(x, y).shift_bits_right(x, y), &#39;rshift&#39;, &#39;&gt;&gt;&#39;)

    def __abs__(self):
        return self._op1(lambda t: choose_backend(t).abs(t))

    def __round__(self, n=None):
        return self._op1(lambda t: choose_backend(t).round(t))

    def __copy__(self):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))

    def __deepcopy__(self, memodict={}):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))

    def __neg__(self) -&gt; &#39;Tensor&#39;:
        return self._op1(lambda t: -t)

    def __invert__(self) -&gt; &#39;Tensor&#39;:
        return self._op1(lambda t: choose_backend(t).invert(t))

    def __reversed__(self):
        assert self.shape.channel.rank == 1
        return self[::-1]

    def __iter__(self):
        if self.rank == 1:
            return iter(self.native())
        elif self.rank == 0:
            return iter([self.native()])
        else:
            native = reshaped_native(self, [self.shape])
            return iter(native)

    def __matmul__(self, other):
        from ._ops import dot
        assert isinstance(other, Tensor), f&#34;Matmul &#39;@&#39; requires two Tensor arguments but got {type(other)}&#34;
        if not self.shape.dual_rank and self.shape.channel_rank:
            match = self.shape.channel.only(other.shape.channel)
            if match:
                return dot(self, match, other, match)
        match_names = self.shape.dual.as_batch().names
        if not match_names:  # this is not a matrix
            assert self.shape.primal.only(other.shape).is_empty, f&#34;Cannot compute matmul {self.shape} @ {other.shape}. First argument is not a matrix; it has no dual dimensions.&#34;
            return self * other
        match_primal = other.shape.only(match_names, reorder=True)
        if not match_primal:
            assert non_batch(other).non_dual.rank == 1, f&#34;Cannot multiply {self.shape} @ {other.shape} because arg2 does not have appropriate non-dual dimensions&#34;
            assert non_batch(other).non_dual.size == match_primal.volume, f&#34;Cannot multiply {self.shape} @ {other.shape} because dual dims of arg1 have no match&#34;
            match_primal = non_batch(other).non_dual
        match_dual = self.shape.dual.only(match_primal.as_dual(), reorder=True)
        left_arg = pack_dims(self, match_dual, dual(&#39;_reduce&#39;))
        right_arg = pack_dims(other, match_primal, channel(&#39;_reduce&#39;))
        return dot(left_arg, &#39;~_reduce&#39;, right_arg, &#39;_reduce&#39;)

    # def __rmatmul__(self, other):

    def _tensor(self, other) -&gt; &#39;Tensor&#39;:
        if isinstance(other, Tensor):
            return other
        elif isinstance(other, (tuple, list)) and any(isinstance(v, Tensor) for v in other):
            if &#39;vector&#39; in self.shape:
                outer_dim = self.shape[&#39;vector&#39;]
            elif self.shape.channel_rank == 1:
                outer_dim = self.shape.channel
            else:
                raise ValueError(f&#34;Cannot combine tensor of shape {self.shape} with tuple {tuple([type(v).__name__ for v in other])}&#34;)
            remaining_shape = self.shape.without(outer_dim)
            other_items = [v if isinstance(v, Tensor) else compatible_tensor(v, compat_shape=remaining_shape, compat_natives=self._natives(), convert=False) for v in other]
            other_stacked = stack(other_items, outer_dim, expand_values=True)
            return other_stacked
        else:
            return compatible_tensor(other, compat_shape=self.shape, compat_natives=self._natives(), convert=False)

    def _op1(self, native_function) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Transform the values of this tensor given a function that can be applied to any native tensor.

        Args:
          native_function:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = &#39;unknown&#39;, op_symbol: str = &#39;?&#39;) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Apply a broadcast operation on two tensors.

        Args:
            other: second argument
            operator: function (Tensor, Tensor) -&gt; Tensor, used to propagate the operation to children tensors to have Python choose the callee
            native_function: function (native tensor, native tensor) -&gt; native tensor
            op_name: Name of the python function without leading and trailing `__`.
                Examples: &#39;add&#39;, &#39;radd&#39;, &#39;sub&#39;, &#39;mul&#39;, &#39;and&#39;, &#39;eq&#39;, &#39;ge&#39;.
            op_symbol: Operation symbol, such as &#39;+&#39;, &#39;-&#39;, &#39;&amp;&#39;, &#39;%&#39;, &#39;&gt;=&#39;

        Returns:
            `Tensor`
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _natives(self) -&gt; tuple:
        raise NotImplementedError(self.__class__)

    def _spec_dict(self) -&gt; dict:
        raise NotImplementedError(self.__class__)

    @classmethod
    def _from_spec_and_natives(cls, spec: dict, natives: list):
        raise NotImplementedError(cls)

    def _simplify(self):
        &#34;&#34;&#34; Does not cache this value but if it is already cached, returns the cached version. &#34;&#34;&#34;
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phiml.math._sparse.CompactSparseTensor</li>
<li>phiml.math._sparse.CompressedSparseMatrix</li>
<li>phiml.math._sparse.SparseCoordinateTensor</li>
<li>phiml.math._tensors.Layout</li>
<li>phiml.math._tensors.NativeTensor</li>
<li>phiml.math._tensors.TensorStack</li>
<li>phiml.math._trace.GatherLinTracer</li>
<li>phiml.math._trace.ShiftLinTracer</li>
<li>phiml.math._trace.SparseLinTracer</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.math.Tensor.T"><code class="name">prop <span class="ident">T</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def T(self):
    return self._with_shape_replaced(self.shape.transposed)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.Tc"><code class="name">prop <span class="ident">Tc</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Tc(self):
    return self._with_shape_replaced(self.shape.transpose(channel))</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.Ti"><code class="name">prop <span class="ident">Ti</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Ti(self):
    return self._with_shape_replaced(self.shape.transpose(instance))</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.Ts"><code class="name">prop <span class="ident">Ts</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Ts(self):
    return self._with_shape_replaced(self.shape.transpose(channel))</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.all"><code class="name">prop <span class="ident">all</span></code></dt>
<dd>
<div class="desc"><p>Whether all values of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> are <code>True</code> as a native bool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def all(self):
    &#34;&#34;&#34; Whether all values of this `Tensor` are `True` as a native bool. &#34;&#34;&#34;
    from ._ops import all_, cast
    if self.rank == 0:
        return cast(self, DType(bool)).native()
    else:
        return all_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.any"><code class="name">prop <span class="ident">any</span></code></dt>
<dd>
<div class="desc"><p>Whether this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> contains a <code>True</code> value as a native bool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def any(self):
    &#34;&#34;&#34; Whether this `Tensor` contains a `True` value as a native bool. &#34;&#34;&#34;
    from ._ops import any_, cast
    if self.rank == 0:
        return cast(self, DType(bool)).native()
    else:
        return any_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.available"><code class="name">prop <span class="ident">available</span> : bool</code></dt>
<dd>
<div class="desc"><p>A tensor is available if it stores concrete values and these can currently be read.</p>
<p>Tracers used inside jit compilation are typically not available.</p>
<p>See Also:
<code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def available(self) -&gt; bool:
    &#34;&#34;&#34;
    A tensor is available if it stores concrete values and these can currently be read.

    Tracers used inside jit compilation are typically not available.

    See Also:
        `phiml.math.jit_compile()`.
    &#34;&#34;&#34;
    if self._is_tracer:
        return False
    natives = self._natives()
    natives_available = [choose_backend(native).is_available(native) for native in natives]
    return all(natives_available)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.backend"><code class="name">prop <span class="ident">backend</span> : phiml.backend._backend.Backend</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def backend(self) -&gt; Backend:
    from ._ops import choose_backend_t
    return choose_backend_t(self)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.default_backend"><code class="name">prop <span class="ident">default_backend</span> : phiml.backend._backend.Backend</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def backend(self) -&gt; Backend:
    from ._ops import choose_backend_t
    return choose_backend_t(self)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.device"><code class="name">prop <span class="ident">device</span> : Optional[phiml.backend._backend.ComputeDevice]</code></dt>
<dd>
<div class="desc"><p>Returns the <code>ComputeDevice</code> that this tensor is allocated on.
The device belongs to this tensor's <code>default_backend</code>.</p>
<p>See Also:
<code><a title="phiml.math.Tensor.default_backend" href="#phiml.math.Tensor.default_backend">Tensor.default_backend</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def device(self) -&gt; Union[ComputeDevice, None]:
    &#34;&#34;&#34;
    Returns the `ComputeDevice` that this tensor is allocated on.
    The device belongs to this tensor&#39;s `default_backend`.

    See Also:
        `Tensor.default_backend`.
    &#34;&#34;&#34;
    natives = self._natives()
    if not natives:
        return None
    return self.default_backend.get_device(natives[0])</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.dtype"><code class="name">prop <span class="ident">dtype</span> : phiml.backend._dtype.DType</code></dt>
<dd>
<div class="desc"><p>Data type of the elements of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self) -&gt; DType:
    &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
    raise NotImplementedError(self.__class__)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.finite_max"><code class="name">prop <span class="ident">finite_max</span></code></dt>
<dd>
<div class="desc"><p>Maximum finite value of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_max(self):
    &#34;&#34;&#34; Maximum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_max
    return finite_max(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.finite_mean"><code class="name">prop <span class="ident">finite_mean</span></code></dt>
<dd>
<div class="desc"><p>Mean value of all finite values in this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_mean(self):
    &#34;&#34;&#34; Mean value of all finite values in this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_mean
    return finite_mean(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.finite_min"><code class="name">prop <span class="ident">finite_min</span></code></dt>
<dd>
<div class="desc"><p>Minimum finite value of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_min(self):
    &#34;&#34;&#34; Minimum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_min
    return finite_min(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.finite_sum"><code class="name">prop <span class="ident">finite_sum</span></code></dt>
<dd>
<div class="desc"><p>Sum of all finite values of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_sum(self):
    &#34;&#34;&#34; Sum of all finite values of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_sum
    return finite_sum(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.imag"><code class="name">prop <span class="ident">imag</span> : <a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dt>
<dd>
<div class="desc"><p>Returns the imaginary part of this tensor.
If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.</p>
<p>See Also:
<code><a title="phiml.math.imag" href="#phiml.math.imag">imag()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def imag(self) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Returns the imaginary part of this tensor.
    If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.

    See Also:
        `phiml.math.imag()`
    &#34;&#34;&#34;
    from ._ops import imag
    return imag(self)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.max"><code class="name">prop <span class="ident">max</span></code></dt>
<dd>
<div class="desc"><p>Maximum value of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def max(self):
    &#34;&#34;&#34; Maximum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import max_
    return max_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.mean"><code class="name">prop <span class="ident">mean</span></code></dt>
<dd>
<div class="desc"><p>Mean value of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean(self):
    &#34;&#34;&#34; Mean value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import mean
    return mean(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.min"><code class="name">prop <span class="ident">min</span></code></dt>
<dd>
<div class="desc"><p>Minimum value of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def min(self):
    &#34;&#34;&#34; Minimum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import min_
    return min_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.rank"><code class="name">prop <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of explicit dimensions of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code>. Equal to <code>tensor.shape.rank</code>.
This replaces <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html"><code>numpy.ndarray.ndim</code></a> /
<a href="https://pytorch.org/docs/master/generated/torch.Tensor.dim.html"><code>torch.Tensor.dim</code></a> /
<a href="https://www.tensorflow.org/api_docs/python/tf/rank"><code>tf.rank()</code></a> /
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html"><code>jax.numpy.ndim()</code></a>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
    This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
    [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
    [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
    [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
    &#34;&#34;&#34;
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.real"><code class="name">prop <span class="ident">real</span> : <a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></dt>
<dd>
<div class="desc"><p>Returns the real part of this tensor.</p>
<p>See Also:
<code><a title="phiml.math.real" href="#phiml.math.real">real()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def real(self) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Returns the real part of this tensor.

    See Also:
        `phiml.math.real()`
    &#34;&#34;&#34;
    from ._ops import real
    return real(self)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.shape"><code class="name">prop <span class="ident">shape</span> : phiml.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>The <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code> lists the dimensions with their sizes, names and types.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; Shape:
    &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
    raise NotImplementedError(self.__class__)</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.std"><code class="name">prop <span class="ident">std</span></code></dt>
<dd>
<div class="desc"><p>Standard deviation of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def std(self):
    &#34;&#34;&#34; Standard deviation of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import std
    return std(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phiml.math.Tensor.sum"><code class="name">prop <span class="ident">sum</span></code></dt>
<dd>
<div class="desc"><p>Sum of all values of this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sum(self):
    &#34;&#34;&#34; Sum of all values of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import sum_
    return sum_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.math.Tensor.dimension"><code class="name flex">
<span>def <span class="ident">dimension</span></span>(<span>self, name: Union[str, phiml.math._shape.Shape]) ‑> phiml.math._tensors.TensorDim</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a reference to a specific dimension of this tensor.
This is equivalent to the syntax <code>tensor.&lt;name&gt;</code>.</p>
<p>The dimension need not be part of the <code><a title="phiml.math.Tensor.shape" href="#phiml.math.Tensor.shape">Tensor.shape</a></code> in which case its size is 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>TensorDim</code> corresponding to a dimension of this tensor</p></div>
</dd>
<dt id="phiml.math.Tensor.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>self, function: Callable, dims=&lt;function shape&gt;, range=builtins.range, unwrap_scalars=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Tensor.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>self, order: Union[phiml.math._shape.Shape, tuple, list, str] = None, force_expand=True, to_numpy=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native tensor object with the dimensions ordered according to <code>order</code>.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>Additionally, groups of dimensions can be specified to pack dims, see <code><a title="phiml.math.reshaped_native" href="#phiml.math.reshaped_native">reshaped_native()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) Order of dimension names as comma-separated string, list or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd>If <code>False</code>, dimensions along which values are guaranteed to be constant will not be expanded to their true size but returned as singleton dimensions.</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the tensor to a NumPy <code>ndarray</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation, such as PyTorch tensor or NumPy array.</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
</dd>
<dt id="phiml.math.Tensor.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, order: Union[phiml.math._shape.Shape, tuple, list, str] = None, force_expand=True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Converts this tensor to a <code>numpy.ndarray</code> with dimensions ordered according to <code>order</code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phiml.math.Tensor.native" href="#phiml.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If this <code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code> is backed by a NumPy array, a reference to this array may be returned.</p>
<p>See Also:
<code><a title="phiml.math.numpy" href="#phiml.math.numpy">numpy_()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) Order of dimension names as comma-separated string, list or <code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
</dd>
<dt id="phiml.math.Tensor.pack"><code class="name flex">
<span>def <span class="ident">pack</span></span>(<span>self, dims, packed_dim)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims()</a></code></p></div>
</dd>
<dt id="phiml.math.Tensor.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>self, layout='full', float_format=None, threshold=8, include_shape=None, include_dtype=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.math.Tensor.unpack"><code class="name flex">
<span>def <span class="ident">unpack</span></span>(<span>self, dim, unpacked_dims)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phiml.math.unpack_dim" href="#phiml.math.unpack_dim">unpack_dim()</a></code></p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phiml" href="../index.html">phiml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phiml.math.extrapolation" href="extrapolation.html">phiml.math.extrapolation</a></code></li>
<li><code><a title="phiml.math.magic" href="magic.html">phiml.math.magic</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="two-column">
<li><code><a title="phiml.math.INF" href="#phiml.math.INF">INF</a></code></li>
<li><code><a title="phiml.math.NAN" href="#phiml.math.NAN">NAN</a></code></li>
<li><code><a title="phiml.math.NUMPY" href="#phiml.math.NUMPY">NUMPY</a></code></li>
<li><code><a title="phiml.math.PI" href="#phiml.math.PI">PI</a></code></li>
<li><code><a title="phiml.math.f" href="#phiml.math.f">f</a></code></li>
<li><code><a title="phiml.math.math" href="#phiml.math.math">math</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phiml.math.abs" href="#phiml.math.abs">abs</a></code></li>
<li><code><a title="phiml.math.abs_square" href="#phiml.math.abs_square">abs_square</a></code></li>
<li><code><a title="phiml.math.all" href="#phiml.math.all">all</a></code></li>
<li><code><a title="phiml.math.all_available" href="#phiml.math.all_available">all_available</a></code></li>
<li><code><a title="phiml.math.always_close" href="#phiml.math.always_close">always_close</a></code></li>
<li><code><a title="phiml.math.angle" href="#phiml.math.angle">angle</a></code></li>
<li><code><a title="phiml.math.any" href="#phiml.math.any">any</a></code></li>
<li><code><a title="phiml.math.arange" href="#phiml.math.arange">arange</a></code></li>
<li><code><a title="phiml.math.arccos" href="#phiml.math.arccos">arccos</a></code></li>
<li><code><a title="phiml.math.arccosh" href="#phiml.math.arccosh">arccosh</a></code></li>
<li><code><a title="phiml.math.arcsin" href="#phiml.math.arcsin">arcsin</a></code></li>
<li><code><a title="phiml.math.arcsinh" href="#phiml.math.arcsinh">arcsinh</a></code></li>
<li><code><a title="phiml.math.arctan" href="#phiml.math.arctan">arctan</a></code></li>
<li><code><a title="phiml.math.arctanh" href="#phiml.math.arctanh">arctanh</a></code></li>
<li><code><a title="phiml.math.argmax" href="#phiml.math.argmax">argmax</a></code></li>
<li><code><a title="phiml.math.argmin" href="#phiml.math.argmin">argmin</a></code></li>
<li><code><a title="phiml.math.as_extrapolation" href="#phiml.math.as_extrapolation">as_extrapolation</a></code></li>
<li><code><a title="phiml.math.assert_close" href="#phiml.math.assert_close">assert_close</a></code></li>
<li><code><a title="phiml.math.at_max" href="#phiml.math.at_max">at_max</a></code></li>
<li><code><a title="phiml.math.at_max_neighbor" href="#phiml.math.at_max_neighbor">at_max_neighbor</a></code></li>
<li><code><a title="phiml.math.at_min" href="#phiml.math.at_min">at_min</a></code></li>
<li><code><a title="phiml.math.at_min_neighbor" href="#phiml.math.at_min_neighbor">at_min_neighbor</a></code></li>
<li><code><a title="phiml.math.b2i" href="#phiml.math.b2i">b2i</a></code></li>
<li><code><a title="phiml.math.batch" href="#phiml.math.batch">batch</a></code></li>
<li><code><a title="phiml.math.boolean_mask" href="#phiml.math.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phiml.math.broadcast" href="#phiml.math.broadcast">broadcast</a></code></li>
<li><code><a title="phiml.math.c2b" href="#phiml.math.c2b">c2b</a></code></li>
<li><code><a title="phiml.math.c2d" href="#phiml.math.c2d">c2d</a></code></li>
<li><code><a title="phiml.math.cast" href="#phiml.math.cast">cast</a></code></li>
<li><code><a title="phiml.math.ceil" href="#phiml.math.ceil">ceil</a></code></li>
<li><code><a title="phiml.math.channel" href="#phiml.math.channel">channel</a></code></li>
<li><code><a title="phiml.math.choose_backend" href="#phiml.math.choose_backend">choose_backend</a></code></li>
<li><code><a title="phiml.math.clip" href="#phiml.math.clip">clip</a></code></li>
<li><code><a title="phiml.math.clip_length" href="#phiml.math.clip_length">clip_length</a></code></li>
<li><code><a title="phiml.math.close" href="#phiml.math.close">close</a></code></li>
<li><code><a title="phiml.math.closest_grid_values" href="#phiml.math.closest_grid_values">closest_grid_values</a></code></li>
<li><code><a title="phiml.math.concat" href="#phiml.math.concat">concat</a></code></li>
<li><code><a title="phiml.math.concat_shapes" href="#phiml.math.concat_shapes">concat_shapes</a></code></li>
<li><code><a title="phiml.math.conjugate" href="#phiml.math.conjugate">conjugate</a></code></li>
<li><code><a title="phiml.math.const_vec" href="#phiml.math.const_vec">const_vec</a></code></li>
<li><code><a title="phiml.math.contains" href="#phiml.math.contains">contains</a></code></li>
<li><code><a title="phiml.math.convert" href="#phiml.math.convert">convert</a></code></li>
<li><code><a title="phiml.math.convolve" href="#phiml.math.convolve">convolve</a></code></li>
<li><code><a title="phiml.math.copy" href="#phiml.math.copy">copy</a></code></li>
<li><code><a title="phiml.math.copy_with" href="#phiml.math.copy_with">copy_with</a></code></li>
<li><code><a title="phiml.math.cos" href="#phiml.math.cos">cos</a></code></li>
<li><code><a title="phiml.math.cosh" href="#phiml.math.cosh">cosh</a></code></li>
<li><code><a title="phiml.math.count_intersections" href="#phiml.math.count_intersections">count_intersections</a></code></li>
<li><code><a title="phiml.math.count_occurrences" href="#phiml.math.count_occurrences">count_occurrences</a></code></li>
<li><code><a title="phiml.math.cpack" href="#phiml.math.cpack">cpack</a></code></li>
<li><code><a title="phiml.math.cross" href="#phiml.math.cross">cross</a></code></li>
<li><code><a title="phiml.math.cross_product" href="#phiml.math.cross_product">cross_product</a></code></li>
<li><code><a title="phiml.math.cumulative_sum" href="#phiml.math.cumulative_sum">cumulative_sum</a></code></li>
<li><code><a title="phiml.math.custom_gradient" href="#phiml.math.custom_gradient">custom_gradient</a></code></li>
<li><code><a title="phiml.math.d2i" href="#phiml.math.d2i">d2i</a></code></li>
<li><code><a title="phiml.math.d2s" href="#phiml.math.d2s">d2s</a></code></li>
<li><code><a title="phiml.math.degrees_to_radians" href="#phiml.math.degrees_to_radians">degrees_to_radians</a></code></li>
<li><code><a title="phiml.math.dense" href="#phiml.math.dense">dense</a></code></li>
<li><code><a title="phiml.math.dim_mask" href="#phiml.math.dim_mask">dim_mask</a></code></li>
<li><code><a title="phiml.math.dot" href="#phiml.math.dot">dot</a></code></li>
<li><code><a title="phiml.math.downsample2x" href="#phiml.math.downsample2x">downsample2x</a></code></li>
<li><code><a title="phiml.math.dpack" href="#phiml.math.dpack">dpack</a></code></li>
<li><code><a title="phiml.math.dtype" href="#phiml.math.dtype">dtype</a></code></li>
<li><code><a title="phiml.math.dual" href="#phiml.math.dual">dual</a></code></li>
<li><code><a title="phiml.math.eigenvalues" href="#phiml.math.eigenvalues">eigenvalues</a></code></li>
<li><code><a title="phiml.math.enable_debug_checks" href="#phiml.math.enable_debug_checks">enable_debug_checks</a></code></li>
<li><code><a title="phiml.math.equal" href="#phiml.math.equal">equal</a></code></li>
<li><code><a title="phiml.math.erf" href="#phiml.math.erf">erf</a></code></li>
<li><code><a title="phiml.math.exp" href="#phiml.math.exp">exp</a></code></li>
<li><code><a title="phiml.math.expand" href="#phiml.math.expand">expand</a></code></li>
<li><code><a title="phiml.math.factor_ilu" href="#phiml.math.factor_ilu">factor_ilu</a></code></li>
<li><code><a title="phiml.math.factorial" href="#phiml.math.factorial">factorial</a></code></li>
<li><code><a title="phiml.math.fft" href="#phiml.math.fft">fft</a></code></li>
<li><code><a title="phiml.math.fftfreq" href="#phiml.math.fftfreq">fftfreq</a></code></li>
<li><code><a title="phiml.math.find_closest" href="#phiml.math.find_closest">find_closest</a></code></li>
<li><code><a title="phiml.math.find_differences" href="#phiml.math.find_differences">find_differences</a></code></li>
<li><code><a title="phiml.math.finite_fill" href="#phiml.math.finite_fill">finite_fill</a></code></li>
<li><code><a title="phiml.math.finite_max" href="#phiml.math.finite_max">finite_max</a></code></li>
<li><code><a title="phiml.math.finite_mean" href="#phiml.math.finite_mean">finite_mean</a></code></li>
<li><code><a title="phiml.math.finite_min" href="#phiml.math.finite_min">finite_min</a></code></li>
<li><code><a title="phiml.math.finite_sum" href="#phiml.math.finite_sum">finite_sum</a></code></li>
<li><code><a title="phiml.math.flatten" href="#phiml.math.flatten">flatten</a></code></li>
<li><code><a title="phiml.math.floor" href="#phiml.math.floor">floor</a></code></li>
<li><code><a title="phiml.math.fourier_laplace" href="#phiml.math.fourier_laplace">fourier_laplace</a></code></li>
<li><code><a title="phiml.math.fourier_poisson" href="#phiml.math.fourier_poisson">fourier_poisson</a></code></li>
<li><code><a title="phiml.math.frequency_loss" href="#phiml.math.frequency_loss">frequency_loss</a></code></li>
<li><code><a title="phiml.math.from_dict" href="#phiml.math.from_dict">from_dict</a></code></li>
<li><code><a title="phiml.math.gather" href="#phiml.math.gather">gather</a></code></li>
<li><code><a title="phiml.math.get_format" href="#phiml.math.get_format">get_format</a></code></li>
<li><code><a title="phiml.math.get_precision" href="#phiml.math.get_precision">get_precision</a></code></li>
<li><code><a title="phiml.math.get_sparsity" href="#phiml.math.get_sparsity">get_sparsity</a></code></li>
<li><code><a title="phiml.math.gradient" href="#phiml.math.gradient">gradient</a></code></li>
<li><code><a title="phiml.math.grid_sample" href="#phiml.math.grid_sample">grid_sample</a></code></li>
<li><code><a title="phiml.math.histogram" href="#phiml.math.histogram">histogram</a></code></li>
<li><code><a title="phiml.math.i2b" href="#phiml.math.i2b">i2b</a></code></li>
<li><code><a title="phiml.math.identity" href="#phiml.math.identity">identity</a></code></li>
<li><code><a title="phiml.math.ifft" href="#phiml.math.ifft">ifft</a></code></li>
<li><code><a title="phiml.math.imag" href="#phiml.math.imag">imag</a></code></li>
<li><code><a title="phiml.math.incomplete_gamma" href="#phiml.math.incomplete_gamma">incomplete_gamma</a></code></li>
<li><code><a title="phiml.math.index_shift" href="#phiml.math.index_shift">index_shift</a></code></li>
<li><code><a title="phiml.math.instance" href="#phiml.math.instance">instance</a></code></li>
<li><code><a title="phiml.math.ipack" href="#phiml.math.ipack">ipack</a></code></li>
<li><code><a title="phiml.math.is_finite" href="#phiml.math.is_finite">is_finite</a></code></li>
<li><code><a title="phiml.math.is_inf" href="#phiml.math.is_inf">is_inf</a></code></li>
<li><code><a title="phiml.math.is_nan" href="#phiml.math.is_nan">is_nan</a></code></li>
<li><code><a title="phiml.math.is_scalar" href="#phiml.math.is_scalar">is_scalar</a></code></li>
<li><code><a title="phiml.math.is_sparse" href="#phiml.math.is_sparse">is_sparse</a></code></li>
<li><code><a title="phiml.math.iterate" href="#phiml.math.iterate">iterate</a></code></li>
<li><code><a title="phiml.math.jacobian" href="#phiml.math.jacobian">jacobian</a></code></li>
<li><code><a title="phiml.math.jit_compile" href="#phiml.math.jit_compile">jit_compile</a></code></li>
<li><code><a title="phiml.math.jit_compile_linear" href="#phiml.math.jit_compile_linear">jit_compile_linear</a></code></li>
<li><code><a title="phiml.math.l1_loss" href="#phiml.math.l1_loss">l1_loss</a></code></li>
<li><code><a title="phiml.math.l2_loss" href="#phiml.math.l2_loss">l2_loss</a></code></li>
<li><code><a title="phiml.math.laplace" href="#phiml.math.laplace">laplace</a></code></li>
<li><code><a title="phiml.math.layout" href="#phiml.math.layout">layout</a></code></li>
<li><code><a title="phiml.math.length" href="#phiml.math.length">length</a></code></li>
<li><code><a title="phiml.math.linspace" href="#phiml.math.linspace">linspace</a></code></li>
<li><code><a title="phiml.math.load" href="#phiml.math.load">load</a></code></li>
<li><code><a title="phiml.math.log" href="#phiml.math.log">log</a></code></li>
<li><code><a title="phiml.math.log10" href="#phiml.math.log10">log10</a></code></li>
<li><code><a title="phiml.math.log2" href="#phiml.math.log2">log2</a></code></li>
<li><code><a title="phiml.math.log_gamma" href="#phiml.math.log_gamma">log_gamma</a></code></li>
<li><code><a title="phiml.math.map" href="#phiml.math.map">map</a></code></li>
<li><code><a title="phiml.math.map_pairs" href="#phiml.math.map_pairs">map_pairs</a></code></li>
<li><code><a title="phiml.math.map_types" href="#phiml.math.map_types">map_types</a></code></li>
<li><code><a title="phiml.math.masked_fill" href="#phiml.math.masked_fill">masked_fill</a></code></li>
<li><code><a title="phiml.math.matrix_from_function" href="#phiml.math.matrix_from_function">matrix_from_function</a></code></li>
<li><code><a title="phiml.math.matrix_rank" href="#phiml.math.matrix_rank">matrix_rank</a></code></li>
<li><code><a title="phiml.math.max" href="#phiml.math.max">max</a></code></li>
<li><code><a title="phiml.math.maximum" href="#phiml.math.maximum">maximum</a></code></li>
<li><code><a title="phiml.math.mean" href="#phiml.math.mean">mean</a></code></li>
<li><code><a title="phiml.math.median" href="#phiml.math.median">median</a></code></li>
<li><code><a title="phiml.math.merge_shapes" href="#phiml.math.merge_shapes">merge_shapes</a></code></li>
<li><code><a title="phiml.math.meshgrid" href="#phiml.math.meshgrid">meshgrid</a></code></li>
<li><code><a title="phiml.math.min" href="#phiml.math.min">min</a></code></li>
<li><code><a title="phiml.math.minimize" href="#phiml.math.minimize">minimize</a></code></li>
<li><code><a title="phiml.math.minimum" href="#phiml.math.minimum">minimum</a></code></li>
<li><code><a title="phiml.math.nan_to_0" href="#phiml.math.nan_to_0">nan_to_0</a></code></li>
<li><code><a title="phiml.math.native" href="#phiml.math.native">native</a></code></li>
<li><code><a title="phiml.math.native_call" href="#phiml.math.native_call">native_call</a></code></li>
<li><code><a title="phiml.math.ncat" href="#phiml.math.ncat">ncat</a></code></li>
<li><code><a title="phiml.math.neighbor_max" href="#phiml.math.neighbor_max">neighbor_max</a></code></li>
<li><code><a title="phiml.math.neighbor_mean" href="#phiml.math.neighbor_mean">neighbor_mean</a></code></li>
<li><code><a title="phiml.math.neighbor_min" href="#phiml.math.neighbor_min">neighbor_min</a></code></li>
<li><code><a title="phiml.math.neighbor_reduce" href="#phiml.math.neighbor_reduce">neighbor_reduce</a></code></li>
<li><code><a title="phiml.math.neighbor_sum" href="#phiml.math.neighbor_sum">neighbor_sum</a></code></li>
<li><code><a title="phiml.math.non_batch" href="#phiml.math.non_batch">non_batch</a></code></li>
<li><code><a title="phiml.math.non_channel" href="#phiml.math.non_channel">non_channel</a></code></li>
<li><code><a title="phiml.math.non_dual" href="#phiml.math.non_dual">non_dual</a></code></li>
<li><code><a title="phiml.math.non_instance" href="#phiml.math.non_instance">non_instance</a></code></li>
<li><code><a title="phiml.math.non_primal" href="#phiml.math.non_primal">non_primal</a></code></li>
<li><code><a title="phiml.math.non_spatial" href="#phiml.math.non_spatial">non_spatial</a></code></li>
<li><code><a title="phiml.math.nonzero" href="#phiml.math.nonzero">nonzero</a></code></li>
<li><code><a title="phiml.math.norm" href="#phiml.math.norm">norm</a></code></li>
<li><code><a title="phiml.math.normalize" href="#phiml.math.normalize">normalize</a></code></li>
<li><code><a title="phiml.math.normalize_to" href="#phiml.math.normalize_to">normalize_to</a></code></li>
<li><code><a title="phiml.math.numpy" href="#phiml.math.numpy">numpy</a></code></li>
<li><code><a title="phiml.math.ones" href="#phiml.math.ones">ones</a></code></li>
<li><code><a title="phiml.math.ones_like" href="#phiml.math.ones_like">ones_like</a></code></li>
<li><code><a title="phiml.math.pack_dims" href="#phiml.math.pack_dims">pack_dims</a></code></li>
<li><code><a title="phiml.math.pad" href="#phiml.math.pad">pad</a></code></li>
<li><code><a title="phiml.math.pairwise_differences" href="#phiml.math.pairwise_differences">pairwise_differences</a></code></li>
<li><code><a title="phiml.math.pairwise_distances" href="#phiml.math.pairwise_distances">pairwise_distances</a></code></li>
<li><code><a title="phiml.math.perf_counter" href="#phiml.math.perf_counter">perf_counter</a></code></li>
<li><code><a title="phiml.math.pick_random" href="#phiml.math.pick_random">pick_random</a></code></li>
<li><code><a title="phiml.math.precision" href="#phiml.math.precision">precision</a></code></li>
<li><code><a title="phiml.math.primal" href="#phiml.math.primal">primal</a></code></li>
<li><code><a title="phiml.math.print" href="#phiml.math.print">print</a></code></li>
<li><code><a title="phiml.math.print_gradient" href="#phiml.math.print_gradient">print_gradient</a></code></li>
<li><code><a title="phiml.math.prod" href="#phiml.math.prod">prod</a></code></li>
<li><code><a title="phiml.math.quantile" href="#phiml.math.quantile">quantile</a></code></li>
<li><code><a title="phiml.math.radians_to_degrees" href="#phiml.math.radians_to_degrees">radians_to_degrees</a></code></li>
<li><code><a title="phiml.math.rand" href="#phiml.math.rand">rand</a></code></li>
<li><code><a title="phiml.math.randn" href="#phiml.math.randn">randn</a></code></li>
<li><code><a title="phiml.math.random_normal" href="#phiml.math.random_normal">random_normal</a></code></li>
<li><code><a title="phiml.math.random_permutation" href="#phiml.math.random_permutation">random_permutation</a></code></li>
<li><code><a title="phiml.math.random_uniform" href="#phiml.math.random_uniform">random_uniform</a></code></li>
<li><code><a title="phiml.math.range" href="#phiml.math.range">range</a></code></li>
<li><code><a title="phiml.math.range_tensor" href="#phiml.math.range_tensor">range_tensor</a></code></li>
<li><code><a title="phiml.math.ravel_index" href="#phiml.math.ravel_index">ravel_index</a></code></li>
<li><code><a title="phiml.math.real" href="#phiml.math.real">real</a></code></li>
<li><code><a title="phiml.math.rename_dims" href="#phiml.math.rename_dims">rename_dims</a></code></li>
<li><code><a title="phiml.math.replace" href="#phiml.math.replace">replace</a></code></li>
<li><code><a title="phiml.math.replace_dims" href="#phiml.math.replace_dims">replace_dims</a></code></li>
<li><code><a title="phiml.math.reshaped_native" href="#phiml.math.reshaped_native">reshaped_native</a></code></li>
<li><code><a title="phiml.math.reshaped_numpy" href="#phiml.math.reshaped_numpy">reshaped_numpy</a></code></li>
<li><code><a title="phiml.math.reshaped_tensor" href="#phiml.math.reshaped_tensor">reshaped_tensor</a></code></li>
<li><code><a title="phiml.math.rotate_vector" href="#phiml.math.rotate_vector">rotate_vector</a></code></li>
<li><code><a title="phiml.math.rotation_matrix" href="#phiml.math.rotation_matrix">rotation_matrix</a></code></li>
<li><code><a title="phiml.math.round" href="#phiml.math.round">round</a></code></li>
<li><code><a title="phiml.math.s2b" href="#phiml.math.s2b">s2b</a></code></li>
<li><code><a title="phiml.math.safe_div" href="#phiml.math.safe_div">safe_div</a></code></li>
<li><code><a title="phiml.math.safe_mul" href="#phiml.math.safe_mul">safe_mul</a></code></li>
<li><code><a title="phiml.math.sample_subgrid" href="#phiml.math.sample_subgrid">sample_subgrid</a></code></li>
<li><code><a title="phiml.math.save" href="#phiml.math.save">save</a></code></li>
<li><code><a title="phiml.math.scatter" href="#phiml.math.scatter">scatter</a></code></li>
<li><code><a title="phiml.math.seed" href="#phiml.math.seed">seed</a></code></li>
<li><code><a title="phiml.math.set_global_precision" href="#phiml.math.set_global_precision">set_global_precision</a></code></li>
<li><code><a title="phiml.math.shape" href="#phiml.math.shape">shape</a></code></li>
<li><code><a title="phiml.math.shift" href="#phiml.math.shift">shift</a></code></li>
<li><code><a title="phiml.math.si2d" href="#phiml.math.si2d">si2d</a></code></li>
<li><code><a title="phiml.math.sigmoid" href="#phiml.math.sigmoid">sigmoid</a></code></li>
<li><code><a title="phiml.math.sign" href="#phiml.math.sign">sign</a></code></li>
<li><code><a title="phiml.math.sin" href="#phiml.math.sin">sin</a></code></li>
<li><code><a title="phiml.math.sinh" href="#phiml.math.sinh">sinh</a></code></li>
<li><code><a title="phiml.math.slice" href="#phiml.math.slice">slice</a></code></li>
<li><code><a title="phiml.math.slice_off" href="#phiml.math.slice_off">slice_off</a></code></li>
<li><code><a title="phiml.math.soft_plus" href="#phiml.math.soft_plus">soft_plus</a></code></li>
<li><code><a title="phiml.math.softmax" href="#phiml.math.softmax">softmax</a></code></li>
<li><code><a title="phiml.math.solve_linear" href="#phiml.math.solve_linear">solve_linear</a></code></li>
<li><code><a title="phiml.math.solve_nonlinear" href="#phiml.math.solve_nonlinear">solve_nonlinear</a></code></li>
<li><code><a title="phiml.math.sort" href="#phiml.math.sort">sort</a></code></li>
<li><code><a title="phiml.math.spack" href="#phiml.math.spack">spack</a></code></li>
<li><code><a title="phiml.math.sparse_tensor" href="#phiml.math.sparse_tensor">sparse_tensor</a></code></li>
<li><code><a title="phiml.math.spatial" href="#phiml.math.spatial">spatial</a></code></li>
<li><code><a title="phiml.math.spatial_gradient" href="#phiml.math.spatial_gradient">spatial_gradient</a></code></li>
<li><code><a title="phiml.math.sqrt" href="#phiml.math.sqrt">sqrt</a></code></li>
<li><code><a title="phiml.math.squared_norm" href="#phiml.math.squared_norm">squared_norm</a></code></li>
<li><code><a title="phiml.math.squeeze" href="#phiml.math.squeeze">squeeze</a></code></li>
<li><code><a title="phiml.math.stack" href="#phiml.math.stack">stack</a></code></li>
<li><code><a title="phiml.math.std" href="#phiml.math.std">std</a></code></li>
<li><code><a title="phiml.math.stop_gradient" href="#phiml.math.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="phiml.math.stored_indices" href="#phiml.math.stored_indices">stored_indices</a></code></li>
<li><code><a title="phiml.math.stored_values" href="#phiml.math.stored_values">stored_values</a></code></li>
<li><code><a title="phiml.math.sum" href="#phiml.math.sum">sum</a></code></li>
<li><code><a title="phiml.math.svd" href="#phiml.math.svd">svd</a></code></li>
<li><code><a title="phiml.math.swap_axes" href="#phiml.math.swap_axes">swap_axes</a></code></li>
<li><code><a title="phiml.math.tan" href="#phiml.math.tan">tan</a></code></li>
<li><code><a title="phiml.math.tanh" href="#phiml.math.tanh">tanh</a></code></li>
<li><code><a title="phiml.math.tcat" href="#phiml.math.tcat">tcat</a></code></li>
<li><code><a title="phiml.math.tensor" href="#phiml.math.tensor">tensor</a></code></li>
<li><code><a title="phiml.math.tensor_like" href="#phiml.math.tensor_like">tensor_like</a></code></li>
<li><code><a title="phiml.math.to_complex" href="#phiml.math.to_complex">to_complex</a></code></li>
<li><code><a title="phiml.math.to_device" href="#phiml.math.to_device">to_device</a></code></li>
<li><code><a title="phiml.math.to_dict" href="#phiml.math.to_dict">to_dict</a></code></li>
<li><code><a title="phiml.math.to_float" href="#phiml.math.to_float">to_float</a></code></li>
<li><code><a title="phiml.math.to_format" href="#phiml.math.to_format">to_format</a></code></li>
<li><code><a title="phiml.math.to_int32" href="#phiml.math.to_int32">to_int32</a></code></li>
<li><code><a title="phiml.math.to_int64" href="#phiml.math.to_int64">to_int64</a></code></li>
<li><code><a title="phiml.math.trace_check" href="#phiml.math.trace_check">trace_check</a></code></li>
<li><code><a title="phiml.math.unpack_dim" href="#phiml.math.unpack_dim">unpack_dim</a></code></li>
<li><code><a title="phiml.math.unstack" href="#phiml.math.unstack">unstack</a></code></li>
<li><code><a title="phiml.math.upsample2x" href="#phiml.math.upsample2x">upsample2x</a></code></li>
<li><code><a title="phiml.math.use" href="#phiml.math.use">use</a></code></li>
<li><code><a title="phiml.math.vec" href="#phiml.math.vec">vec</a></code></li>
<li><code><a title="phiml.math.vec_length" href="#phiml.math.vec_length">vec_length</a></code></li>
<li><code><a title="phiml.math.vec_normalize" href="#phiml.math.vec_normalize">vec_normalize</a></code></li>
<li><code><a title="phiml.math.vec_squared" href="#phiml.math.vec_squared">vec_squared</a></code></li>
<li><code><a title="phiml.math.when_available" href="#phiml.math.when_available">when_available</a></code></li>
<li><code><a title="phiml.math.where" href="#phiml.math.where">where</a></code></li>
<li><code><a title="phiml.math.with_diagonal" href="#phiml.math.with_diagonal">with_diagonal</a></code></li>
<li><code><a title="phiml.math.wrap" href="#phiml.math.wrap">wrap</a></code></li>
<li><code><a title="phiml.math.zeros" href="#phiml.math.zeros">zeros</a></code></li>
<li><code><a title="phiml.math.zeros_like" href="#phiml.math.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phiml.math.ConvergenceException" href="#phiml.math.ConvergenceException">ConvergenceException</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.ConvergenceException.result" href="#phiml.math.ConvergenceException.result">result</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.DType" href="#phiml.math.DType">DType</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.DType.as_dtype" href="#phiml.math.DType.as_dtype">as_dtype</a></code></li>
<li><code><a title="phiml.math.DType.bits" href="#phiml.math.DType.bits">bits</a></code></li>
<li><code><a title="phiml.math.DType.itemsize" href="#phiml.math.DType.itemsize">itemsize</a></code></li>
<li><code><a title="phiml.math.DType.kind" href="#phiml.math.DType.kind">kind</a></code></li>
<li><code><a title="phiml.math.DType.precision" href="#phiml.math.DType.precision">precision</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.Dict" href="#phiml.math.Dict">Dict</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.Dict.copy" href="#phiml.math.Dict.copy">copy</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.Diverged" href="#phiml.math.Diverged">Diverged</a></code></h4>
</li>
<li>
<h4><code><a title="phiml.math.IncompatibleShapes" href="#phiml.math.IncompatibleShapes">IncompatibleShapes</a></code></h4>
</li>
<li>
<h4><code><a title="phiml.math.LinearFunction" href="#phiml.math.LinearFunction">LinearFunction</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.LinearFunction.sparse_matrix" href="#phiml.math.LinearFunction.sparse_matrix">sparse_matrix</a></code></li>
<li><code><a title="phiml.math.LinearFunction.sparse_matrix_and_bias" href="#phiml.math.LinearFunction.sparse_matrix_and_bias">sparse_matrix_and_bias</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.NotConverged" href="#phiml.math.NotConverged">NotConverged</a></code></h4>
</li>
<li>
<h4><code><a title="phiml.math.Shape" href="#phiml.math.Shape">Shape</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.Shape.after_gather" href="#phiml.math.Shape.after_gather">after_gather</a></code></li>
<li><code><a title="phiml.math.Shape.after_pad" href="#phiml.math.Shape.after_pad">after_pad</a></code></li>
<li><code><a title="phiml.math.Shape.are_adjacent" href="#phiml.math.Shape.are_adjacent">are_adjacent</a></code></li>
<li><code><a title="phiml.math.Shape.as_batch" href="#phiml.math.Shape.as_batch">as_batch</a></code></li>
<li><code><a title="phiml.math.Shape.as_channel" href="#phiml.math.Shape.as_channel">as_channel</a></code></li>
<li><code><a title="phiml.math.Shape.as_dual" href="#phiml.math.Shape.as_dual">as_dual</a></code></li>
<li><code><a title="phiml.math.Shape.as_instance" href="#phiml.math.Shape.as_instance">as_instance</a></code></li>
<li><code><a title="phiml.math.Shape.as_spatial" href="#phiml.math.Shape.as_spatial">as_spatial</a></code></li>
<li><code><a title="phiml.math.Shape.as_type" href="#phiml.math.Shape.as_type">as_type</a></code></li>
<li><code><a title="phiml.math.Shape.assert_all_sizes_defined" href="#phiml.math.Shape.assert_all_sizes_defined">assert_all_sizes_defined</a></code></li>
<li><code><a title="phiml.math.Shape.batch" href="#phiml.math.Shape.batch">batch</a></code></li>
<li><code><a title="phiml.math.Shape.batch_rank" href="#phiml.math.Shape.batch_rank">batch_rank</a></code></li>
<li><code><a title="phiml.math.Shape.channel" href="#phiml.math.Shape.channel">channel</a></code></li>
<li><code><a title="phiml.math.Shape.channel_rank" href="#phiml.math.Shape.channel_rank">channel_rank</a></code></li>
<li><code><a title="phiml.math.Shape.defined" href="#phiml.math.Shape.defined">defined</a></code></li>
<li><code><a title="phiml.math.Shape.dim_type" href="#phiml.math.Shape.dim_type">dim_type</a></code></li>
<li><code><a title="phiml.math.Shape.dual" href="#phiml.math.Shape.dual">dual</a></code></li>
<li><code><a title="phiml.math.Shape.dual_rank" href="#phiml.math.Shape.dual_rank">dual_rank</a></code></li>
<li><code><a title="phiml.math.Shape.first_index" href="#phiml.math.Shape.first_index">first_index</a></code></li>
<li><code><a title="phiml.math.Shape.flipped" href="#phiml.math.Shape.flipped">flipped</a></code></li>
<li><code><a title="phiml.math.Shape.get_dim_type" href="#phiml.math.Shape.get_dim_type">get_dim_type</a></code></li>
<li><code><a title="phiml.math.Shape.get_item_names" href="#phiml.math.Shape.get_item_names">get_item_names</a></code></li>
<li><code><a title="phiml.math.Shape.get_size" href="#phiml.math.Shape.get_size">get_size</a></code></li>
<li><code><a title="phiml.math.Shape.get_sizes" href="#phiml.math.Shape.get_sizes">get_sizes</a></code></li>
<li><code><a title="phiml.math.Shape.get_type" href="#phiml.math.Shape.get_type">get_type</a></code></li>
<li><code><a title="phiml.math.Shape.get_types" href="#phiml.math.Shape.get_types">get_types</a></code></li>
<li><code><a title="phiml.math.Shape.index" href="#phiml.math.Shape.index">index</a></code></li>
<li><code><a title="phiml.math.Shape.indices" href="#phiml.math.Shape.indices">indices</a></code></li>
<li><code><a title="phiml.math.Shape.instance" href="#phiml.math.Shape.instance">instance</a></code></li>
<li><code><a title="phiml.math.Shape.instance_rank" href="#phiml.math.Shape.instance_rank">instance_rank</a></code></li>
<li><code><a title="phiml.math.Shape.is_compatible" href="#phiml.math.Shape.is_compatible">is_compatible</a></code></li>
<li><code><a title="phiml.math.Shape.is_empty" href="#phiml.math.Shape.is_empty">is_empty</a></code></li>
<li><code><a title="phiml.math.Shape.is_non_uniform" href="#phiml.math.Shape.is_non_uniform">is_non_uniform</a></code></li>
<li><code><a title="phiml.math.Shape.is_uniform" href="#phiml.math.Shape.is_uniform">is_uniform</a></code></li>
<li><code><a title="phiml.math.Shape.isdisjoint" href="#phiml.math.Shape.isdisjoint">isdisjoint</a></code></li>
<li><code><a title="phiml.math.Shape.mask" href="#phiml.math.Shape.mask">mask</a></code></li>
<li><code><a title="phiml.math.Shape.meshgrid" href="#phiml.math.Shape.meshgrid">meshgrid</a></code></li>
<li><code><a title="phiml.math.Shape.name" href="#phiml.math.Shape.name">name</a></code></li>
<li><code><a title="phiml.math.Shape.name_list" href="#phiml.math.Shape.name_list">name_list</a></code></li>
<li><code><a title="phiml.math.Shape.names" href="#phiml.math.Shape.names">names</a></code></li>
<li><code><a title="phiml.math.Shape.non_batch" href="#phiml.math.Shape.non_batch">non_batch</a></code></li>
<li><code><a title="phiml.math.Shape.non_channel" href="#phiml.math.Shape.non_channel">non_channel</a></code></li>
<li><code><a title="phiml.math.Shape.non_dual" href="#phiml.math.Shape.non_dual">non_dual</a></code></li>
<li><code><a title="phiml.math.Shape.non_instance" href="#phiml.math.Shape.non_instance">non_instance</a></code></li>
<li><code><a title="phiml.math.Shape.non_primal" href="#phiml.math.Shape.non_primal">non_primal</a></code></li>
<li><code><a title="phiml.math.Shape.non_singleton" href="#phiml.math.Shape.non_singleton">non_singleton</a></code></li>
<li><code><a title="phiml.math.Shape.non_spatial" href="#phiml.math.Shape.non_spatial">non_spatial</a></code></li>
<li><code><a title="phiml.math.Shape.non_uniform" href="#phiml.math.Shape.non_uniform">non_uniform</a></code></li>
<li><code><a title="phiml.math.Shape.non_uniform_shape" href="#phiml.math.Shape.non_uniform_shape">non_uniform_shape</a></code></li>
<li><code><a title="phiml.math.Shape.only" href="#phiml.math.Shape.only">only</a></code></li>
<li><code><a title="phiml.math.Shape.prepare_gather" href="#phiml.math.Shape.prepare_gather">prepare_gather</a></code></li>
<li><code><a title="phiml.math.Shape.prepare_renaming_gather" href="#phiml.math.Shape.prepare_renaming_gather">prepare_renaming_gather</a></code></li>
<li><code><a title="phiml.math.Shape.primal" href="#phiml.math.Shape.primal">primal</a></code></li>
<li><code><a title="phiml.math.Shape.rank" href="#phiml.math.Shape.rank">rank</a></code></li>
<li><code><a title="phiml.math.Shape.replace" href="#phiml.math.Shape.replace">replace</a></code></li>
<li><code><a title="phiml.math.Shape.resolve_index" href="#phiml.math.Shape.resolve_index">resolve_index</a></code></li>
<li><code><a title="phiml.math.Shape.reversed" href="#phiml.math.Shape.reversed">reversed</a></code></li>
<li><code><a title="phiml.math.Shape.shape" href="#phiml.math.Shape.shape">shape</a></code></li>
<li><code><a title="phiml.math.Shape.singleton" href="#phiml.math.Shape.singleton">singleton</a></code></li>
<li><code><a title="phiml.math.Shape.size" href="#phiml.math.Shape.size">size</a></code></li>
<li><code><a title="phiml.math.Shape.sizes" href="#phiml.math.Shape.sizes">sizes</a></code></li>
<li><code><a title="phiml.math.Shape.spatial" href="#phiml.math.Shape.spatial">spatial</a></code></li>
<li><code><a title="phiml.math.Shape.spatial_rank" href="#phiml.math.Shape.spatial_rank">spatial_rank</a></code></li>
<li><code><a title="phiml.math.Shape.transpose" href="#phiml.math.Shape.transpose">transpose</a></code></li>
<li><code><a title="phiml.math.Shape.transposed" href="#phiml.math.Shape.transposed">transposed</a></code></li>
<li><code><a title="phiml.math.Shape.type" href="#phiml.math.Shape.type">type</a></code></li>
<li><code><a title="phiml.math.Shape.undefined" href="#phiml.math.Shape.undefined">undefined</a></code></li>
<li><code><a title="phiml.math.Shape.unstack" href="#phiml.math.Shape.unstack">unstack</a></code></li>
<li><code><a title="phiml.math.Shape.untyped_dict" href="#phiml.math.Shape.untyped_dict">untyped_dict</a></code></li>
<li><code><a title="phiml.math.Shape.volume" href="#phiml.math.Shape.volume">volume</a></code></li>
<li><code><a title="phiml.math.Shape.well_defined" href="#phiml.math.Shape.well_defined">well_defined</a></code></li>
<li><code><a title="phiml.math.Shape.with_dim_size" href="#phiml.math.Shape.with_dim_size">with_dim_size</a></code></li>
<li><code><a title="phiml.math.Shape.with_size" href="#phiml.math.Shape.with_size">with_size</a></code></li>
<li><code><a title="phiml.math.Shape.with_sizes" href="#phiml.math.Shape.with_sizes">with_sizes</a></code></li>
<li><code><a title="phiml.math.Shape.without" href="#phiml.math.Shape.without">without</a></code></li>
<li><code><a title="phiml.math.Shape.without_sizes" href="#phiml.math.Shape.without_sizes">without_sizes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.Solve" href="#phiml.math.Solve">Solve</a></code></h4>
<ul class="two-column">
<li><code><a title="phiml.math.Solve.abs_tol" href="#phiml.math.Solve.abs_tol">abs_tol</a></code></li>
<li><code><a title="phiml.math.Solve.gradient_solve" href="#phiml.math.Solve.gradient_solve">gradient_solve</a></code></li>
<li><code><a title="phiml.math.Solve.max_iterations" href="#phiml.math.Solve.max_iterations">max_iterations</a></code></li>
<li><code><a title="phiml.math.Solve.method" href="#phiml.math.Solve.method">method</a></code></li>
<li><code><a title="phiml.math.Solve.preprocess_y" href="#phiml.math.Solve.preprocess_y">preprocess_y</a></code></li>
<li><code><a title="phiml.math.Solve.rank_deficiency" href="#phiml.math.Solve.rank_deficiency">rank_deficiency</a></code></li>
<li><code><a title="phiml.math.Solve.rel_tol" href="#phiml.math.Solve.rel_tol">rel_tol</a></code></li>
<li><code><a title="phiml.math.Solve.suppress" href="#phiml.math.Solve.suppress">suppress</a></code></li>
<li><code><a title="phiml.math.Solve.with_defaults" href="#phiml.math.Solve.with_defaults">with_defaults</a></code></li>
<li><code><a title="phiml.math.Solve.with_preprocessing" href="#phiml.math.Solve.with_preprocessing">with_preprocessing</a></code></li>
<li><code><a title="phiml.math.Solve.x0" href="#phiml.math.Solve.x0">x0</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.SolveInfo" href="#phiml.math.SolveInfo">SolveInfo</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.SolveInfo.converged" href="#phiml.math.SolveInfo.converged">converged</a></code></li>
<li><code><a title="phiml.math.SolveInfo.convergence_check" href="#phiml.math.SolveInfo.convergence_check">convergence_check</a></code></li>
<li><code><a title="phiml.math.SolveInfo.diverged" href="#phiml.math.SolveInfo.diverged">diverged</a></code></li>
<li><code><a title="phiml.math.SolveInfo.function_evaluations" href="#phiml.math.SolveInfo.function_evaluations">function_evaluations</a></code></li>
<li><code><a title="phiml.math.SolveInfo.iterations" href="#phiml.math.SolveInfo.iterations">iterations</a></code></li>
<li><code><a title="phiml.math.SolveInfo.method" href="#phiml.math.SolveInfo.method">method</a></code></li>
<li><code><a title="phiml.math.SolveInfo.msg" href="#phiml.math.SolveInfo.msg">msg</a></code></li>
<li><code><a title="phiml.math.SolveInfo.residual" href="#phiml.math.SolveInfo.residual">residual</a></code></li>
<li><code><a title="phiml.math.SolveInfo.snapshot" href="#phiml.math.SolveInfo.snapshot">snapshot</a></code></li>
<li><code><a title="phiml.math.SolveInfo.solve" href="#phiml.math.SolveInfo.solve">solve</a></code></li>
<li><code><a title="phiml.math.SolveInfo.solve_time" href="#phiml.math.SolveInfo.solve_time">solve_time</a></code></li>
<li><code><a title="phiml.math.SolveInfo.x" href="#phiml.math.SolveInfo.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.SolveTape" href="#phiml.math.SolveTape">SolveTape</a></code></h4>
<ul class="">
<li><code><a title="phiml.math.SolveTape.should_record_trajectory_for" href="#phiml.math.SolveTape.should_record_trajectory_for">should_record_trajectory_for</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.math.Tensor" href="#phiml.math.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="phiml.math.Tensor.T" href="#phiml.math.Tensor.T">T</a></code></li>
<li><code><a title="phiml.math.Tensor.Tc" href="#phiml.math.Tensor.Tc">Tc</a></code></li>
<li><code><a title="phiml.math.Tensor.Ti" href="#phiml.math.Tensor.Ti">Ti</a></code></li>
<li><code><a title="phiml.math.Tensor.Ts" href="#phiml.math.Tensor.Ts">Ts</a></code></li>
<li><code><a title="phiml.math.Tensor.all" href="#phiml.math.Tensor.all">all</a></code></li>
<li><code><a title="phiml.math.Tensor.any" href="#phiml.math.Tensor.any">any</a></code></li>
<li><code><a title="phiml.math.Tensor.available" href="#phiml.math.Tensor.available">available</a></code></li>
<li><code><a title="phiml.math.Tensor.backend" href="#phiml.math.Tensor.backend">backend</a></code></li>
<li><code><a title="phiml.math.Tensor.default_backend" href="#phiml.math.Tensor.default_backend">default_backend</a></code></li>
<li><code><a title="phiml.math.Tensor.device" href="#phiml.math.Tensor.device">device</a></code></li>
<li><code><a title="phiml.math.Tensor.dimension" href="#phiml.math.Tensor.dimension">dimension</a></code></li>
<li><code><a title="phiml.math.Tensor.dtype" href="#phiml.math.Tensor.dtype">dtype</a></code></li>
<li><code><a title="phiml.math.Tensor.finite_max" href="#phiml.math.Tensor.finite_max">finite_max</a></code></li>
<li><code><a title="phiml.math.Tensor.finite_mean" href="#phiml.math.Tensor.finite_mean">finite_mean</a></code></li>
<li><code><a title="phiml.math.Tensor.finite_min" href="#phiml.math.Tensor.finite_min">finite_min</a></code></li>
<li><code><a title="phiml.math.Tensor.finite_sum" href="#phiml.math.Tensor.finite_sum">finite_sum</a></code></li>
<li><code><a title="phiml.math.Tensor.imag" href="#phiml.math.Tensor.imag">imag</a></code></li>
<li><code><a title="phiml.math.Tensor.map" href="#phiml.math.Tensor.map">map</a></code></li>
<li><code><a title="phiml.math.Tensor.max" href="#phiml.math.Tensor.max">max</a></code></li>
<li><code><a title="phiml.math.Tensor.mean" href="#phiml.math.Tensor.mean">mean</a></code></li>
<li><code><a title="phiml.math.Tensor.min" href="#phiml.math.Tensor.min">min</a></code></li>
<li><code><a title="phiml.math.Tensor.native" href="#phiml.math.Tensor.native">native</a></code></li>
<li><code><a title="phiml.math.Tensor.numpy" href="#phiml.math.Tensor.numpy">numpy</a></code></li>
<li><code><a title="phiml.math.Tensor.pack" href="#phiml.math.Tensor.pack">pack</a></code></li>
<li><code><a title="phiml.math.Tensor.print" href="#phiml.math.Tensor.print">print</a></code></li>
<li><code><a title="phiml.math.Tensor.rank" href="#phiml.math.Tensor.rank">rank</a></code></li>
<li><code><a title="phiml.math.Tensor.real" href="#phiml.math.Tensor.real">real</a></code></li>
<li><code><a title="phiml.math.Tensor.shape" href="#phiml.math.Tensor.shape">shape</a></code></li>
<li><code><a title="phiml.math.Tensor.std" href="#phiml.math.Tensor.std">std</a></code></li>
<li><code><a title="phiml.math.Tensor.sum" href="#phiml.math.Tensor.sum">sum</a></code></li>
<li><code><a title="phiml.math.Tensor.unpack" href="#phiml.math.Tensor.unpack">unpack</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
