<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>phiml.nn API documentation</title>
<meta name="description" content="Unified neural network library.
Includes …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phiml.nn</code></h1>
</header>
<section id="section-intro">
<p>Unified neural network library.
Includes</p>
<ul>
<li>Flexible NN creation of popular architectures</li>
<li>Optimizer creation</li>
<li>Training functionality</li>
<li>Parameter access</li>
<li>Saving and loading networks and optimizer states.</li>
</ul>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phiml.nn.adagrad"><code class="name flex">
<span>def <span class="ident">adagrad</span></span>(<span>net: ~Network, learning_rate: float = 0.001, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0.0, eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adagrad optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html">'torch.optim.Adagrad'</a>
Analogue functions exist for other learning frameworks.</p></div>
</dd>
<dt id="phiml.nn.adam"><code class="name flex">
<span>def <span class="ident">adam</span></span>(<span>net: ~Network, learning_rate: float = 0.001, betas=(0.9, 0.999), epsilon=1e-07)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adam optimizer for <code>net</code>, alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>torch.optim.Adam</code></a>.
Analogue functions exist for other learning frameworks.</p></div>
</dd>
<dt id="phiml.nn.conv_classifier"><code class="name flex">
<span>def <span class="ident">conv_classifier</span></span>(<span>in_features: int, in_spatial: Union[tuple, list], num_classes: int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Based on VGG16.</p></div>
</dd>
<dt id="phiml.nn.conv_net"><code class="name flex">
<span>def <span class="ident">conv_net</span></span>(<span>in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) ‑> ~Network</span>
</code></dt>
<dd>
<div class="desc"><p>Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_channels</code></strong></dt>
<dd>input channels of the feature map, dtype: int</dd>
<dt><strong><code>out_channels</code></strong></dt>
<dd>output channels of the feature map, dtype: int</dd>
<dt><strong><code>layers</code></strong></dt>
<dd>list or tuple of output channels for each intermediate layer between the input and final output channels, dtype: list or tuple</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>activation function used within the layers, dtype: string</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>use of batchnorm after each conv layer, dtype: bool</dd>
<dt><strong><code>in_spatial</code></strong></dt>
<dd>spatial dimensions of the input feature map, dtype: int</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Conv-net model as specified by input arguments</p></div>
</dd>
<dt id="phiml.nn.get_learning_rate"><code class="name flex">
<span>def <span class="ident">get_learning_rate</span></span>(<span>optimizer: ~Optimizer) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the global learning rate of the given optimizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>optim.Optimizer</code></dt>
<dd>The optimizer whose learning rate needs to be retrieved.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The learning rate of the optimizer.</dd>
</dl></div>
</dd>
<dt id="phiml.nn.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>net: ~Network) ‑> Dict[str, phiml.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns all parameters of a neural network.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>Neural network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>dict</code> mapping parameter names to <code><a title="phiml.math.Tensor" href="math/index.html#phiml.math.Tensor">Tensor</a></code>s.</p></div>
</dd>
<dt id="phiml.nn.invertible_net"><code class="name flex">
<span>def <span class="ident">invertible_net</span></span>(<span>num_blocks: int = 3, construct_net: Union[str, Callable] = 'u_net', **construct_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Invertible NNs are capable of inverting the output tensor back to the input tensor initially passed.
These networks have far-reaching applications in predicting input parameters of a problem given its observations.
Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.</p>
<p>Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or mlp blocks with in_channels = out_channels.
The architecture used is popularized by <a href="https://arxiv.org/abs/1605.08803">"Real NVP"</a>.</p>
<p>Invertible nets are only implemented for PyTorch and TensorFlow.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_blocks</code></strong></dt>
<dd>number of coupling blocks inside the invertible net, dtype: int</dd>
<dt><strong><code>construct_net</code></strong></dt>
<dd>Function to construct one part of the neural network.
This network must have the same number of inputs and outputs.
Can be a <code>lambda</code> function or one of the following strings: <code><a title="phiml.nn.mlp" href="#phiml.nn.mlp">mlp()</a>, <a title="phiml.nn.u_net" href="#phiml.nn.u_net">u_net()</a>, <a title="phiml.nn.res_net" href="#phiml.nn.res_net">res_net()</a>, <a title="phiml.nn.conv_net" href="#phiml.nn.conv_net">conv_net()</a></code></dd>
<dt><strong><code>construct_kwargs</code></strong></dt>
<dd>Keyword arguments passed to <code>construct_net</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Invertible neural network model</p></div>
</dd>
<dt id="phiml.nn.load_state"><code class="name flex">
<span>def <span class="ident">load_state</span></span>(<span>obj: Union[~Network, ~Optimizer], path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Read the state of a module or optimizer from a file.</p>
<p>See Also:
<code><a title="phiml.nn.save_state" href="#phiml.nn.save_state">save_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>torch.Network or torch.optim.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.nn.mlp"><code class="name flex">
<span>def <span class="ident">mlp</span></span>(<span>in_channels: int, out_channels: int, layers: Sequence[int], batch_norm=False, activation: Union[str, Callable] = 'ReLU', softmax=False) ‑> ~Network</span>
</code></dt>
<dd>
<div class="desc"><p>Fully-connected neural networks are available in Φ-ML via mlp().</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_channels</code></strong></dt>
<dd>size of input layer, int</dd>
<dt>out_channels = size of output layer, int</dt>
<dt><strong><code>layers</code></strong></dt>
<dd>tuple of linear layers between input and output neurons, list or tuple</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>activation function used within the layers, string</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>use of batch norm after each linear layer, bool</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dense net model as specified by input arguments</p></div>
</dd>
<dt id="phiml.nn.parameter_count"><code class="name flex">
<span>def <span class="ident">parameter_count</span></span>(<span>net: ~Network) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Counts the number of parameters in a model.</p>
<p>See Also:
<code><a title="phiml.nn.get_parameters" href="#phiml.nn.get_parameters">get_parameters()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>PyTorch model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Total parameter count as <code>int</code>.</p></div>
</dd>
<dt id="phiml.nn.res_net"><code class="name flex">
<span>def <span class="ident">res_net</span></span>(<span>in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) ‑> ~Network</span>
</code></dt>
<dd>
<div class="desc"><p>Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
A default filter size of 3 is used in the convolutional layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_channels</code></strong></dt>
<dd>input channels of the feature map, dtype: int</dd>
<dt><strong><code>out_channels</code></strong></dt>
<dd>output channels of the feature map, dtype: int</dd>
<dt><strong><code>layers</code></strong></dt>
<dd>list or tuple of output channels for each intermediate layer between the input and final output channels, dtype: list or tuple</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>activation function used within the layers, dtype: string</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>use of batchnorm after each conv layer, dtype: bool</dd>
<dt><strong><code>in_spatial</code></strong></dt>
<dd>spatial dimensions of the input feature map, dtype: int</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Res-net model as specified by input arguments</p></div>
</dd>
<dt id="phiml.nn.rmsprop"><code class="name flex">
<span>def <span class="ident">rmsprop</span></span>(<span>net: ~Network, learning_rate: float = 0.001, alpha=0.99, eps=1e-08, weight_decay=0.0, momentum=0.0, centered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an RMSProp optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html">'torch.optim.RMSprop'</a>
Analogue functions exist for other learning frameworks.</p></div>
</dd>
<dt id="phiml.nn.save_state"><code class="name flex">
<span>def <span class="ident">save_state</span></span>(<span>obj: Union[~Network, ~Optimizer], path: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Write the state of a module or optimizer to a file.</p>
<p>See Also:
<code><a title="phiml.nn.load_state" href="#phiml.nn.load_state">load_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>torch.Network or torch.optim.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Path to the saved file.</p></div>
</dd>
<dt id="phiml.nn.set_learning_rate"><code class="name flex">
<span>def <span class="ident">set_learning_rate</span></span>(<span>optimizer: ~Optimizer, learning_rate: Union[float, phiml.math._tensors.Tensor])</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the global learning rate for the given optimizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>optim.Optimizer</code></dt>
<dd>The optimizer whose learning rate needs to be updated.</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>The new learning rate to set.</dd>
</dl></div>
</dd>
<dt id="phiml.nn.sgd"><code class="name flex">
<span>def <span class="ident">sgd</span></span>(<span>net: ~Network, learning_rate: float = 0.001, momentum=0.0, dampening=0.0, weight_decay=0.0, nesterov=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an SGD optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">'torch.optim.SGD'</a>
Analogue functions exist for other learning frameworks.</p></div>
</dd>
<dt id="phiml.nn.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>name: Optional[str], model, optimizer, loss_fn: Callable, *files_or_data: Union[str, phiml.math._tensors.Tensor], max_epochs: int = None, max_iter: int = None, max_hours: float = None, stop_on_loss: float = None, batch_size: int = 1, file_shape: phiml.math._shape.Shape = (), dataset_dims: Union[str, Sequence[+T_co], set, phiml.math._shape.Shape, Callable, None] = &lt;function batch&gt;, device: phiml.backend._backend.ComputeDevice = None, drop_last=False, loss_kwargs=None, lr_schedule_iter=None, checkpoint_frequency=None, loader=&lt;function load&gt;, on_iter_end: Callable = None, on_epoch_end: Callable = None, measure_peak_memory: bool = True) ‑> <a title="phiml.nn.TrainingState" href="#phiml.nn.TrainingState">TrainingState</a></span>
</code></dt>
<dd>
<div class="desc"><p>Call <code><a title="phiml.nn.update_weights" href="#phiml.nn.update_weights">update_weights()</a></code> for each batch in a loop for each epoch.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Name of the model. This is used as a name to save the model and optimizer states.
If not specified, no model or optimizer states are saved.</dd>
<dt><strong><code>model</code></strong></dt>
<dd>PyTorch module or Keras model (TensorFlow).</dd>
<dt><strong><code>optimizer</code></strong></dt>
<dd>PyTorch or TensorFlow/Keras optimizer.</dd>
<dt><strong><code>loss_fn</code></strong></dt>
<dd>Loss function for training. This function should take the data as input, run the model and return the loss. It may return additional outputs, but the loss must be the first value.</dd>
<dt><strong><code>*files_or_data</code></strong></dt>
<dd>Training data or file names containing training data or a mixture of both. Files are loaded using <code>loader</code>.</dd>
<dt><strong><code>max_epochs</code></strong></dt>
<dd>Epoch limit.</dd>
<dt><strong><code>max_iter</code></strong></dt>
<dd>Iteration limit. The number of iterations depends on the batch size and the number of files.</dd>
<dt><strong><code>max_hours</code></strong></dt>
<dd>Training time limit in hours (<code>float</code>).</dd>
<dt><strong><code>stop_on_loss</code></strong></dt>
<dd>Stop training if the mean epoch loss falls below this value.</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>Batch size for training. The batch size is limited by the number of data points in the dataset.</dd>
<dt><strong><code>file_shape</code></strong></dt>
<dd>Shape of data stored in each file.</dd>
<dt><strong><code>dataset_dims</code></strong></dt>
<dd>Which dims of the training data list training examples, as opposed to features of data points.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Device to use for training. If <code>None</code>, the default device is used.</dd>
<dt><strong><code>drop_last</code></strong></dt>
<dd>If <code>True</code>, drop the last batch if it is smaller than <code>batch_size</code>.</dd>
<dt><strong><code>loss_kwargs</code></strong></dt>
<dd>Keyword arguments passed to <code>loss_fn</code>.</dd>
<dt><strong><code>lr_schedule_iter</code></strong></dt>
<dd>Function <code>(i: int) -&gt; float</code> that returns the learning rate for iteration <code>i</code>. If <code>None</code>, the learning rate of the <code>optimizer</code> is used as is.</dd>
<dt><strong><code>checkpoint_frequency</code></strong></dt>
<dd>If not <code>None</code>, save the model and optimizer state every <code>checkpoint_frequency</code> epochs.</dd>
<dt><strong><code>loader</code></strong></dt>
<dd>Function <code>(file: str) -&gt; data: Tensor</code> to load data from files. Defaults to <code><a title="phiml.math.load" href="math/index.html#phiml.math.load">load()</a></code>.</dd>
<dt><strong><code>on_iter_end</code></strong></dt>
<dd>Function <code>(i: int, max_iter: int, name: str, model, optimizer, learning_rate, loss, *additional_output) -&gt; None</code> called after each iteration. The function is called with the current iteration number <code>i</code> starting at 0, the maximum number of iterations <code>max_iter</code>, the name of the model <code>name</code>, the model <code>model</code>, the optimizer <code>optimizer</code>, the learning rate <code>learning_rate</code>, the loss value <code>loss</code> and any additional output from <code>loss_fn</code>.</dd>
<dt><strong><code>on_epoch_end</code></strong></dt>
<dd>Function <code>(epoch: int, max_epochs: int, name: str, model, optimizer, learning_rate, epoch_loss) -&gt; None</code> called after each epoch. The function is called with the current epoch number <code>epoch</code> starting at 0, the maximum number of epochs <code>max_epochs</code>, the name of the model <code>name</code>, the model <code>model</code>, the optimizer <code>optimizer</code>, the learning rate <code>learning_rate</code> and the average loss for the epoch <code>epoch_loss</code>.</dd>
<dt><strong><code>measure_peak_memory</code></strong></dt>
<dd>If <code>True</code>, measure the peak memory usage during training and store it in the returned <code><a title="phiml.nn.TrainingState" href="#phiml.nn.TrainingState">TrainingState</a></code>. This is only supported by some backends.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>TrainingResult</code> containing the termination reason, last epoch and last iteration.</p></div>
</dd>
<dt id="phiml.nn.u_net"><code class="name flex">
<span>def <span class="ident">u_net</span></span>(<span>in_channels: int, out_channels: int, levels: int = 4, filters: Union[int, Sequence[+T_co]] = 16, batch_norm: bool = True, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False, use_res_blocks: bool = False, down_kernel_size=3, up_kernel_size=3) ‑> ~Network</span>
</code></dt>
<dd>
<div class="desc"><p>Built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_channels</code></strong></dt>
<dd>input channels of the feature map, dtype: int</dd>
<dt><strong><code>out_channels</code></strong></dt>
<dd>output channels of the feature map, dtype: int</dd>
<dt><strong><code>levels</code></strong></dt>
<dd>number of levels of down-sampling and upsampling, dtype: int</dd>
<dt><strong><code>filters</code></strong></dt>
<dd>filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>activation function used within the layers, dtype: string</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>use of batchnorm after each conv layer, dtype: bool</dd>
<dt><strong><code>in_spatial</code></strong></dt>
<dd>spatial dimensions of the input feature map, dtype: int</dd>
<dt><strong><code>use_res_blocks</code></strong></dt>
<dd>use convolutional blocks with skip connections instead of regular convolutional blocks, dtype: bool</dd>
<dt><strong><code>down_kernel_size</code></strong></dt>
<dd>Kernel size for convolutions on the down-sampling (first half) side of the U-Net.</dd>
<dt><strong><code>up_kernel_size</code></strong></dt>
<dd>Kernel size for convolutions on the up-sampling (second half) of the U-Net.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>U-net model as specified by input arguments.</p></div>
</dd>
<dt id="phiml.nn.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>net: ~Network, optimizer: ~Optimizer, loss_function: Callable, *loss_args, **loss_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the gradients of <code>loss_function</code> w.r.t. the parameters of <code>net</code> and updates its weights using <code>optimizer</code>.</p>
<p>This is the PyTorch version. Analogue functions exist for other learning frameworks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>Learning model.</dd>
<dt><strong><code>optimizer</code></strong></dt>
<dd>Optimizer.</dd>
<dt><strong><code>loss_function</code></strong></dt>
<dd>Loss function, called as <code>loss_function(*loss_args, **loss_kwargs)</code>.</dd>
<dt><strong><code>*loss_args</code></strong></dt>
<dd>Arguments given to <code>loss_function</code>.</dd>
<dt><strong><code>**loss_kwargs</code></strong></dt>
<dd>Keyword arguments given to <code>loss_function</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output of <code>loss_function</code>.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phiml.nn.StopTraining"><code class="flex name class">
<span>class <span class="ident">StopTraining</span></span>
<span>(</span><span>reason: str = 'stop')</span>
</code></dt>
<dd>
<div class="desc"><p>This exception is raised by the <code>on_epoch_end</code> or <code>on_iter_end</code> callbacks to stop training.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StopTraining(Exception):
    &#34;&#34;&#34; This exception is raised by the `on_epoch_end` or `on_iter_end` callbacks to stop training. &#34;&#34;&#34;
    def __init__(self, reason: str = &#39;stop&#39;):
        super().__init__(reason)
        self.reason = reason</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phiml.nn.TrainingState"><code class="flex name class">
<span>class <span class="ident">TrainingState</span></span>
<span>(</span><span>name: str, model: ~Network, optimizer: ~Optimizer, learning_rate: float, epoch: int, max_epochs: Optional[int], iter: int, max_iter: Optional[int], is_epoch_end: bool, epoch_loss: phiml.math._tensors.Tensor, batch_loss: Optional[phiml.math._tensors.Tensor], additional_batch_output: Optional[tuple], indices: phiml.math._tensors.Tensor, termination_reason: Optional[str], peak_memory: Optional[int])</span>
</code></dt>
<dd>
<div class="desc"><p>TrainingState(name: str, model: ~Network, optimizer: ~Optimizer, learning_rate: float, epoch: int, max_epochs: Union[int, NoneType], iter: int, max_iter: Union[int, NoneType], is_epoch_end: bool, epoch_loss: phiml.math._tensors.Tensor, batch_loss: Union[phiml.math._tensors.Tensor, NoneType], additional_batch_output: Union[tuple, NoneType], indices: phiml.math._tensors.Tensor, termination_reason: Union[str, NoneType], peak_memory: Union[int, NoneType])</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrainingState:
    name: str
    model: Network
    optimizer: Optimizer
    learning_rate: float
    epoch: int
    max_epochs: Optional[int]
    iter: int
    max_iter: Optional[int]
    is_epoch_end: bool
    epoch_loss: Tensor
    batch_loss: Optional[Tensor]
    additional_batch_output: Optional[tuple]
    indices: Tensor
    termination_reason: Optional[str]
    peak_memory: Optional[int]

    @property
    def current(self) -&gt; int:
        return self.epoch if self.is_epoch_end else self.iter

    @property
    def max(self) -&gt; int:
        return self.max_epochs if self.is_epoch_end else self.max_iter

    @property
    def mean_loss(self) -&gt; float:
        return float(self.epoch_loss) if self.is_epoch_end else float(math.mean(self.batch_loss, &#39;dset_linear&#39;))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="phiml.nn.TrainingState.additional_batch_output"><code class="name">var <span class="ident">additional_batch_output</span> : Optional[tuple]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.batch_loss"><code class="name">var <span class="ident">batch_loss</span> : Optional[phiml.math._tensors.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.epoch"><code class="name">var <span class="ident">epoch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.epoch_loss"><code class="name">var <span class="ident">epoch_loss</span> : phiml.math._tensors.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.indices"><code class="name">var <span class="ident">indices</span> : phiml.math._tensors.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.is_epoch_end"><code class="name">var <span class="ident">is_epoch_end</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.iter"><code class="name">var <span class="ident">iter</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.learning_rate"><code class="name">var <span class="ident">learning_rate</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.max_epochs"><code class="name">var <span class="ident">max_epochs</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.max_iter"><code class="name">var <span class="ident">max_iter</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.model"><code class="name">var <span class="ident">model</span> : ~Network</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.optimizer"><code class="name">var <span class="ident">optimizer</span> : ~Optimizer</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.peak_memory"><code class="name">var <span class="ident">peak_memory</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.nn.TrainingState.termination_reason"><code class="name">var <span class="ident">termination_reason</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.nn.TrainingState.current"><code class="name">prop <span class="ident">current</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def current(self) -&gt; int:
    return self.epoch if self.is_epoch_end else self.iter</code></pre>
</details>
</dd>
<dt id="phiml.nn.TrainingState.max"><code class="name">prop <span class="ident">max</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def max(self) -&gt; int:
    return self.max_epochs if self.is_epoch_end else self.max_iter</code></pre>
</details>
</dd>
<dt id="phiml.nn.TrainingState.mean_loss"><code class="name">prop <span class="ident">mean_loss</span> : float</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean_loss(self) -&gt; float:
    return float(self.epoch_loss) if self.is_epoch_end else float(math.mean(self.batch_loss, &#39;dset_linear&#39;))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phiml" href="index.html">phiml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="phiml.nn.adagrad" href="#phiml.nn.adagrad">adagrad</a></code></li>
<li><code><a title="phiml.nn.adam" href="#phiml.nn.adam">adam</a></code></li>
<li><code><a title="phiml.nn.conv_classifier" href="#phiml.nn.conv_classifier">conv_classifier</a></code></li>
<li><code><a title="phiml.nn.conv_net" href="#phiml.nn.conv_net">conv_net</a></code></li>
<li><code><a title="phiml.nn.get_learning_rate" href="#phiml.nn.get_learning_rate">get_learning_rate</a></code></li>
<li><code><a title="phiml.nn.get_parameters" href="#phiml.nn.get_parameters">get_parameters</a></code></li>
<li><code><a title="phiml.nn.invertible_net" href="#phiml.nn.invertible_net">invertible_net</a></code></li>
<li><code><a title="phiml.nn.load_state" href="#phiml.nn.load_state">load_state</a></code></li>
<li><code><a title="phiml.nn.mlp" href="#phiml.nn.mlp">mlp</a></code></li>
<li><code><a title="phiml.nn.parameter_count" href="#phiml.nn.parameter_count">parameter_count</a></code></li>
<li><code><a title="phiml.nn.res_net" href="#phiml.nn.res_net">res_net</a></code></li>
<li><code><a title="phiml.nn.rmsprop" href="#phiml.nn.rmsprop">rmsprop</a></code></li>
<li><code><a title="phiml.nn.save_state" href="#phiml.nn.save_state">save_state</a></code></li>
<li><code><a title="phiml.nn.set_learning_rate" href="#phiml.nn.set_learning_rate">set_learning_rate</a></code></li>
<li><code><a title="phiml.nn.sgd" href="#phiml.nn.sgd">sgd</a></code></li>
<li><code><a title="phiml.nn.train" href="#phiml.nn.train">train</a></code></li>
<li><code><a title="phiml.nn.u_net" href="#phiml.nn.u_net">u_net</a></code></li>
<li><code><a title="phiml.nn.update_weights" href="#phiml.nn.update_weights">update_weights</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phiml.nn.StopTraining" href="#phiml.nn.StopTraining">StopTraining</a></code></h4>
</li>
<li>
<h4><code><a title="phiml.nn.TrainingState" href="#phiml.nn.TrainingState">TrainingState</a></code></h4>
<ul class="">
<li><code><a title="phiml.nn.TrainingState.additional_batch_output" href="#phiml.nn.TrainingState.additional_batch_output">additional_batch_output</a></code></li>
<li><code><a title="phiml.nn.TrainingState.batch_loss" href="#phiml.nn.TrainingState.batch_loss">batch_loss</a></code></li>
<li><code><a title="phiml.nn.TrainingState.current" href="#phiml.nn.TrainingState.current">current</a></code></li>
<li><code><a title="phiml.nn.TrainingState.epoch" href="#phiml.nn.TrainingState.epoch">epoch</a></code></li>
<li><code><a title="phiml.nn.TrainingState.epoch_loss" href="#phiml.nn.TrainingState.epoch_loss">epoch_loss</a></code></li>
<li><code><a title="phiml.nn.TrainingState.indices" href="#phiml.nn.TrainingState.indices">indices</a></code></li>
<li><code><a title="phiml.nn.TrainingState.is_epoch_end" href="#phiml.nn.TrainingState.is_epoch_end">is_epoch_end</a></code></li>
<li><code><a title="phiml.nn.TrainingState.iter" href="#phiml.nn.TrainingState.iter">iter</a></code></li>
<li><code><a title="phiml.nn.TrainingState.learning_rate" href="#phiml.nn.TrainingState.learning_rate">learning_rate</a></code></li>
<li><code><a title="phiml.nn.TrainingState.max" href="#phiml.nn.TrainingState.max">max</a></code></li>
<li><code><a title="phiml.nn.TrainingState.max_epochs" href="#phiml.nn.TrainingState.max_epochs">max_epochs</a></code></li>
<li><code><a title="phiml.nn.TrainingState.max_iter" href="#phiml.nn.TrainingState.max_iter">max_iter</a></code></li>
<li><code><a title="phiml.nn.TrainingState.mean_loss" href="#phiml.nn.TrainingState.mean_loss">mean_loss</a></code></li>
<li><code><a title="phiml.nn.TrainingState.model" href="#phiml.nn.TrainingState.model">model</a></code></li>
<li><code><a title="phiml.nn.TrainingState.name" href="#phiml.nn.TrainingState.name">name</a></code></li>
<li><code><a title="phiml.nn.TrainingState.optimizer" href="#phiml.nn.TrainingState.optimizer">optimizer</a></code></li>
<li><code><a title="phiml.nn.TrainingState.peak_memory" href="#phiml.nn.TrainingState.peak_memory">peak_memory</a></code></li>
<li><code><a title="phiml.nn.TrainingState.termination_reason" href="#phiml.nn.TrainingState.termination_reason">termination_reason</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
