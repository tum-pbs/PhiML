<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>phiml.backend API documentation</title>
<meta name="description" content="Low-level library wrappers for delegating vector operations.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phiml.backend</code></h1>
</header>
<section id="section-intro">
<p>Low-level library wrappers for delegating vector operations.</p>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phiml.backend.jax" href="jax/index.html">phiml.backend.jax</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="phiml.backend.tensorflow" href="tensorflow/index.html">phiml.backend.tensorflow</a></code></dt>
<dd>
<div class="desc"><p>TensorFlow integration.</p></div>
</dd>
<dt><code class="name"><a title="phiml.backend.torch" href="torch/index.html">phiml.backend.torch</a></code></dt>
<dd>
<div class="desc"><p>PyTorch integration.</p></div>
</dd>
<dt><code class="name"><a title="phiml.backend.xops" href="xops.html">phiml.backend.xops</a></code></dt>
<dd>
<div class="desc"><p>Extra operators …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phiml.backend.OBJECTS"><code class="name">var <span class="ident">OBJECTS</span></code></dt>
<dd>
<div class="desc"><p>Backend for Python objects.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phiml.backend.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False) ‑> phiml.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Selects a suitable backend to handle the given values.</p>
<p>This function is used by most math functions operating on <code>Tensor</code> objects to delegate the actual computations.</p>
<p>Backends need to be registered to be available, e.g. via <code>init()</code> or <code>use()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt>*values:</dt>
<dt><strong><code>prefer_default</code></strong></dt>
<dd>Whether to always select the default backend if it can work with <code>values</code>, see <code><a title="phiml.backend.default_backend" href="#phiml.backend.default_backend">default_backend()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The selected <code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code></p></div>
</dd>
<dt id="phiml.backend.context_backend"><code class="name flex">
<span>def <span class="ident">context_backend</span></span>(<span>) ‑> Optional[phiml.backend._backend.Backend]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the backend set by the inner-most surrounding <code>with backend:</code> block.
If called outside a backend context, returns <code>None</code>.</p>
<h2 id="returns">Returns</h2>
<p><code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code> or <code>None</code></p></div>
</dd>
<dt id="phiml.backend.convert"><code class="name flex">
<span>def <span class="ident">convert</span></span>(<span>tensor, backend: phiml.backend._backend.Backend = None, use_dlpack=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a Tensor to the native format of <code>backend</code>.
If the target backend can operate natively on <code>tensor</code>, returns <code>tensor</code>.</p>
<p>If both backends support <em>DLPack</em> and <code>use_dlpack=True</code>, uses zero-copy conversion using the DLPack library.
Else, intermediately converts <code>tensor</code> to a NumPy array.</p>
<p><em>Warning</em>: This operation breaks the automatic differentiation chain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Native tensor belonging to any registered backend.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Target backend. If <code>None</code>, uses the current default backend, see <code><a title="phiml.backend.default_backend" href="#phiml.backend.default_backend">default_backend()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor belonging to <code>backend</code>.</p></div>
</dd>
<dt id="phiml.backend.default_backend"><code class="name flex">
<span>def <span class="ident">default_backend</span></span>(<span>) ‑> phiml.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>The default backend is preferred by <code><a title="phiml.backend.choose_backend" href="#phiml.backend.choose_backend">choose_backend()</a></code>.</p>
<p>The default backend can be set globally using <code><a title="phiml.backend.set_global_default_backend" href="#phiml.backend.set_global_default_backend">set_global_default_backend()</a></code> and locally using <code>with backend:</code>.</p>
<h2 id="returns">Returns</h2>
<p>current default <code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code></p></div>
</dd>
<dt id="phiml.backend.get_current_profile"><code class="name flex">
<span>def <span class="ident">get_current_profile</span></span>(<span>) ‑> Optional[phiml.backend._profile.Profile]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the currently active <code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code> if one is active. Otherwise returns <code>None</code>.</p></div>
</dd>
<dt id="phiml.backend.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phiml.backend.set_global_precision" href="#phiml.backend.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
</dd>
<dt id="phiml.backend.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phiml.backend.set_global_precision" href="#phiml.backend.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
</dd>
<dt id="phiml.backend.profile"><code class="name flex">
<span>def <span class="ident">profile</span></span>(<span>backends=None, trace=True, subtract_trace_time=True, save: Optional[str] = None) ‑> phiml.backend._profile.Profile</span>
</code></dt>
<dd>
<div class="desc"><p>To be used in <code>with</code> statements, <code>with math.backend.profile() as prof: ...</code>.
Creates a <code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code> for the code executed within the context by tracking calls to the <code>backends</code> and optionally tracing the call.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>backends</code></strong></dt>
<dd>List of backends to profile, <code>None</code> to profile all.</dd>
<dt><strong><code>trace</code></strong></dt>
<dd>Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.</dd>
<dt><strong><code>subtract_trace_time</code></strong></dt>
<dd>If True, subtracts the time it took to trace the call stack from the event times</dd>
<dt><strong><code>save</code></strong></dt>
<dd>(Optional) File path to save the profile to. This will call <code><a title="phiml.backend.Profile.save" href="#phiml.backend.Profile.save">Profile.save()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Created <code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code></p></div>
</dd>
<dt id="phiml.backend.profile_function"><code class="name flex">
<span>def <span class="ident">profile_function</span></span>(<span>fun: Callable, args: Union[tuple, list] = (), kwargs: Optional[dict] = None, backends=None, trace=True, subtract_trace_time=True, retime=True, warmup=1, call_count=1) ‑> phiml.backend._profile.Profile</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code> for the function <code>fun(*args, **kwargs)</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>fun</code></strong></dt>
<dd>Function to be profiled. In case <code>retime=True</code>, this function must perform the same operations each time it is called.
Use <code>warmup&gt;0</code> to ensure that internal caching does not interfere with the operations.</dd>
<dt><strong><code>args</code></strong></dt>
<dd>Arguments to be passed to <code>fun</code>.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Keyword arguments to be passed to <code>fun</code>.</dd>
<dt><strong><code>backends</code></strong></dt>
<dd>List of backends to profile, <code>None</code> to profile all.</dd>
<dt><strong><code>trace</code></strong></dt>
<dd>Whether to perform a full stack trace for each backend call. If true, groups backend calls by function.</dd>
<dt><strong><code>subtract_trace_time</code></strong></dt>
<dd>If True, subtracts the time it took to trace the call stack from the event times. Has no effect if <code>retime=True</code>.</dd>
<dt><strong><code>retime</code></strong></dt>
<dd>If true, calls <code>fun</code> another time without tracing the calls and updates the profile.
This gives a much better indication of the true timing.
See <code><a title="phiml.backend.Profile.retime" href="#phiml.backend.Profile.retime">Profile.retime()</a></code>.</dd>
<dt><strong><code>warmup</code></strong></dt>
<dd>Number of times to call <code>fun</code> before profiling it.</dd>
<dt><strong><code>call_count</code></strong></dt>
<dd>How often to call the function (excluding retime and warmup). The times will be averaged over multiple runs if <code>call_count &gt; 1</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Created <code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code> for <code>fun</code>.</p></div>
</dd>
<dt id="phiml.backend.set_global_default_backend"><code class="name flex">
<span>def <span class="ident">set_global_default_backend</span></span>(<span>backend: Union[str, phiml.backend._backend.Backend]) ‑> phiml.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the given backend as default.
This setting can be overridden using <code>with backend:</code>.</p>
<p>See <code><a title="phiml.backend.default_backend" href="#phiml.backend.default_backend">default_backend()</a></code>, <code><a title="phiml.backend.choose_backend" href="#phiml.backend.choose_backend">choose_backend()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>backend</code></strong></dt>
<dd><code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code> or backend name to set as default.
Possible names are <code>'torch'</code>, <code>'tensorflow'</code>, <code>'jax'</code>, <code>'numpy'</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The chosen backend as a `Backend´ instance.</p></div>
</dd>
<dt id="phiml.backend.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phiml.backend.Backend"><code class="flex name class">
<span>class <span class="ident">Backend</span></span>
<span>(</span><span>name: str, devices: List[phiml.backend._backend.ComputeDevice], default_device: phiml.backend._backend.ComputeDevice)</span>
</code></dt>
<dd>
<div class="desc"><p>Backends delegate low-level operations to a ML or numerics library or emulate them.
The methods of <code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code> form a comprehensive list of available operations.</p>
<p>To support a library, subclass <code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code> and register it by adding it to <code>BACKENDS</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Human-readable string</dd>
<dt><strong><code>default_device</code></strong></dt>
<dd><code><a title="phiml.backend.ComputeDevice" href="#phiml.backend.ComputeDevice">ComputeDevice</a></code> being used by default</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Backend:
    &#34;&#34;&#34;
    Backends delegate low-level operations to a ML or numerics library or emulate them.
    The methods of `Backend` form a comprehensive list of available operations.

    To support a library, subclass `Backend` and register it by adding it to `BACKENDS`.
    &#34;&#34;&#34;

    def __init__(self, name: str, devices: List[ComputeDevice], default_device: ComputeDevice):
        &#34;&#34;&#34;
        Args:
            name: Human-readable string
            default_device: `ComputeDevice` being used by default
        &#34;&#34;&#34;
        self._name = name
        self._devices = tuple(devices)
        self._default_device = default_device

    def __getstate__(self):
        warnings.warn(f&#34;Backend.__getstate__ called on {self}. Pickling backends is highly discouraged as it can lead to problems when unpickling in different environments.&#34;)
        return self.__dict__

    def __enter__(self):
        _DEFAULT.append(self)

    def __exit__(self, exc_type, exc_val, exc_tb):
        _DEFAULT.pop(-1)

    @property
    def name(self) -&gt; str:
        return self._name

    def supports(self, feature: Union[str, Callable]) -&gt; bool:
        &#34;&#34;&#34;
        Tests if this backend supports the given feature.
        Features correspond to a method of this backend that must be implemented if the feature is supported.

        Possible features:

        * `sparse_coo_tensor`
        * `gradients

        Args:
            feature: `str` or unbound Backend method, e.g. `Backend.sparse_coo_tensor`

        Returns:
            Whether the feature is supported.
        &#34;&#34;&#34;
        feature = feature if isinstance(feature, str) else feature.__name__
        if not hasattr(Backend, feature):
            raise ValueError(f&#34;Not a valid feature: &#39;{feature}&#39;&#34;)
        backend_fun = getattr(Backend, feature)
        impl_fun = getattr(self.__class__, feature)
        return impl_fun is not backend_fun

    def prefers_channels_last(self) -&gt; bool:
        raise NotImplementedError(self.__class__)

    def requires_fixed_shapes_when_tracing(self) -&gt; bool:
        return False

    @property
    def precision(self) -&gt; int:
        &#34;&#34;&#34; Short for math.backend.get_precision() &#34;&#34;&#34;
        return get_precision()

    @property
    def float_type(self) -&gt; DType:
        return DType.by_precision(float, self.precision)

    @property
    def as_registered(self) -&gt; &#39;Backend&#39;:
        from . import BACKENDS
        for backend in BACKENDS:
            if self.name in backend.name:
                return backend
        if not init_backend(self.name):
            raise RuntimeError(f&#34;Backend &#39;{self}&#39; is not registered. Registered backends are: {BACKENDS}&#34;)
        return self.as_registered

    def nn_library(self):
        raise NotImplementedError(self)

    @property
    def complex_type(self) -&gt; DType:
        return DType.by_precision(complex, max(64, self.precision))

    def combine_types(self, *dtypes: DType) -&gt; DType:
        return combine_types(*dtypes, fp_precision=self.precision)

    def auto_cast(self, *tensors, bool_to_int=False, int_to_float=False) -&gt; list:
        &#34;&#34;&#34;
        Determines the appropriate values type resulting from operations involving the tensors as input.

        This method is called by the default implementations of basic operators.
        Backends can override this method to prevent unnecessary casting.

        Args:
            *tensors: tensors to cast and to consider when determining the common data type
            bool_to_int: Whether to convert boolean values to integers if all values are boolean.

        Returns:
            tensors cast to a common data type
        &#34;&#34;&#34;
        dtypes = [self.dtype(t) for t in tensors]
        result_type = combine_types(*dtypes, fp_precision=get_precision())
        if result_type.kind == bool and bool_to_int:
            result_type = INT32
        if result_type.kind == int and int_to_float:
            result_type = DType.by_precision(float, self.precision)
        if result_type.kind in (int, float, complex, bool):  # do not cast everything to string!
            tensors = [self.cast(t, result_type) for t in tensors]
        return tensors

    def auto_cast1(self, tensor):
        if isinstance(tensor, (bool, int, float, complex)):
            return tensor
        dtype = self.dtype(tensor)
        if dtype.kind in {int, bool}:
            return tensor
        if dtype.precision != get_precision():
            result_type = DType.by_precision(dtype.kind, precision=get_precision())
            return self.cast(tensor, result_type)
        return tensor

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.name

    def list_devices(self, device_type: Union[str, None] = None) -&gt; List[ComputeDevice]:
        &#34;&#34;&#34;
        Fetches information about all available compute devices this backend can use.

        Implementations:

        * NumPy: [`os.cpu_count`](https://docs.python.org/3/library/os.html#os.cpu_count)
        * PyTorch: [`torch.cuda.get_device_properties`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.get_device_properties)
        * TensorFlow: `tensorflow.python.client.device_lib.list_local_devices`
        * Jax: [`jax.devices`](https://jax.readthedocs.io/en/latest/jax.html#jax.devices)

        See Also:
            `Backend.set_default_device()`.

        Args:
            device_type: (optional) Return only devices of this type, e.g. `&#39;GPU&#39;` or `&#39;CPU&#39;`. See `ComputeDevice.device_type`.

        Returns:
            `list` of all currently available devices.
        &#34;&#34;&#34;
        if device_type is None:
            return list(self._devices)
        else:
            assert device_type in (&#39;CPU&#39;, &#39;GPU&#39;, &#39;TPU&#39;), &#34;Device&#34;
            return [d for d in self._devices if d.device_type == device_type]

    def get_default_device(self) -&gt; ComputeDevice:
        return self._default_device

    def set_default_device(self, device: Union[ComputeDevice, str]) -&gt; bool:
        &#34;&#34;&#34;
        Sets the device new tensors will be allocated on.
        This function will do nothing if the target device type is not available.

        See Also:
            `Backend.list_devices()`, `Backend.get_default_device()`.

        Args:
            device: `ComputeDevice` or device type as `str`, such as `&#39;CPU&#39;` or `&#39;GPU&#39;`.

        Returns:
            `bool` whether the device was successfully set.
        &#34;&#34;&#34;
        if isinstance(device, str):
            devices = self.list_devices(device)
            if not devices:
                warnings.warn(f&#34;{self.name}: Cannot select &#39;{device}&#39; because no device of this type is available.&#34;, RuntimeWarning)
                return False
            device = devices[0]
        assert device.backend is self, f&#34;Cannot set default device to {device.name} for backend {self.name} because the devices belongs to backend {device.backend.name}&#34;
        self._default_device = device
        return True

    def get_device(self, tensor: TensorType) -&gt; ComputeDevice:
        &#34;&#34;&#34; Returns the device `tensor` is located on. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def get_device_by_ref(self, ref):
        for device in self._devices:
            if device.ref == ref:
                return device
        raise KeyError(f&#34;{self.name} has no device with ref &#39;{ref}&#39;. Available: {[d.ref for d in self._devices]}&#34;)

    def allocate_on_device(self, tensor: TensorType, device: ComputeDevice) -&gt; TensorType:
        &#34;&#34;&#34;
        Moves `tensor` to `device`. May copy the tensor if it is already on the device.

        Args:
            tensor: Existing tensor native to this backend.
            device: Target device, associated with this backend.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def get_peak_memory(self, device: ComputeDevice):
        raise NotImplementedError(self.__class__)

    def reset_peak_memory(self, device: ComputeDevice):
        raise NotImplementedError(self.__class__)

    def seed(self, seed: int):
        raise NotImplementedError(self.__class__)

    def is_module(self, obj) -&gt; bool:
        &#34;&#34;&#34;
        Tests if `obj` is of a type that is specific to this backend, e.g. a neural network.
        If `True`, this backend will be chosen for operations involving `obj`.

        See Also:
            `Backend.is_tensor()`.

        Args:
            obj: Object to test.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def is_tensor(self, x, only_native=False):
        &#34;&#34;&#34;
        An object is considered a native tensor by a backend if no internal conversion is required by backend methods.
        An object is considered a tensor (nativer or otherwise) by a backend if it is not a struct (e.g. tuple, list) and all methods of the backend accept it as a tensor argument.

        If `True`, this backend will be chosen for operations involving `x`.

        See Also:
            `Backend.is_module()`.

        Args:
          x: object to check
          only_native: If True, only accepts true native tensor representations, not Python numbers or others that are also supported as tensors (Default value = False)

        Returns:
          bool: whether `x` is considered a tensor by this backend

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def is_sparse(self, x) -&gt; bool:
        &#34;&#34;&#34;
        Args:
            x: Tensor native to this `Backend`.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def get_sparse_format(self, x) -&gt; str:
        &#34;&#34;&#34;Returns lower-case format string, such as &#39;coo&#39;, &#39;csr&#39;, &#39;csc&#39; &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def disassemble(self, x) -&gt; Tuple[Callable, Sequence[TensorType]]:
        &#34;&#34;&#34;
        Disassemble a (sparse) tensor into its individual constituents, such as values and indices.

        Args:
            x: Tensor

        Returns:
            assemble: Function `assemble(backend, *constituents)` that reassembles `x` from the constituents.
            constituents: Tensors contained in `x`.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def as_tensor(self, x, convert_external=True):
        &#34;&#34;&#34;
        Converts a tensor-like object to the native tensor representation of this backend.
        If x is a native tensor of this backend, it is returned without modification.
        If x is a Python number (numbers.Number instance), `convert_numbers` decides whether to convert it unless the backend cannot handle Python numbers.
        
        *Note:* There may be objects that are considered tensors by this backend but are not native and thus, will be converted by this method.

        Args:
          x: tensor-like, e.g. list, tuple, Python number, tensor
          convert_external: if False and `x` is a Python number that is understood by this backend, this method returns the number as-is. This can help prevent type clashes like int32 vs int64. (Default value = True)

        Returns:
          tensor representation of `x`

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def variable(self, x):
        raise NotImplementedError(self.__class__)

    def module(self, variables: Dict[str, TensorType], forward: Callable = None):
        raise NotImplementedError(self.__class__)

    def is_available(self, tensor) -&gt; bool:
        &#34;&#34;&#34;
        Tests if the value of the tensor is known and can be read at this point.
        If true, `numpy(tensor)` must return a valid NumPy representation of the value.
        
        Tensors are typically available when the backend operates in eager mode.

        Args:
          tensor: backend-compatible tensor

        Returns:
          bool

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def numpy(self, tensor) -&gt; numpy.ndarray:
        &#34;&#34;&#34;
        Returns a NumPy representation of the given tensor.
        If `tensor` is already a NumPy array, it is returned without modification.
        
        This method raises an error if the value of the tensor is not known at this point, e.g. because it represents a node in a graph.
        Use `is_available(tensor)` to check if the value can be represented as a NumPy array.

        Args:
            tensor: backend-compatible tensor or sparse tensor

        Returns:
          NumPy representation of the values stored in the tensor

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def to_dlpack(self, tensor):
        raise NotImplementedError(self.__class__)

    def from_dlpack(self, capsule):
        raise NotImplementedError(self.__class__)

    def copy(self, tensor, only_mutable=False):
        raise NotImplementedError(self.__class__)

    def copy_leaves(self, tree, only_mutable=False):
        if isinstance(tree, tuple):
            return tuple([self.copy_leaves(e, only_mutable) for e in tree])
        elif isinstance(tree, list):
            return [self.copy_leaves(e, only_mutable) for e in tree]
        elif isinstance(tree, dict):
            return {k: self.copy_leaves(e, only_mutable) for k, e in tree.items()}
        else:
            return self.copy(tree, only_mutable=only_mutable)

    def call(self, f: Callable, *args, name=None):
        &#34;&#34;&#34;
        Calls `f(*args)` and returns the result.
        This method may be used to register internal calls with the profiler.

        Usage:

            choose_backend(key).call(custom_function, *args)
        &#34;&#34;&#34;
        return f(*args)

    def block_until_ready(self, values):
        pass

    def vectorized_call(self, f, *args, output_dtypes=None, **aux_args):
        &#34;&#34;&#34;
        Args:
            f: Function with only positional tensor argument, returning one or multiple tensors.
            *args: Batched inputs for `f`. The first dimension of all `args` is vectorized.
                All tensors in `args` must have the same size or `1` in their first dimension.
            output_dtypes: Single `DType` or tuple of DTypes declaring the dtypes of the tensors returned by `f`.
            **aux_args: Non-vectorized keyword arguments to be passed to `f`.
        &#34;&#34;&#34;
        batch_dim = self.determine_size(args, 0)
        result = []
        for b in range(batch_dim):
            result.append(f(*[t[min(b, self.staticshape(t)[0] - 1)] for t in args], **aux_args))
        return self.stack(result)

    def numpy_call(self, f, output_shapes, output_dtypes, *args, **aux_args):
        &#34;&#34;&#34;
        This call can be used in jit-compiled code but is not differentiable.

        Args:
            f: Function operating on numpy arrays.
            output_shapes: Single shape `tuple` or tuple of shapes declaring the shapes of the tensors returned by `f`.
            output_dtypes: Single `DType` or tuple of DTypes declaring the dtypes of the tensors returned by `f`.
            *args: Tensor arguments to be converted to NumPy arrays and then passed to `f`.
            **aux_args: Keyword arguments to be passed to `f` without conversion.

        Returns:
            Returned arrays of `f` converted to tensors.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def determine_size(self, tensors, axis):
        sizes = [self.staticshape(t)[axis] for t in tensors]
        non_singleton_sizes = [b for b in sizes if b != 1]
        size = non_singleton_sizes[0] if non_singleton_sizes else 1
        assert all([b in (1, size) for b in sizes])
        return size

    def tile_to(self, x, axis, size):
        current_size = self.staticshape(x)[axis]
        if current_size == size:
            return x
        assert size &gt; current_size
        assert size % current_size == 0
        multiples = [size // current_size if i == axis else 1 for i in range(self.ndims(x))]
        return self.tile(x, multiples)

    def jit_compile(self, f: Callable) -&gt; Callable:
        raise NotImplementedError(self.__class__)

    def jacobian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
        &#34;&#34;&#34;
        Args:
            f: Function to differentiate. Returns a tuple containing `(reduced_loss, output)`
            wrt: Argument indices for which to compute the gradient.
            get_output: Whether the derivative function should return the output of `f` in addition to the gradient.
            is_f_scalar: Whether `f` is guaranteed to return a scalar output.

        Returns:
            A function `g` with the same arguments as `f`.
            If `get_output=True`, `g` returns a `tuple`containing the outputs of `f` followed by the gradients.
            The gradients retain the dimensions of `reduced_loss` in order as outer (first) dimensions.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool) -&gt; tuple:
        &#34;&#34;&#34;
        First dimension of all inputs/outputs of `f` is assumed to be a batch dimension.
        Element-wise Hessians will be computed along the batch dimension.
        All other dimensions are parameter dimensions and will appear twice in the Hessian matrices.

        Args:
            f: Function whose first output is a scalar float or complex value.
            wrt:
            get_output:
            get_gradient:

        Returns:
            Function returning `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
            The result is always a `tuple` holding at most these three items.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def custom_gradient(self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) -&gt; Callable:
        &#34;&#34;&#34;
        Creates a function based on `f` that uses a custom gradient for backprop.

        Args:
            f: Forward function.
            gradient: Function for backprop. Will be called as `gradient(*d_out)` to compute the gradient of `f`.

        Returns:
            Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
        &#34;&#34;&#34;
        return NotImplemented

    def jit_compile_grad(self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool):
        raise NotImplementedError(self.__class__)

    def jit_compile_hessian(self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool):
        raise NotImplementedError(self.__class__)

    def transpose(self, tensor, axes):
        &#34;&#34;&#34; Transposes the dimensions of `tensor` given the new axes order. The tensor will be cast to the default precision in the process. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def random_uniform(self, shape, low, high, dtype: Union[DType, None]):
        &#34;&#34;&#34; Float tensor of selected precision containing random values in the range [0, 1) &#34;&#34;&#34;
        raise NotImplementedError(self)

    def random_normal(self, shape, dtype: DType):
        &#34;&#34;&#34; Float tensor of selected precision containing random values sampled from a normal distribution with mean 0 and std 1. &#34;&#34;&#34;
        raise NotImplementedError(self)

    def random_permutations(self, permutations: int, n: int):
        &#34;&#34;&#34;Generate `permutations` stacked arrays of shuffled integers between `0` and `n`.&#34;&#34;&#34;
        raise NotImplementedError(self)

    def random_subsets(self, element_count: int, subset_size: int, subset_count: int, allow_duplicates: bool, element_weights=None):
        raise NotImplementedError(self)

    def stack(self, values, axis=0):
        raise NotImplementedError(self)

    def stack_leaves(self, trees: Union[tuple, list], axis=0):
        tree0 = trees[0]
        if isinstance(tree0, tuple):
            return tuple([self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))])
        elif isinstance(tree0, list):
            return [self.stack_leaves([tree[i] for tree in trees], axis=axis) for i in range(len(tree0))]
        elif isinstance(tree0, dict):
            return {k: self.stack_leaves([tree[k] for tree in trees], axis=axis) for k in tree0}
        else:
            return self.stack(trees, axis=axis)

    def pad_stack(self, tensors, shape, pad_value=0):
        padded = []
        for t in tensors:
            widths = [(0, target - current) for target, current in zip(shape, self.shape(t))]
            padded.append(self.pad(t, widths, constant_values=pad_value))
        return self.stack(padded)

    def concat(self, values, axis):
        raise NotImplementedError(self)

    def pad(self, value, pad_width, mode: str = &#39;constant&#39;, constant_values=0):
        &#34;&#34;&#34;
        Pad a tensor with values as specified by `mode` and `constant_values`.
        
        If the mode is not supported, returns NotImplemented.

        Args:
            value: tensor
            pad_width: 2D tensor specifying the number of values padded to the edges of each axis in the form [[axis 0 lower, axis 0 upper], ...] including batch and component axes.
            mode: constant&#39;, &#39;boundary&#39;, &#39;periodic&#39;, &#39;symmetric&#39;, &#39;reflect&#39;
            constant_values: Scalar value used for out-of-bounds points if mode=&#39;constant&#39;. Must be a Python primitive type or scalar tensor.
            mode: str:  (Default value = &#39;constant&#39;)

        Returns:
            padded tensor or `NotImplemented`

        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def pad_to(self, x, axis, new_size, fill_value):
        shape = self.staticshape(x)
        current = shape[axis]
        if new_size &gt; current:
            pad_width = [(0, new_size - current) if i == axis else (0, 0) for i in range(len(shape))]
            return self.pad(x, pad_width, mode=&#39;constant&#39;, constant_values=fill_value)
        elif new_size &lt; current:
            return x[tuple([slice(new_size) if i == axis else slice(None) for i in range(len(shape))])]
        else:
            return x

    def reshape(self, value, shape):
        raise NotImplementedError(self)

    def flip(self, value, axes: Union[tuple, list]):
        slices = tuple(slice(None, None, -1 if i in axes else None) for i in range(self.ndims(value)))
        return value[slices]

    def sum(self, value, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def prod(self, value, axis=None):
        raise NotImplementedError(self)

    def divide_no_nan(self, x, y):
        &#34;&#34;&#34; Computes x/y but returns 0 if y=0. &#34;&#34;&#34;
        raise NotImplementedError(self)

    def where(self, condition, x=None, y=None):
        raise NotImplementedError(self)

    def nonzero(self, values, length=None, fill_value=-1):
        &#34;&#34;&#34;
        Args:
            values: Tensor with only spatial dimensions
            length: (Optional) Length of the resulting array. If specified, the result array will be padded with `fill_value` or trimmed.

        Returns:
            non-zero multi-indices as tensor of shape (nnz/length, vector)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def mean(self, value, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def range(self, start, limit=None, delta=1, dtype: DType = INT32):
        raise NotImplementedError(self)

    def zeros(self, shape, dtype: DType = None):
        raise NotImplementedError(self)

    def zeros_like(self, tensor):
        raise NotImplementedError(self)

    def ones(self, shape, dtype: DType = None):
        raise NotImplementedError(self)

    def ones_like(self, tensor):
        raise NotImplementedError(self)

    def meshgrid(self, *coordinates):
        raise NotImplementedError(self)

    def linspace(self, start, stop, number):
        raise NotImplementedError(self)

    def linspace_without_last(self, start, stop, number):
        return self.linspace(start, stop, number+1)[:-1]

    def tensordot(self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list]):
        &#34;&#34;&#34; Multiply-sum-reduce a_axes of a with b_axes of b. &#34;&#34;&#34;
        raise NotImplementedError(self)

    def mul_matrix_batched_vector(self, A, b):
        raise NotImplementedError(self)

    def einsum(self, equation, *tensors):
        raise NotImplementedError(self)

    def cumsum(self, x, axis: int):
        raise NotImplementedError(self)

    def while_loop(self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]]):
        &#34;&#34;&#34;
        If `max_iter is None`, runs

        ```python
        while any(values[0]):
            values = loop(*values)
        return values
        ```

        This operation does not support backpropagation.

        Args:
            loop: Loop function, must return a `tuple` with entries equal to `values` in shape and data type.
            values: Initial values of loop variables.
            max_iter: Maximum number of iterations to run, single `int` or sequence of integers.
        Returns:
            Loop variables upon loop completion if `max_iter` is a single integer.
            If `max_iter` is a sequence, stacks the variables after each entry in `max_iter`, adding an outer dimension of size `&lt;= len(max_iter)`.
            If the condition is fulfilled before the maximum max_iter is reached, the loop may be broken or not, depending on the implementation.
            If the loop is broken, the values returned by the last loop are expected to be constant and filled.
        &#34;&#34;&#34;
        values = self.stop_gradient_tree(values)
        if isinstance(max_iter, (tuple, list)):
            trj = [self.copy_leaves(values, only_mutable=True)] if 0 in max_iter else []
            for i in range(1, max(max_iter) + 1):
                values = loop(*values)
                if i in max_iter:
                    trj.append(self.copy_leaves(values, only_mutable=True))
                if not self.any(values[0]):
                    break
            trj.extend([trj[-1]] * (len(max_iter) - len(trj)))  # fill trj with final values
            return self.stop_gradient_tree(self.stack_leaves(trj))
        else:
            for i in range(1, max_iter + 1):
                if not self.any(values[0]):
                    break
                values = loop(*values)
            return self.stop_gradient_tree(values)

    def abs(self, x):
        raise NotImplementedError(self)

    def sign(self, x):
        raise NotImplementedError(self)

    def round(self, x):
        raise NotImplementedError(self)

    def ceil(self, x):
        raise NotImplementedError(self)

    def floor(self, x):
        raise NotImplementedError(self)

    def max(self, x, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def min(self, x, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def maximum(self, a, b):
        raise NotImplementedError(self)

    def minimum(self, a, b):
        raise NotImplementedError(self)

    def clip(self, x, minimum, maximum):
        raise NotImplementedError(self)

    def argmax(self, x, axis: int, keepdims=False):
        raise NotImplementedError(self)

    def argmin(self, x, axis: int, keepdims=False):
        raise NotImplementedError(self)

    def sqrt(self, x):
        raise NotImplementedError(self)

    def exp(self, x):
        raise NotImplementedError(self)

    def erf(self, x):
        raise NotImplementedError(self)

    def softplus(self, x):
        raise NotImplementedError(self)

    def log_gamma(self, x):
        raise NotImplementedError(self)

    def gamma_inc_l(self, a, x):
        &#34;&#34;&#34;Regularized lower incomplete gamma function.&#34;&#34;&#34;
        raise NotImplementedError(self)

    def gamma_inc_u(self, a, x):
        &#34;&#34;&#34;Regularized upper incomplete gamma function.&#34;&#34;&#34;
        raise NotImplementedError(self)

    def factorial(self, x: TensorType) -&gt; TensorType:
        if self.dtype(x).kind == int:
            import scipy
            max_factorial = {32: 12, 64: 19}[self.dtype(x).bits]
            factorial_list = [int(scipy.special.factorial(i)) for i in range(max_factorial+1)]
            return self.gather(self.cast(self.as_tensor(factorial_list), self.dtype(x)), x, 0)
        else:
            return self.exp(self.log_gamma(self.to_float(x) + 1))

    def conv(self, value, kernel, strides: Sequence[int], out_sizes: Sequence[int], transpose: bool):
        &#34;&#34;&#34;
        Convolve value with kernel.
        Depending on the tensor rank, the convolution is either 1D (rank=3), 2D (rank=4) or 3D (rank=5).
        Higher dimensions may not be supported.

        Args:
            value: tensor of shape (batch_size, in_channel, spatial...)
            kernel: tensor of shape (batch_size or 1, out_channel, in_channel, spatial...)
            strides: Convolution strides, one `int` for each spatial dim. For transpose, they act as upsampling factors.
            out_sizes: Spatial shape of the output tensor. This determines how much zero-padding or slicing is used.
            transpose: If `True`, performs a transposed convolution, according to PyTorch&#39;s definition.

        Returns:
            Convolution result as tensor of shape (batch_size, out_channel, spatial...)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def expand_dims(self, a, axis=0, number=1):
        raise NotImplementedError(self)

    def shape(self, tensor):
        &#34;&#34;&#34;
        Returns the shape of a tensor.
        The shape is iterable and implements `len()`.
        For non-eager tensors, undefined dimensions should return a placeholder value representing the size.

        See Also:
            `Backend.staticshape()`.

        Args:
            tensor: Native tensor compatible with this backend.

        Returns:
            Shape of `tensor`
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def staticshape(self, tensor) -&gt; tuple:
        &#34;&#34;&#34;
        Evaluates the static shape of a native tensor.
        If the tensor is eager, the shape is a `tuple[int]`.
        For placeholder tensors, unknown dimensions are represented as `None`.

        See Also:
            `Backend.shape()`.

        Args:
            tensor: Native tensor compatible with this backend.

        Returns:
            `tuple` of sizes. Each size is an `int` if the size is defined, else `None`.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def sizeof(self, tensor) -&gt; int:
        &#34;&#34;&#34;Returns the size in bytes&#34;&#34;&#34;
        raise NotImplementedError(self)

    def cast(self, x, dtype: DType):
        raise NotImplementedError(self)

    def to_float(self, x):
        &#34;&#34;&#34;
        Converts a tensor to floating point values with precision equal to the currently set default precision.

        See Also:
            `Backend.precision()`.

        If `x` is mutable and of the correct floating type, returns a copy of `x`.

        To convert float tensors to the backend precision but leave non-float tensors untouched, use `Backend.as_tensor()`.

        Args:
            x: tensor of bool, int or float

        Returns:
            Values of `x` as float tensor
        &#34;&#34;&#34;
        return self.cast(x, self.float_type)

    def to_int32(self, x):
        return self.cast(x, INT32)

    def to_int64(self, x):
        return self.cast(x, INT64)

    def to_complex(self, x):
        return self.cast(x, DType.by_precision(complex, max(32, self.precision)))

    def unravel_index(self, flat_index, shape):
        strides = [1]
        for size in reversed(shape[1:]):
            strides.append(strides[-1] * size)
        strides = strides[::-1]
        result = []
        for i in range(len(shape)):
            result.append(flat_index // strides[i] % shape[i])
        return self.stack(result, -1)

    def ravel_multi_index(self, multi_index, shape, mode: Union[str, int] = &#39;undefined&#39;):
        &#34;&#34;&#34;
        Args:
            multi_index: (batch..., index_dim)
            shape: 1D tensor or tuple/list
            mode: `&#39;undefined&#39;`, `&#39;periodic&#39;`, `&#39;clamp&#39;` or an `int` to use for all invalid indices.

        Returns:
            Integer tensor of shape (batch...) of same dtype as `multi_index`.
        &#34;&#34;&#34;
        strides = [self.ones((), self.dtype(multi_index))]
        for size in reversed(shape[1:]):
            strides.append(strides[-1] * size)
        strides = self.stack(strides[::-1])
        if mode == &#39;periodic&#39;:
            multi_index %= self.as_tensor(shape)
        elif mode == &#39;clamp&#39;:
            multi_index = self.clip(multi_index, 0, self.as_tensor(shape) - 1)
        result = self.sum(multi_index * strides, -1)
        if isinstance(mode, int):
            inside = self.all((0 &lt;= multi_index) &amp; (multi_index &lt; self.as_tensor(shape)), -1)
            result = self.where(inside, result, mode)
        return result

    def gather(self, values, indices, axis: int):
        &#34;&#34;&#34;
        Gathers values from the tensor `values` at locations `indices`.

        Args:
            values: tensor
            indices: 1D tensor
            axis: Axis along which to gather slices

        Returns:
            tensor, with size along `axis` being the length of `indices`
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def gather_by_component_indices(self, values, *component_indices):
        return values[component_indices]

    def batched_gather_nd(self, values, indices):
        &#34;&#34;&#34;
        Gathers values from the tensor `values` at locations `indices`.
        The first dimension of `values` and `indices` is the batch dimension which must be either equal for both or one for either.

        Args:
            values: tensor of shape (batch, spatial..., channel)
            indices: int tensor of shape (batch, any..., multi_index) where the size of multi_index is values.rank - 2.

        Returns:
            Gathered values as tensor of shape (batch, any..., channel)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def batched_gather_1d(self, values, indices):
        &#34;&#34;&#34;
        Args:
            values: (batch, spatial)
            indices: (batch, indices)

        Returns:
            (batch, indices)
        &#34;&#34;&#34;
        return self.batched_gather_nd(values[:, :, None], indices[:, :, None])[..., 0]

    def gather_nd(self, values, indices):
        &#34;&#34;&#34;
        Args:
            values: (spatial, channels)
            indices: (indices, multi_index)

        Returns:
            (indices, channels)
        &#34;&#34;&#34;
        return self.batched_gather_nd(values[None, :, :], indices[None, :, :])[0, :, :]

    def gather_1d(self, values, indices):
        return self.gather(values, indices, 0)

    def flatten(self, x):
        return self.reshape(x, (-1,))

    def std(self, x, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def boolean_mask(self, x, mask, axis=0, new_length=None, fill_value=0):
        &#34;&#34;&#34;
        Args:
            x: tensor with any number of dimensions
            mask: 1D mask tensor
            axis: Axis index &gt;= 0
            new_length: Maximum size of the output along `axis`. This must be set when jit-compiling with Jax.
            fill_value: If `new_length` is larger than the filtered result, the remaining values will be set to `fill_value`.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def isfinite(self, x):
        raise NotImplementedError(self)

    def isnan(self, x):
        raise NotImplementedError(self)

    def isinf(self, x):
        raise NotImplementedError(self)

    def scatter(self, base_grid, indices, values, mode: str):
        &#34;&#34;&#34;
        Batched n-dimensional scatter.

        Args:
            base_grid: Tensor into which scatter values are inserted at indices. Tensor of shape (batch_size, spatial..., channels)
            indices: Tensor of shape (batch_size or 1, update_count, index_vector)
            values: Values to scatter at indices. Tensor of shape (batch_size or 1, update_count or 1, channels or 1)
            mode: One of (&#39;update&#39;, &#39;add&#39;, &#39;max&#39;, &#39;min&#39;)

        Returns:
            Copy of base_grid with values at `indices` updated by `values`.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def scatter_nd(self, base_grid, indices, values, mode: str):
        &#34;&#34;&#34;
        Non-batched scatter.

        Args:
            base_grid: (spatial..., channels)
            indices: (update_count, index_vector)
            values: (update_count or 1, channels or 1)
            mode: One of (&#39;update&#39;, &#39;add&#39;)
        &#34;&#34;&#34;
        return self.scatter(base_grid[None, ...], indices[None, ...], values[None, ...], mode=mode)[0, ...]

    def scatter_1d_scalar(self, base_grid, indices, values, mode: str):
        &#34;&#34;&#34;
        Args:
            base_grid: (spatial,)
            indices: (update_count,)
            values: (update_count or 1,)
            mode: One of (&#39;update&#39;, &#39;add&#39;)
        &#34;&#34;&#34;
        return self.scatter(base_grid[None, :, None], indices[None, :, None], values[None, :, None], mode=mode)[0, :, 0]

    def scatter_nd_scalar(self, base_grid, indices, values, mode: str):
        &#34;&#34;&#34;
        Args:
            base_grid: (spatial...,)
            indices: (update_count, index_vector)
            values: (update_count or 1,)
            mode: One of (&#39;update&#39;, &#39;add&#39;)
        &#34;&#34;&#34;
        return self.scatter(base_grid[None, ..., None], indices[None, ...], values[None, :, None], mode=mode)[0, ..., 0]

    def histogram1d(self, values, weights, bin_edges):
        &#34;&#34;&#34;
        Args:
            values: (batch, values)
            bin_edges: (batch, edges)
            weights: (batch, values)

        Returns:
            (batch, edges) with dtype matching weights
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def bincount(self, x, weights: Optional[TensorType], bins: int, x_sorted=False):
        &#34;&#34;&#34;
        Args:
            x: Bin indices, 1D int tensor.
            weights: Weights corresponding to `x`, 1D tensor. All weights are 1 if `weights=None`.
            bins: Number of bins.
            x_sorted: Whether `x` is sorted from lowest to highest bin.

        Returns:
            bin_counts
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def batched_bincount(self, x, weights: Optional[TensorType], bins: int):
        if weights is None:
            return self.vectorized_call(self.bincount, x, weights=None, bins=bins)
        else:
            return self.vectorized_call(self.bincount, x, weights, bins=bins)

    def unique(self, x: TensorType, return_inverse: bool, return_counts: bool, axis: int) -&gt; Tuple[TensorType, ...]:
        &#34;&#34;&#34;
        Args:
            x: n-dimensional int array. Will compare `axis`-slices of `x` for multidimensional `x`.
            return_inverse: Whether to return the inverse
            return_counts: Whether to return the counts.
            axis: Axis along which slices of `x` should be compared.

        Returns:
            unique_slices: Sorted unique slices of `x`
            unique_inverse: (optional) index of the unique slice for each slice of `x`
            unique_counts: Number of occurrences of each unique slices
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def any(self, boolean_tensor, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def all(self, boolean_tensor, axis=None, keepdims=False):
        raise NotImplementedError(self)

    def quantile(self, x, quantiles):
        &#34;&#34;&#34;
        Reduces the last / inner axis of x.

        Args:
            x: Tensor
            quantiles: List or 1D tensor of quantiles to compute.

        Returns:
            Tensor with shape (quantiles, *x.shape[:-1])
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def argsort(self, x, axis=-1):
        raise NotImplementedError(self)

    def sort(self, x, axis=-1):
        raise NotImplementedError(self)

    def searchsorted(self, sorted_sequence, search_values, side: str, dtype=INT32):
        raise NotImplementedError(self)

    def fft(self, x, axes: Union[tuple, list]):
        &#34;&#34;&#34;
        Computes the n-dimensional FFT along all but the first and last dimensions.

        Args:
          x: tensor of dimension 3 or higher
          axes: Along which axes to perform the FFT

        Returns:
            Complex tensor `k`
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def ifft(self, k, axes: Union[tuple, list]):
        &#34;&#34;&#34;
        Computes the n-dimensional inverse FFT along all but the first and last dimensions.

        Args:
          k: tensor of dimension 3 or higher
          axes: Along which axes to perform the inverse FFT

        Returns:
            Complex tensor `x`
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def imag(self, x):
        raise NotImplementedError(self)

    def real(self, x):
        raise NotImplementedError(self)

    def conj(self, x):
        raise NotImplementedError(self)

    def sin(self, x):
        raise NotImplementedError(self)

    def arcsin(self, x):
        raise NotImplementedError(self)

    def cos(self, x):
        raise NotImplementedError(self)

    def arccos(self, x):
        raise NotImplementedError(self)

    def tan(self, x):
        raise NotImplementedError(self)

    def arctan(self, x):
        raise NotImplementedError(self)

    def arctan2(self, y, x):
        raise NotImplementedError(self)

    def sinh(self, x):
        raise NotImplementedError(self)

    def arcsinh(self, x):
        raise NotImplementedError(self)

    def cosh(self, x):
        raise NotImplementedError(self)

    def arccosh(self, x):
        raise NotImplementedError(self)

    def tanh(self, x):
        raise NotImplementedError(self)

    def arctanh(self, x):
        raise NotImplementedError(self)

    def log(self, x):
        &#34;&#34;&#34; Natural logarithm &#34;&#34;&#34;
        raise NotImplementedError(self)

    def log2(self, x):
        raise NotImplementedError(self)

    def log10(self, x):
        raise NotImplementedError(self)

    def sigmoid(self, x):
        return 1 / (1 + self.exp(-x))

    def dtype(self, array) -&gt; DType:
        raise NotImplementedError(self)

    def tile(self, value, multiples):
        &#34;&#34;&#34;
        Repeats the full tensor along each axis the number of times given by multiples.
        If `multiples` has more dimensions than `value`, these dimensions are added to `value` as outer dimensions.

        Args:
            value: tensor
            multiples: tuple or list of integers

        Returns:
            tiled tensor
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def repeat(self, x, repeats, axis: int, new_length=None):
        &#34;&#34;&#34;
        Repeats the elements along `axis` `repeats` times.

        Args:
            x: Tensor
            repeats: How often to repeat each element. 1D tensor of length x.shape[axis]
            axis: Which axis to repeat elements along
            new_length: Set the length of `axis` after repeating. This is required for jit compilation with Jax.

        Returns:
            repeated Tensor
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def get_diagonal(self, matrices, offset=0):
        &#34;&#34;&#34;

        Args:
            matrices: (batch, rows, cols, channels)
            offset: 0=diagonal, positive=above diagonal, negative=below diagonal

        Returns:
            diagonal: (batch, max(rows,cols), channels)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def indexed_segment_sum(self, x, indices, axis: int):
        &#34;&#34;&#34;
        Args:
            x: Values to sum. Segments are laid out contiguously along `axis`. (batch, ...)
            indices: should start with 0 along `axis`. (batch, indices)
            axis: Axis along which to sum

        Returns:
            Tensor with `len(indices)` elements along `axis`. (batch, ..., indices, ...)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def sparse_coo_tensor(self, indices: TensorType, values: TensorType, shape: tuple):
        &#34;&#34;&#34;
        Create a sparse matrix in coordinate list (COO) format.

        Optional feature.

        See Also:
            `Backend.csr_matrix()`, `Backend.csc_matrix()`.

        Args:
            indices: 2D tensor of shape `(nnz, dims)`.
            values: 1D values tensor matching `indices`
            shape: Shape of the sparse matrix

        Returns:
            Native representation of the sparse matrix
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def sparse_coo_tensor_batched(self, indices: Union[tuple, list], values, shape: tuple):
        &#34;&#34;&#34;
        Args:
            indices: shape (batch_size, dims, nnz)
            values: Values tensor matching `indices`, shape (batch_size, nnz)
            shape: tuple of two ints representing the dense shape, (dims...)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def mul_coo_dense(self, indices, values, shape, dense):
        &#34;&#34;&#34;
        Multiply a batch of sparse coordinate matrices by a batch of dense matrices.
        Every backend should implement this feature.
        This is the fallback if CSR multiplication is not supported.

        Args:
            indices: (batch, nnz, ndims)
            values: (batch, nnz, channels)
            shape: Shape of the full matrix, tuple of length ndims (sparse_rows, sparse_cols)
            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)

        Returns:
            (batch, dense_rows=sparse_cols, channels, dense_cols)
        &#34;&#34;&#34;
        values, dense = self.auto_cast(values, dense)
        batch_size, nnz, channel_count = self.staticshape(values)
        batch_size_d, dense_rows, channel_count_d, dense_cols = self.staticshape(dense)
        assert batch_size_d == batch_size
        assert dense_rows == shape[1]
        assert channel_count == channel_count_d
        dense_formatted = self.reshape(dense, (batch_size, dense_rows, channel_count * dense_cols))
        dense_gathered = self.batched_gather_nd(dense_formatted, indices[:, :, 1:2])
        base_grid = self.zeros((batch_size, shape[0], channel_count * dense_cols), self.dtype(dense))
        result = self.scatter(base_grid, indices[:, :, 0:1], values * dense_gathered, mode=&#39;add&#39;)
        return self.reshape(result, (batch_size, shape[0], channel_count, dense_cols))

    def coo_to_dense(self, indices, values, shape, contains_duplicates: bool):
        batch_size, nnz, channel_count = self.staticshape(values)
        base = self.zeros((batch_size, *shape, channel_count), dtype=self.dtype(values))
        result = self.scatter(base, indices, values, mode=&#39;add&#39; if contains_duplicates else &#39;update&#39;)
        return result

    def csr_matrix(self, column_indices: TensorOrArray, row_pointers: TensorOrArray, values: TensorOrArray, shape: Tuple[int, int]):
        &#34;&#34;&#34;
        Create a sparse matrix in compressed sparse row (CSR) format.

        Optional feature.

        See Also:
            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.

        Args:
            column_indices: Column indices corresponding to `values`, 1D tensor
            row_pointers: Indices in `values` where any row starts, 1D tensor of length `rows + 1`
            values: Non-zero values, 1D tensor
            shape: Shape of the full matrix

        Returns:
            Native representation of the sparse matrix
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def csr_matrix_batched(self, column_indices, row_pointers, values, shape: Tuple[int, int]):
        &#34;&#34;&#34;
        Args:
            column_indices: Column indices corresponding to `values`, shape (batch_size, nnz)
            row_pointers: Indices in `values` where any row starts, shape (batch_size, rows+1)
            values: Non-zero values, shape (batch_size, nnz, channels)
            shape: tuple of two ints representing the dense shape, (cols, rows)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def mul_csr_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int], dense):
        &#34;&#34;&#34;
        Multiply a batch of compressed sparse row matrices by a batch of dense matrices.

        Optional feature.

        See Also:
            `Backend.sparse_coo_tensor()`, `Backend.csc_matrix()`.

        Args:
            column_indices: (batch, nnz)
            row_pointers: (batch, rows + 1)
            values: (batch, nnz, channels)
            shape: Shape of the full matrix (cols, rows)
            dense: (batch, dense_rows=sparse_cols, channels, dense_cols)

        Returns:
            (batch, dense_rows=sparse_cols, channels, dense_cols)
        &#34;&#34;&#34;
        # if not self.supports(Backend.indexed_segment_sum):
        native_coo_indices = self.csr_to_coo(column_indices, row_pointers)
        return self.mul_coo_dense(native_coo_indices, values, shape, dense)
        # values, dense = self.auto_cast(values, dense)
        # batch_size, nnz, channel_count = self.staticshape(values)
        # _, dense_rows, _, dense_cols = self.staticshape(dense)
        # assert dense_cols == 1
        # dense_formatted = self.reshape(dense, (batch_size, dense_rows, channel_count * dense_cols))
        # dense_gathered = self.batched_gather_nd(dense_formatted, self.expand_dims(column_indices, -1))  # (batch, nnz, channels*rhs_cols)
        # dense_gathered = self.reshape(dense_gathered, (batch_size, nnz, channel_count, dense_cols))
        # values = self.reshape(values, (batch_size, nnz, channel_count, 1))
        # result = self.indexed_segment_sum(values * dense_gathered, row_pointers[:, :-1], 1)
        # return self.reshape(result, (batch_size, channel_count, rhs_rows, rhs_cols))

    def csr_to_coo(self, column_indices, row_pointers):
        &#34;&#34;&#34;
        Convert a batch of compressed sparse matrices to sparse coordinate matrices.

        Args:
            column_indices: (batch, nnz)
            row_pointers: (batch, rows + 1)

        Returns:
            indices: (batch, nnz, 2)
        &#34;&#34;&#34;
        batch_size, index_count = self.staticshape(column_indices)
        repeats = row_pointers[:, 1:] - row_pointers[:, :-1]
        row_count = self.shape(repeats)[-1]
        row_indices = [self.repeat(self.range(row_count, dtype=self.dtype(column_indices)), repeats[b], -1, new_length=index_count) for b in range(batch_size)]
        return self.stack([self.stack(row_indices), column_indices], axis=-1)

    def csr_to_dense(self, column_indices, row_pointers, values, shape: Tuple[int, int], contains_duplicates=False):
        indices = self.csr_to_coo(column_indices, row_pointers)
        return self.coo_to_dense(indices, values, shape, contains_duplicates=contains_duplicates)

    def csc_matrix(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
        &#34;&#34;&#34;
        Create a sparse matrix in compressed sparse column (CSC) format.

        Optional feature.

        See Also:
            `Backend.sparse_coo_tensor()`, `Backend.csr_matrix()`.

        Args:
            column_pointers: Indices in `values` where any column starts, 1D tensor of length `cols + 1`
            row_indices: Row indices corresponding to `values`.
            values: Non-zero values, 1D tensor
            shape: Shape of the full matrix

        Returns:
            Native representation of the sparse matrix
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def csc_matrix_batched(self, column_pointers, row_indices, values, shape: Tuple[int, int]):
        &#34;&#34;&#34;
        Args:
            column_pointers: Indices in `values` where any row starts, shape (batch_size, cols+1)
            row_indices: Row indices corresponding to `values`, shape (batch_size, nnz)
            values: Non-zero values, shape (batch_size, nnz, channels)
            shape: tuple of two ints representing the dense shape, (cols, rows)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def minimize(self, method: str, f, x0, atol, max_iter, trj: bool):
        if method == &#39;auto&#39;:
            method = &#39;L-BFGS-B&#39;
        if method == &#39;GD&#39;:
            from ._minimize import gradient_descent
            return gradient_descent(self, f, x0, atol, max_iter, trj)
        else:
            from ._minimize import scipy_minimize
            return scipy_minimize(self, method, f, x0, atol, max_iter, trj)

    def linear_solve(self,
                     method: str,
                     lin: Union[Callable, TensorType],
                     y: TensorType,
                     x0: TensorType,
                     rtol: Union[ndarray, TensorType],
                     atol: Union[ndarray, TensorType],
                     max_iter: ndarray,
                     pre: Optional[Preconditioner],
                     matrix_offset: Optional[TensorType]) -&gt; SolveResult:
        &#34;&#34;&#34;
        Solve the system of linear equations A · x = y.
        This method need not provide a gradient for the operation.

        Args:
            method: Which algorithm to use. One of:
                * &#39;auto&#39;
                * &#39;CG&#39;
                * &#39;CG-adaptive&#39;
                * &#39;biCG-stab&#39; or &#39;biCG-stab(1)&#39;
                * &#39;biCG-stab(n)&#39;
                * &#39;scipy-direct&#39;
                * &#39;scipy-CG&#39;, &#39;scipy-GMres&#39;, &#39;scipy-biCG&#39;, &#39;scipy-biCG-stab&#39;, &#39;scipy-CGS&#39;, &#39;scipy-QMR&#39;, &#39;scipy-GCrotMK&#39;
            lin: Linear operation. One of
                * sparse/dense matrix valid for all instances
                * tuple/list of sparse/dense matrices for varying matrices along batch, must have the same nonzero locations.
                * linear function A(x), must be called on all instances in parallel
            y: target result of A * x. 2nd order tensor (batch, vector) or list of vectors.
            x0: Initial guess of size (batch, parameters)
            rtol: Relative tolerance of size (batch,)
            atol: Absolute tolerance of size (batch,)
            max_iter: Maximum number of iterations of shape (checkpoints, batch).
            pre: Preconditioner, function taking one native tensor like `y` as input and returning a native tensor like `x0`.
            matrix_offset: Constant value to be added to every matrix entry, explicitly or implicitly. This can be used to stabilize solves for singular matrices.

        Returns:
            `SolveResult`
        &#34;&#34;&#34;
        if method == &#39;auto&#39;:
            return self.conjugate_gradient_adaptive(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)
        elif method.startswith(&#39;scipy-&#39;):
            from ._linalg import scipy_sparse_solve
            result = scipy_sparse_solve(self, method[len(&#39;scipy-&#39;):], lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)
            return SolveResult(result.method, self.as_tensor(result.x), self.as_tensor(result.residual), result.iterations, result.function_evaluations, result.converged, result.diverged, result.message)
        elif method == &#39;CG&#39;:
            return self.conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)
        elif method == &#39;CG-adaptive&#39;:
            return self.conjugate_gradient_adaptive(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)
        elif method in [&#39;biCG&#39;, &#39;biCG-stab(0)&#39;]:
            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset, poly_order=0)
        elif method == &#39;biCG-stab&#39;:
            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset, poly_order=1)
        elif method.startswith(&#39;biCG-stab(&#39;):
            order = int(method[len(&#39;biCG-stab(&#39;):-1])
            return self.bi_conjugate_gradient(lin, y, x0, rtol, atol, max_iter, pre, matrix_offset, poly_order=order)
        else:
            raise NotImplementedError(f&#34;Method &#39;{method}&#39; not supported for linear solve.&#34;)

    def conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset) -&gt; SolveResult:
        &#34;&#34;&#34; Standard conjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. &#34;&#34;&#34;
        from ._linalg import cg
        return cg(self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)

    def conjugate_gradient_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset) -&gt; SolveResult:
        &#34;&#34;&#34; Conjugate gradient algorithm with adaptive step size. Signature matches to `Backend.linear_solve()`. &#34;&#34;&#34;
        from ._linalg import cg_adaptive
        return cg_adaptive(self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset)

    def bi_conjugate_gradient(self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset, poly_order=2) -&gt; SolveResult:
        &#34;&#34;&#34; Generalized stabilized biconjugate gradient algorithm. Signature matches to `Backend.linear_solve()`. &#34;&#34;&#34;
        from ._linalg import bicg
        return bicg(self, lin, y, x0, rtol, atol, max_iter, pre, poly_order, matrix_offset)

    def linear(self, lin, vector):
        if callable(lin):
            return lin(vector)
        elif isinstance(lin, (tuple, list)):
            for lin_i in lin:
                lin_shape = self.staticshape(lin_i)
                assert len(lin_shape) == 2
            return self.stack([self.mul_matrix_batched_vector(m, v) for m, v in zip(lin, self.unstack(vector))])
        else:
            lin_shape = self.staticshape(lin)
            assert len(lin_shape) == 2, f&#34;A must be a matrix but got shape {lin_shape}&#34;
            return self.mul_matrix_batched_vector(lin, vector)

    def matrix_solve_least_squares(self, matrix: TensorType, rhs: TensorType) -&gt; Tuple[TensorType, TensorType, TensorType, TensorType]:
        &#34;&#34;&#34;
        Args:
            matrix: Shape (batch, vec, constraints)
            rhs: Shape (batch, vec, batch_per_matrix)

        Returns:
            solution: Solution vector of Shape (batch, constraints, batch_per_matrix)
            residuals: Optional, can be `None`
            rank: Optional, can be `None`
            singular_values: Optional, can be `None`
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def solve_triangular(self, matrix, rhs, lower: bool, unit_diagonal: bool):
        &#34;&#34;&#34;Performs a sparse or dense triangular solve, depending on the format of `matrix`.&#34;&#34;&#34;
        if self.is_sparse(matrix):
            return self.solve_triangular_sparse(matrix, rhs, lower, unit_diagonal)
        else:
            return self.solve_triangular_dense(matrix, rhs, lower, unit_diagonal)

    def solve_triangular_dense(self, matrix, rhs, lower: bool, unit_diagonal: bool):
        &#34;&#34;&#34;
        Args:
            matrix: (batch_size, rows, cols)
            rhs: (batch_size, cols)
            lower:
            unit_diagonal:

        Returns:
            (batch_size, cols)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def solve_triangular_sparse(self, matrix, rhs, lower: bool, unit_diagonal: bool):
        raise NotImplementedError(f&#34;sparse triangular solves are not supported by {self.name}. Try using a SciPy solver instead, such as &#39;scipy-CG&#39; or &#39;scipy-biCG-stab&#39;.&#34;)
        np_matrix = self.numpy(matrix)
        np_rhs = self.numpy(rhs)
        from scipy.sparse.linalg import spsolve_triangular
        np_result = spsolve_triangular(np_matrix, np_rhs.T, lower=lower, unit_diagonal=unit_diagonal).T
        return self.as_tensor(np_result)

    def matrix_rank_dense(self, matrix, hermitian=False) -&gt; TensorType:
        &#34;&#34;&#34;
        Args:
            matrix: Dense matrix of shape (batch, rows, cols)
            hermitian: Whether all matrices are guaranteed to be hermitian.
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def eigvals(self, matrix: TensorType) -&gt; TensorType:
        &#34;&#34;&#34;
        Args:
            matrix: (batch..., n, n)

        Returns:
            eigenvalues as (batch..., n,)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def eig(self, matrix: TensorType) -&gt; TensorType:
        &#34;&#34;&#34;
        Args:
            matrix: (batch..., n, n)

        Returns:
            eigenvalues: (batch..., n,)
            eigenvectors: (batch..., n, n)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def svd(self, matrix: TensorType, full_matrices=True) -&gt; Tuple[TensorType, TensorType, TensorType]:
        &#34;&#34;&#34;
        Args:
            matrix: (batch..., m, n)

        Returns:
            eigenvalues: (batch..., n,)
            eigenvectors: (batch..., n, n)
        &#34;&#34;&#34;
        raise NotImplementedError(self)

    def stop_gradient(self, value):
        raise NotImplementedError(self)

    def stop_gradient_tree(self, tree):
        if isinstance(tree, tuple):
            return tuple([self.stop_gradient_tree(v) for v in tree])
        if isinstance(tree, list):
            return [self.stop_gradient_tree(v) for v in tree]
        if isinstance(tree, dict):
            return {k: self.stop_gradient_tree(v) for k, v in tree.items()}
        return self.stop_gradient(tree)

    def grid_sample(self, grid, coordinates, extrapolation: str):
        &#34;&#34;&#34;
        Interpolates a regular grid at the specified coordinates.

        Args:
            grid: Tensor of shape (batch, spatial..., channel)
            coordinates: Tensor of floating grid indices of shape (batch, instance..., vector).
                The last dimension must match `spatial_dims`.
                The first grid point of dimension i lies at position 0, the last at values.shape[i]-1.
            extrapolation: Values to use for coordinates outside the grid.
                One of `(&#39;undefined&#39;, &#39;zeros&#39;, &#39;boundary&#39;, &#39;periodic&#39;, &#39;symmetric&#39;, &#39;reflect&#39;)`.

        Returns:
            sampled values with linear interpolation
        &#34;&#34;&#34;
        return NotImplemented

    def ndims(self, tensor):
        return len(self.staticshape(tensor))

    def size(self, array):
        return self.prod(self.shape(array))

    def multi_slice(self, tensor, slices: tuple):
        &#34;&#34;&#34;
        Args:
            tensor: value to slice
            slices: `tuple` of `slice`, `int`, or scalar integer tensors
        &#34;&#34;&#34;
        return tensor[slices]

    def batch_gather(self, tensor, batches):
        if isinstance(batches, int):
            batches = [batches]
        return tensor[batches, ...]

    def unstack(self, tensor, axis=0, keepdims=False) -&gt; tuple:
        if axis &lt; 0:
            axis += len(tensor.shape)
        if axis &gt;= len(tensor.shape) or axis &lt; 0:
            raise ValueError(&#34;Illegal axis value&#34;)
        result = []
        for slice_idx in range(tensor.shape[axis]):
            if keepdims:
                component = tensor[tuple([slice(slice_idx, slice_idx + 1) if d == axis else slice(None) for d in range(len(tensor.shape))])]
            else:
                component = tensor[tuple([slice_idx if d == axis else slice(None) for d in range(len(tensor.shape))])]
            result.append(component)
        return tuple(result)

    def equal(self, x, y):
        &#34;&#34;&#34; Element-wise equality check &#34;&#34;&#34;
        raise NotImplementedError(self)

    def not_equal(self, x, y):
        return ~self.equal(x, y)

    def greater_than(self, x, y):
        x, y = self.auto_cast(x, y)
        return x &gt; y

    def greater_or_equal(self, x, y):
        x, y = self.auto_cast(x, y)
        return x &gt;= y

    def add(self, a, b):
        a, b = self.auto_cast(a, b, bool_to_int=True)
        return a + b

    def sub(self, a, b):
        a, b = self.auto_cast(a, b, bool_to_int=True)
        return a - b

    def mul(self, a, b):
        a, b = self.auto_cast(a, b)
        return a * b

    def div(self, numerator, denominator):
        numerator, denominator = self.auto_cast(numerator, denominator)
        return numerator / denominator

    def pow(self, base, exp):
        base, exp = self.auto_cast(base, exp)
        return base ** exp

    def mod(self, dividend, divisor):
        dividend, divisor = self.auto_cast(dividend, divisor)
        return dividend % divisor

    def and_(self, a, b):
        a, b = self.auto_cast(a, b)
        return a &amp; b

    def or_(self, a, b):
        a, b = self.auto_cast(a, b)
        return a | b

    def xor(self, a, b):
        a, b = self.auto_cast(a, b)
        return a ^ b

    def floordiv(self, a, b):
        a, b = self.auto_cast(a, b)
        return a // b

    def shift_bits_left(self, a, b):
        a, b = self.auto_cast(a, b)
        return a &lt;&lt; b

    def shift_bits_right(self, a, b):
        a, b = self.auto_cast(a, b)
        return a &gt;&gt; b

    def invert(self, x):
        if isinstance(x, bool):
            return not x
        return ~x</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phiml.backend._numpy_backend.NumPyBackend</li>
<li>phiml.backend._object.ObjectBackend</li>
<li>phiml.backend.jax._jax_backend.JaxBackend</li>
<li>phiml.backend.tensorflow._tf_backend.TFBackend</li>
<li>phiml.backend.torch._torch_backend.TorchBackend</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.backend.Backend.as_registered"><code class="name">prop <span class="ident">as_registered</span> : <a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def as_registered(self) -&gt; &#39;Backend&#39;:
    from . import BACKENDS
    for backend in BACKENDS:
        if self.name in backend.name:
            return backend
    if not init_backend(self.name):
        raise RuntimeError(f&#34;Backend &#39;{self}&#39; is not registered. Registered backends are: {BACKENDS}&#34;)
    return self.as_registered</code></pre>
</details>
</dd>
<dt id="phiml.backend.Backend.complex_type"><code class="name">prop <span class="ident">complex_type</span> : phiml.backend._dtype.DType</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def complex_type(self) -&gt; DType:
    return DType.by_precision(complex, max(64, self.precision))</code></pre>
</details>
</dd>
<dt id="phiml.backend.Backend.float_type"><code class="name">prop <span class="ident">float_type</span> : phiml.backend._dtype.DType</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def float_type(self) -&gt; DType:
    return DType.by_precision(float, self.precision)</code></pre>
</details>
</dd>
<dt id="phiml.backend.Backend.name"><code class="name">prop <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    return self._name</code></pre>
</details>
</dd>
<dt id="phiml.backend.Backend.precision"><code class="name">prop <span class="ident">precision</span> : int</code></dt>
<dd>
<div class="desc"><p>Short for math.backend.get_precision()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self) -&gt; int:
    &#34;&#34;&#34; Short for math.backend.get_precision() &#34;&#34;&#34;
    return get_precision()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.Backend.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>self, boolean_tensor, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.allocate_on_device"><code class="name flex">
<span>def <span class="ident">allocate_on_device</span></span>(<span>self, tensor: ~TensorType, device: phiml.backend._backend.ComputeDevice) ‑> ~TensorType</span>
</code></dt>
<dd>
<div class="desc"><p>Moves <code>tensor</code> to <code>device</code>. May copy the tensor if it is already on the device.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Existing tensor native to this backend.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Target device, associated with this backend.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.and_"><code class="name flex">
<span>def <span class="ident">and_</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>self, boolean_tensor, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arccos"><code class="name flex">
<span>def <span class="ident">arccos</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arccosh"><code class="name flex">
<span>def <span class="ident">arccosh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arcsin"><code class="name flex">
<span>def <span class="ident">arcsin</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arcsinh"><code class="name flex">
<span>def <span class="ident">arcsinh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arctan"><code class="name flex">
<span>def <span class="ident">arctan</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arctan2"><code class="name flex">
<span>def <span class="ident">arctan2</span></span>(<span>self, y, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.arctanh"><code class="name flex">
<span>def <span class="ident">arctanh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.argmax"><code class="name flex">
<span>def <span class="ident">argmax</span></span>(<span>self, x, axis: int, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.argmin"><code class="name flex">
<span>def <span class="ident">argmin</span></span>(<span>self, x, axis: int, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.argsort"><code class="name flex">
<span>def <span class="ident">argsort</span></span>(<span>self, x, axis=-1)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.as_tensor"><code class="name flex">
<span>def <span class="ident">as_tensor</span></span>(<span>self, x, convert_external=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a tensor-like object to the native tensor representation of this backend.
If x is a native tensor of this backend, it is returned without modification.
If x is a Python number (numbers.Number instance), <code>convert_numbers</code> decides whether to convert it unless the backend cannot handle Python numbers.</p>
<p><em>Note:</em> There may be objects that are considered tensors by this backend but are not native and thus, will be converted by this method.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor-like, e.g. list, tuple, Python number, tensor</dd>
<dt><strong><code>convert_external</code></strong></dt>
<dd>if False and <code>x</code> is a Python number that is understood by this backend, this method returns the number as-is. This can help prevent type clashes like int32 vs int64. (Default value = True)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor representation of <code>x</code></p></div>
</dd>
<dt id="phiml.backend.Backend.auto_cast"><code class="name flex">
<span>def <span class="ident">auto_cast</span></span>(<span>self, *tensors, bool_to_int=False, int_to_float=False) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the appropriate values type resulting from operations involving the tensors as input.</p>
<p>This method is called by the default implementations of basic operators.
Backends can override this method to prevent unnecessary casting.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*tensors</code></strong></dt>
<dd>tensors to cast and to consider when determining the common data type</dd>
<dt><strong><code>bool_to_int</code></strong></dt>
<dd>Whether to convert boolean values to integers if all values are boolean.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensors cast to a common data type</p></div>
</dd>
<dt id="phiml.backend.Backend.auto_cast1"><code class="name flex">
<span>def <span class="ident">auto_cast1</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.batch_gather"><code class="name flex">
<span>def <span class="ident">batch_gather</span></span>(<span>self, tensor, batches)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.batched_bincount"><code class="name flex">
<span>def <span class="ident">batched_bincount</span></span>(<span>self, x, weights: Optional[~TensorType], bins: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.batched_gather_1d"><code class="name flex">
<span>def <span class="ident">batched_gather_1d</span></span>(<span>self, values, indices)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>(batch, spatial)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>(batch, indices)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(batch, indices)</p></div>
</dd>
<dt id="phiml.backend.Backend.batched_gather_nd"><code class="name flex">
<span>def <span class="ident">batched_gather_nd</span></span>(<span>self, values, indices)</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers values from the tensor <code>values</code> at locations <code>indices</code>.
The first dimension of <code>values</code> and <code>indices</code> is the batch dimension which must be either equal for both or one for either.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensor of shape (batch, spatial&hellip;, channel)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>int tensor of shape (batch, any&hellip;, multi_index) where the size of multi_index is values.rank - 2.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Gathered values as tensor of shape (batch, any&hellip;, channel)</p></div>
</dd>
<dt id="phiml.backend.Backend.bi_conjugate_gradient"><code class="name flex">
<span>def <span class="ident">bi_conjugate_gradient</span></span>(<span>self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset, poly_order=2) ‑> phiml.backend._backend.SolveResult</span>
</code></dt>
<dd>
<div class="desc"><p>Generalized stabilized biconjugate gradient algorithm. Signature matches to <code><a title="phiml.backend.Backend.linear_solve" href="#phiml.backend.Backend.linear_solve">Backend.linear_solve()</a></code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.bincount"><code class="name flex">
<span>def <span class="ident">bincount</span></span>(<span>self, x, weights: Optional[~TensorType], bins: int, x_sorted=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Bin indices, 1D int tensor.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>Weights corresponding to <code>x</code>, 1D tensor. All weights are 1 if <code>weights=None</code>.</dd>
<dt><strong><code>bins</code></strong></dt>
<dd>Number of bins.</dd>
<dt><strong><code>x_sorted</code></strong></dt>
<dd>Whether <code>x</code> is sorted from lowest to highest bin.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>bin_counts</p></div>
</dd>
<dt id="phiml.backend.Backend.block_until_ready"><code class="name flex">
<span>def <span class="ident">block_until_ready</span></span>(<span>self, values)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>self, x, mask, axis=0, new_length=None, fill_value=0)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor with any number of dimensions</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>1D mask tensor</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis index &gt;= 0</dd>
<dt><strong><code>new_length</code></strong></dt>
<dd>Maximum size of the output along <code>axis</code>. This must be set when jit-compiling with Jax.</dd>
<dt><strong><code>fill_value</code></strong></dt>
<dd>If <code>new_length</code> is larger than the filtered result, the remaining values will be set to <code>fill_value</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, f: Callable, *args, name=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>f(*args)</code> and returns the result.
This method may be used to register internal calls with the profiler.</p>
<h2 id="usage">Usage</h2>
<p>choose_backend(key).call(custom_function, *args)</p></div>
</dd>
<dt id="phiml.backend.Backend.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>self, x, dtype: phiml.backend._dtype.DType)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>self, x, minimum, maximum)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.combine_types"><code class="name flex">
<span>def <span class="ident">combine_types</span></span>(<span>self, *dtypes: phiml.backend._dtype.DType) ‑> phiml.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>self, values, axis)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.conj"><code class="name flex">
<span>def <span class="ident">conj</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.conjugate_gradient"><code class="name flex">
<span>def <span class="ident">conjugate_gradient</span></span>(<span>self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset) ‑> phiml.backend._backend.SolveResult</span>
</code></dt>
<dd>
<div class="desc"><p>Standard conjugate gradient algorithm. Signature matches to <code><a title="phiml.backend.Backend.linear_solve" href="#phiml.backend.Backend.linear_solve">Backend.linear_solve()</a></code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.conjugate_gradient_adaptive"><code class="name flex">
<span>def <span class="ident">conjugate_gradient_adaptive</span></span>(<span>self, lin, y, x0, rtol, atol, max_iter, pre, matrix_offset) ‑> phiml.backend._backend.SolveResult</span>
</code></dt>
<dd>
<div class="desc"><p>Conjugate gradient algorithm with adaptive step size. Signature matches to <code><a title="phiml.backend.Backend.linear_solve" href="#phiml.backend.Backend.linear_solve">Backend.linear_solve()</a></code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>self, value, kernel, strides: Sequence[int], out_sizes: Sequence[int], transpose: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Convolve value with kernel.
Depending on the tensor rank, the convolution is either 1D (rank=3), 2D (rank=4) or 3D (rank=5).
Higher dimensions may not be supported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor of shape (batch_size, in_channel, spatial&hellip;)</dd>
<dt><strong><code>kernel</code></strong></dt>
<dd>tensor of shape (batch_size or 1, out_channel, in_channel, spatial&hellip;)</dd>
<dt><strong><code>strides</code></strong></dt>
<dd>Convolution strides, one <code>int</code> for each spatial dim. For transpose, they act as upsampling factors.</dd>
<dt><strong><code>out_sizes</code></strong></dt>
<dd>Spatial shape of the output tensor. This determines how much zero-padding or slicing is used.</dd>
<dt><strong><code>transpose</code></strong></dt>
<dd>If <code>True</code>, performs a transposed convolution, according to PyTorch's definition.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Convolution result as tensor of shape (batch_size, out_channel, spatial&hellip;)</p></div>
</dd>
<dt id="phiml.backend.Backend.coo_to_dense"><code class="name flex">
<span>def <span class="ident">coo_to_dense</span></span>(<span>self, indices, values, shape, contains_duplicates: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, tensor, only_mutable=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.copy_leaves"><code class="name flex">
<span>def <span class="ident">copy_leaves</span></span>(<span>self, tree, only_mutable=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.cosh"><code class="name flex">
<span>def <span class="ident">cosh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.csc_matrix"><code class="name flex">
<span>def <span class="ident">csc_matrix</span></span>(<span>self, column_pointers, row_indices, values, shape: Tuple[int, int])</span>
</code></dt>
<dd>
<div class="desc"><p>Create a sparse matrix in compressed sparse column (CSC) format.</p>
<p>Optional feature.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.sparse_coo_tensor" href="#phiml.backend.Backend.sparse_coo_tensor">Backend.sparse_coo_tensor()</a></code>, <code><a title="phiml.backend.Backend.csr_matrix" href="#phiml.backend.Backend.csr_matrix">Backend.csr_matrix()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_pointers</code></strong></dt>
<dd>Indices in <code>values</code> where any column starts, 1D tensor of length <code>cols + 1</code></dd>
<dt><strong><code>row_indices</code></strong></dt>
<dd>Row indices corresponding to <code>values</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Non-zero values, 1D tensor</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the full matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native representation of the sparse matrix</p></div>
</dd>
<dt id="phiml.backend.Backend.csc_matrix_batched"><code class="name flex">
<span>def <span class="ident">csc_matrix_batched</span></span>(<span>self, column_pointers, row_indices, values, shape: Tuple[int, int])</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_pointers</code></strong></dt>
<dd>Indices in <code>values</code> where any row starts, shape (batch_size, cols+1)</dd>
<dt><strong><code>row_indices</code></strong></dt>
<dd>Row indices corresponding to <code>values</code>, shape (batch_size, nnz)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Non-zero values, shape (batch_size, nnz, channels)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tuple of two ints representing the dense shape, (cols, rows)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.csr_matrix"><code class="name flex">
<span>def <span class="ident">csr_matrix</span></span>(<span>self, column_indices: Union[~TensorType, numpy.ndarray], row_pointers: Union[~TensorType, numpy.ndarray], values: Union[~TensorType, numpy.ndarray], shape: Tuple[int, int])</span>
</code></dt>
<dd>
<div class="desc"><p>Create a sparse matrix in compressed sparse row (CSR) format.</p>
<p>Optional feature.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.sparse_coo_tensor" href="#phiml.backend.Backend.sparse_coo_tensor">Backend.sparse_coo_tensor()</a></code>, <code><a title="phiml.backend.Backend.csc_matrix" href="#phiml.backend.Backend.csc_matrix">Backend.csc_matrix()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_indices</code></strong></dt>
<dd>Column indices corresponding to <code>values</code>, 1D tensor</dd>
<dt><strong><code>row_pointers</code></strong></dt>
<dd>Indices in <code>values</code> where any row starts, 1D tensor of length <code>rows + 1</code></dd>
<dt><strong><code>values</code></strong></dt>
<dd>Non-zero values, 1D tensor</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the full matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native representation of the sparse matrix</p></div>
</dd>
<dt id="phiml.backend.Backend.csr_matrix_batched"><code class="name flex">
<span>def <span class="ident">csr_matrix_batched</span></span>(<span>self, column_indices, row_pointers, values, shape: Tuple[int, int])</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_indices</code></strong></dt>
<dd>Column indices corresponding to <code>values</code>, shape (batch_size, nnz)</dd>
<dt><strong><code>row_pointers</code></strong></dt>
<dd>Indices in <code>values</code> where any row starts, shape (batch_size, rows+1)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Non-zero values, shape (batch_size, nnz, channels)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tuple of two ints representing the dense shape, (cols, rows)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.csr_to_coo"><code class="name flex">
<span>def <span class="ident">csr_to_coo</span></span>(<span>self, column_indices, row_pointers)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a batch of compressed sparse matrices to sparse coordinate matrices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_indices</code></strong></dt>
<dd>(batch, nnz)</dd>
<dt><strong><code>row_pointers</code></strong></dt>
<dd>(batch, rows + 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>indices</code></dt>
<dd>(batch, nnz, 2)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.csr_to_dense"><code class="name flex">
<span>def <span class="ident">csr_to_dense</span></span>(<span>self, column_indices, row_pointers, values, shape: Tuple[int, int], contains_duplicates=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.cumsum"><code class="name flex">
<span>def <span class="ident">cumsum</span></span>(<span>self, x, axis: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.custom_gradient"><code class="name flex">
<span>def <span class="ident">custom_gradient</span></span>(<span>self, f: Callable, gradient: Callable, get_external_cache: Callable = None, on_call_skipped: Callable = None) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function based on <code>f</code> that uses a custom gradient for backprop.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Forward function.</dd>
<dt><strong><code>gradient</code></strong></dt>
<dd>Function for backprop. Will be called as <code>gradient(*d_out)</code> to compute the gradient of <code>f</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>. However, the returned function does not support keyword arguments.</p></div>
</dd>
<dt id="phiml.backend.Backend.determine_size"><code class="name flex">
<span>def <span class="ident">determine_size</span></span>(<span>self, tensors, axis)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.disassemble"><code class="name flex">
<span>def <span class="ident">disassemble</span></span>(<span>self, x) ‑> Tuple[Callable, Sequence[~TensorType]]</span>
</code></dt>
<dd>
<div class="desc"><p>Disassemble a (sparse) tensor into its individual constituents, such as values and indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>assemble</code></dt>
<dd>Function <code>assemble(backend, *constituents)</code> that reassembles <code>x</code> from the constituents.</dd>
<dt><code>constituents</code></dt>
<dd>Tensors contained in <code>x</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.div"><code class="name flex">
<span>def <span class="ident">div</span></span>(<span>self, numerator, denominator)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes x/y but returns 0 if y=0.</p></div>
</dd>
<dt id="phiml.backend.Backend.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>self, array) ‑> phiml.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.eig"><code class="name flex">
<span>def <span class="ident">eig</span></span>(<span>self, matrix: ~TensorType) ‑> ~TensorType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>(batch&hellip;, n, n)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>eigenvalues</code></dt>
<dd>(batch&hellip;, n,)</dd>
<dt><code>eigenvectors</code></dt>
<dd>(batch&hellip;, n, n)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.eigvals"><code class="name flex">
<span>def <span class="ident">eigvals</span></span>(<span>self, matrix: ~TensorType) ‑> ~TensorType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>(batch&hellip;, n, n)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>eigenvalues as (batch&hellip;, n,)</p></div>
</dd>
<dt id="phiml.backend.Backend.einsum"><code class="name flex">
<span>def <span class="ident">einsum</span></span>(<span>self, equation, *tensors)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.equal"><code class="name flex">
<span>def <span class="ident">equal</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Element-wise equality check</p></div>
</dd>
<dt id="phiml.backend.Backend.erf"><code class="name flex">
<span>def <span class="ident">erf</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.expand_dims"><code class="name flex">
<span>def <span class="ident">expand_dims</span></span>(<span>self, a, axis=0, number=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.factorial"><code class="name flex">
<span>def <span class="ident">factorial</span></span>(<span>self, x: ~TensorType) ‑> ~TensorType</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>self, x, axes: Union[tuple, list])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the n-dimensional FFT along all but the first and last dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of dimension 3 or higher</dd>
<dt><strong><code>axes</code></strong></dt>
<dd>Along which axes to perform the FFT</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex tensor <code>k</code></p></div>
</dd>
<dt id="phiml.backend.Backend.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self, value, axes: Union[tuple, list])</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.floordiv"><code class="name flex">
<span>def <span class="ident">floordiv</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.from_dlpack"><code class="name flex">
<span>def <span class="ident">from_dlpack</span></span>(<span>self, capsule)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.gamma_inc_l"><code class="name flex">
<span>def <span class="ident">gamma_inc_l</span></span>(<span>self, a, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Regularized lower incomplete gamma function.</p></div>
</dd>
<dt id="phiml.backend.Backend.gamma_inc_u"><code class="name flex">
<span>def <span class="ident">gamma_inc_u</span></span>(<span>self, a, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Regularized upper incomplete gamma function.</p></div>
</dd>
<dt id="phiml.backend.Backend.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>self, values, indices, axis: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers values from the tensor <code>values</code> at locations <code>indices</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensor</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>1D tensor</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis along which to gather slices</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor, with size along <code>axis</code> being the length of <code>indices</code></p></div>
</dd>
<dt id="phiml.backend.Backend.gather_1d"><code class="name flex">
<span>def <span class="ident">gather_1d</span></span>(<span>self, values, indices)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.gather_by_component_indices"><code class="name flex">
<span>def <span class="ident">gather_by_component_indices</span></span>(<span>self, values, *component_indices)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.gather_nd"><code class="name flex">
<span>def <span class="ident">gather_nd</span></span>(<span>self, values, indices)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>(spatial, channels)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>(indices, multi_index)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(indices, channels)</p></div>
</dd>
<dt id="phiml.backend.Backend.get_default_device"><code class="name flex">
<span>def <span class="ident">get_default_device</span></span>(<span>self) ‑> phiml.backend._backend.ComputeDevice</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.get_device"><code class="name flex">
<span>def <span class="ident">get_device</span></span>(<span>self, tensor: ~TensorType) ‑> phiml.backend._backend.ComputeDevice</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the device <code>tensor</code> is located on.</p></div>
</dd>
<dt id="phiml.backend.Backend.get_device_by_ref"><code class="name flex">
<span>def <span class="ident">get_device_by_ref</span></span>(<span>self, ref)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.get_diagonal"><code class="name flex">
<span>def <span class="ident">get_diagonal</span></span>(<span>self, matrices, offset=0)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrices</code></strong></dt>
<dd>(batch, rows, cols, channels)</dd>
<dt><strong><code>offset</code></strong></dt>
<dd>0=diagonal, positive=above diagonal, negative=below diagonal</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>diagonal</code></dt>
<dd>(batch, max(rows,cols), channels)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.get_peak_memory"><code class="name flex">
<span>def <span class="ident">get_peak_memory</span></span>(<span>self, device: phiml.backend._backend.ComputeDevice)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.get_sparse_format"><code class="name flex">
<span>def <span class="ident">get_sparse_format</span></span>(<span>self, x) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Returns lower-case format string, such as 'coo', 'csr', 'csc'</p></div>
</dd>
<dt id="phiml.backend.Backend.greater_or_equal"><code class="name flex">
<span>def <span class="ident">greater_or_equal</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.greater_than"><code class="name flex">
<span>def <span class="ident">greater_than</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>self, grid, coordinates, extrapolation: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Interpolates a regular grid at the specified coordinates.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor of shape (batch, spatial&hellip;, channel)</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Tensor of floating grid indices of shape (batch, instance&hellip;, vector).
The last dimension must match <code>spatial_dims</code>.
The first grid point of dimension i lies at position 0, the last at values.shape[i]-1.</dd>
<dt><strong><code>extrapolation</code></strong></dt>
<dd>Values to use for coordinates outside the grid.
One of <code>('undefined', 'zeros', 'boundary', 'periodic', 'symmetric', 'reflect')</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>sampled values with linear interpolation</p></div>
</dd>
<dt id="phiml.backend.Backend.hessian"><code class="name flex">
<span>def <span class="ident">hessian</span></span>(<span>self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>First dimension of all inputs/outputs of <code>f</code> is assumed to be a batch dimension.
Element-wise Hessians will be computed along the batch dimension.
All other dimensions are parameter dimensions and will appear twice in the Hessian matrices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose first output is a scalar float or complex value.</dd>
</dl>
<p>wrt:
get_output:
get_gradient:</p>
<h2 id="returns">Returns</h2>
<p>Function returning <code>(f(x), g(x), H(x))</code> or less depending on <code>get_output</code> and <code>get_gradient</code>.
The result is always a <code>tuple</code> holding at most these three items.</p></div>
</dd>
<dt id="phiml.backend.Backend.histogram1d"><code class="name flex">
<span>def <span class="ident">histogram1d</span></span>(<span>self, values, weights, bin_edges)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>(batch, values)</dd>
<dt><strong><code>bin_edges</code></strong></dt>
<dd>(batch, edges)</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>(batch, values)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(batch, edges) with dtype matching weights</p></div>
</dd>
<dt id="phiml.backend.Backend.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>self, k, axes: Union[tuple, list])</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the n-dimensional inverse FFT along all but the first and last dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>tensor of dimension 3 or higher</dd>
<dt><strong><code>axes</code></strong></dt>
<dd>Along which axes to perform the inverse FFT</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex tensor <code>x</code></p></div>
</dd>
<dt id="phiml.backend.Backend.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.indexed_segment_sum"><code class="name flex">
<span>def <span class="ident">indexed_segment_sum</span></span>(<span>self, x, indices, axis: int)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Values to sum. Segments are laid out contiguously along <code>axis</code>. (batch, &hellip;)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>should start with 0 along <code>axis</code>. (batch, indices)</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis along which to sum</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor with <code>len(indices)</code> elements along <code>axis</code>. (batch, &hellip;, indices, &hellip;)</p></div>
</dd>
<dt id="phiml.backend.Backend.invert"><code class="name flex">
<span>def <span class="ident">invert</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.is_available"><code class="name flex">
<span>def <span class="ident">is_available</span></span>(<span>self, tensor) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the value of the tensor is known and can be read at this point.
If true, <code>numpy(tensor)</code> must return a valid NumPy representation of the value.</p>
<p>Tensors are typically available when the backend operates in eager mode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>backend-compatible tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>bool</p></div>
</dd>
<dt id="phiml.backend.Backend.is_module"><code class="name flex">
<span>def <span class="ident">is_module</span></span>(<span>self, obj) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if <code>obj</code> is of a type that is specific to this backend, e.g. a neural network.
If <code>True</code>, this backend will be chosen for operations involving <code>obj</code>.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.is_tensor" href="#phiml.backend.Backend.is_tensor">Backend.is_tensor()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>Object to test.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.is_sparse"><code class="name flex">
<span>def <span class="ident">is_sparse</span></span>(<span>self, x) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor native to this <code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code>.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.is_tensor"><code class="name flex">
<span>def <span class="ident">is_tensor</span></span>(<span>self, x, only_native=False)</span>
</code></dt>
<dd>
<div class="desc"><p>An object is considered a native tensor by a backend if no internal conversion is required by backend methods.
An object is considered a tensor (nativer or otherwise) by a backend if it is not a struct (e.g. tuple, list) and all methods of the backend accept it as a tensor argument.</p>
<p>If <code>True</code>, this backend will be chosen for operations involving <code>x</code>.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.is_module" href="#phiml.backend.Backend.is_module">Backend.is_module()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>object to check</dd>
<dt><strong><code>only_native</code></strong></dt>
<dd>If True, only accepts true native tensor representations, not Python numbers or others that are also supported as tensors (Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>whether <code>x</code> is considered a tensor by this backend</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.isinf"><code class="name flex">
<span>def <span class="ident">isinf</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.isnan"><code class="name flex">
<span>def <span class="ident">isnan</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.jacobian"><code class="name flex">
<span>def <span class="ident">jacobian</span></span>(<span>self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to differentiate. Returns a tuple containing <code>(reduced_loss, output)</code></dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Argument indices for which to compute the gradient.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the derivative function should return the output of <code>f</code> in addition to the gradient.</dd>
<dt><strong><code>is_f_scalar</code></strong></dt>
<dd>Whether <code>f</code> is guaranteed to return a scalar output.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A function <code>g</code> with the same arguments as <code>f</code>.
If <code>get_output=True</code>, <code>g</code> returns a <code>tuple</code>containing the outputs of <code>f</code> followed by the gradients.
The gradients retain the dimensions of <code>reduced_loss</code> in order as outer (first) dimensions.</p></div>
</dd>
<dt id="phiml.backend.Backend.jit_compile"><code class="name flex">
<span>def <span class="ident">jit_compile</span></span>(<span>self, f: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.jit_compile_grad"><code class="name flex">
<span>def <span class="ident">jit_compile_grad</span></span>(<span>self, f: Callable, wrt: Union[tuple, list], get_output: bool, is_f_scalar: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.jit_compile_hessian"><code class="name flex">
<span>def <span class="ident">jit_compile_hessian</span></span>(<span>self, f: Callable, wrt: Union[tuple, list], get_output: bool, get_gradient: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.linear"><code class="name flex">
<span>def <span class="ident">linear</span></span>(<span>self, lin, vector)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.linear_solve"><code class="name flex">
<span>def <span class="ident">linear_solve</span></span>(<span>self, method: str, lin: Union[Callable, ~TensorType], y: ~TensorType, x0: ~TensorType, rtol: Union[~TensorType, numpy.ndarray], atol: Union[~TensorType, numpy.ndarray], max_iter: numpy.ndarray, pre: Optional[phiml.backend._backend.Preconditioner], matrix_offset: Optional[~TensorType]) ‑> phiml.backend._backend.SolveResult</span>
</code></dt>
<dd>
<div class="desc"><p>Solve the system of linear equations A · x = y.
This method need not provide a gradient for the operation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>method</code></strong></dt>
<dd>Which algorithm to use. One of:
* 'auto'
* 'CG'
* 'CG-adaptive'
* 'biCG-stab' or 'biCG-stab(1)'
* 'biCG-stab(n)'
* 'scipy-direct'
* 'scipy-CG', 'scipy-GMres', 'scipy-biCG', 'scipy-biCG-stab', 'scipy-CGS', 'scipy-QMR', 'scipy-GCrotMK'</dd>
<dt><strong><code>lin</code></strong></dt>
<dd>Linear operation. One of
* sparse/dense matrix valid for all instances
* tuple/list of sparse/dense matrices for varying matrices along batch, must have the same nonzero locations.
* linear function A(x), must be called on all instances in parallel</dd>
<dt><strong><code>y</code></strong></dt>
<dd>target result of A * x. 2nd order tensor (batch, vector) or list of vectors.</dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial guess of size (batch, parameters)</dd>
<dt><strong><code>rtol</code></strong></dt>
<dd>Relative tolerance of size (batch,)</dd>
<dt><strong><code>atol</code></strong></dt>
<dd>Absolute tolerance of size (batch,)</dd>
<dt><strong><code>max_iter</code></strong></dt>
<dd>Maximum number of iterations of shape (checkpoints, batch).</dd>
<dt><strong><code>pre</code></strong></dt>
<dd>Preconditioner, function taking one native tensor like <code>y</code> as input and returning a native tensor like <code>x0</code>.</dd>
<dt><strong><code>matrix_offset</code></strong></dt>
<dd>Constant value to be added to every matrix entry, explicitly or implicitly. This can be used to stabilize solves for singular matrices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>SolveResult</code></p></div>
</dd>
<dt id="phiml.backend.Backend.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>self, start, stop, number)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.linspace_without_last"><code class="name flex">
<span>def <span class="ident">linspace_without_last</span></span>(<span>self, start, stop, number)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.list_devices"><code class="name flex">
<span>def <span class="ident">list_devices</span></span>(<span>self, device_type: Optional[str] = None) ‑> List[phiml.backend._backend.ComputeDevice]</span>
</code></dt>
<dd>
<div class="desc"><p>Fetches information about all available compute devices this backend can use.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://docs.python.org/3/library/os.html#os.cpu_count"><code>os.cpu_count</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.get_device_properties"><code>torch.cuda.get_device_properties</code></a></li>
<li>TensorFlow: <code>tensorflow.python.client.device_lib.list_local_devices</code></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.devices"><code>jax.devices</code></a></li>
</ul>
<p>See Also:
<code><a title="phiml.backend.Backend.set_default_device" href="#phiml.backend.Backend.set_default_device">Backend.set_default_device()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device_type</code></strong></dt>
<dd>(optional) Return only devices of this type, e.g. <code>'GPU'</code> or <code>'CPU'</code>. See <code><a title="phiml.backend.ComputeDevice.device_type" href="#phiml.backend.ComputeDevice.device_type">ComputeDevice.device_type</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>list</code> of all currently available devices.</p></div>
</dd>
<dt id="phiml.backend.Backend.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Natural logarithm</p></div>
</dd>
<dt id="phiml.backend.Backend.log10"><code class="name flex">
<span>def <span class="ident">log10</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.log2"><code class="name flex">
<span>def <span class="ident">log2</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.log_gamma"><code class="name flex">
<span>def <span class="ident">log_gamma</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.matrix_rank_dense"><code class="name flex">
<span>def <span class="ident">matrix_rank_dense</span></span>(<span>self, matrix, hermitian=False) ‑> ~TensorType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Dense matrix of shape (batch, rows, cols)</dd>
<dt><strong><code>hermitian</code></strong></dt>
<dd>Whether all matrices are guaranteed to be hermitian.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.matrix_solve_least_squares"><code class="name flex">
<span>def <span class="ident">matrix_solve_least_squares</span></span>(<span>self, matrix: ~TensorType, rhs: ~TensorType) ‑> Tuple[~TensorType, ~TensorType, ~TensorType, ~TensorType]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>Shape (batch, vec, constraints)</dd>
<dt><strong><code>rhs</code></strong></dt>
<dd>Shape (batch, vec, batch_per_matrix)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>solution</code></dt>
<dd>Solution vector of Shape (batch, constraints, batch_per_matrix)</dd>
<dt><code>residuals</code></dt>
<dd>Optional, can be <code>None</code></dd>
<dt><code>rank</code></dt>
<dd>Optional, can be <code>None</code></dd>
<dt><code>singular_values</code></dt>
<dd>Optional, can be <code>None</code></dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, value, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self, *coordinates)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.minimize"><code class="name flex">
<span>def <span class="ident">minimize</span></span>(<span>self, method: str, f, x0, atol, max_iter, trj: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.mod"><code class="name flex">
<span>def <span class="ident">mod</span></span>(<span>self, dividend, divisor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.module"><code class="name flex">
<span>def <span class="ident">module</span></span>(<span>self, variables: Dict[str, ~TensorType], forward: Callable = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.mul"><code class="name flex">
<span>def <span class="ident">mul</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.mul_coo_dense"><code class="name flex">
<span>def <span class="ident">mul_coo_dense</span></span>(<span>self, indices, values, shape, dense)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiply a batch of sparse coordinate matrices by a batch of dense matrices.
Every backend should implement this feature.
This is the fallback if CSR multiplication is not supported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>(batch, nnz, ndims)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>(batch, nnz, channels)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the full matrix, tuple of length ndims (sparse_rows, sparse_cols)</dd>
<dt><strong><code>dense</code></strong></dt>
<dd>(batch, dense_rows=sparse_cols, channels, dense_cols)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(batch, dense_rows=sparse_cols, channels, dense_cols)</p></div>
</dd>
<dt id="phiml.backend.Backend.mul_csr_dense"><code class="name flex">
<span>def <span class="ident">mul_csr_dense</span></span>(<span>self, column_indices, row_pointers, values, shape: Tuple[int, int], dense)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiply a batch of compressed sparse row matrices by a batch of dense matrices.</p>
<p>Optional feature.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.sparse_coo_tensor" href="#phiml.backend.Backend.sparse_coo_tensor">Backend.sparse_coo_tensor()</a></code>, <code><a title="phiml.backend.Backend.csc_matrix" href="#phiml.backend.Backend.csc_matrix">Backend.csc_matrix()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>column_indices</code></strong></dt>
<dd>(batch, nnz)</dd>
<dt><strong><code>row_pointers</code></strong></dt>
<dd>(batch, rows + 1)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>(batch, nnz, channels)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the full matrix (cols, rows)</dd>
<dt><strong><code>dense</code></strong></dt>
<dd>(batch, dense_rows=sparse_cols, channels, dense_cols)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(batch, dense_rows=sparse_cols, channels, dense_cols)</p></div>
</dd>
<dt id="phiml.backend.Backend.mul_matrix_batched_vector"><code class="name flex">
<span>def <span class="ident">mul_matrix_batched_vector</span></span>(<span>self, A, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.multi_slice"><code class="name flex">
<span>def <span class="ident">multi_slice</span></span>(<span>self, tensor, slices: tuple)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>value to slice</dd>
<dt><strong><code>slices</code></strong></dt>
<dd><code>tuple</code> of <code>slice</code>, <code>int</code>, or scalar integer tensors</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.ndims"><code class="name flex">
<span>def <span class="ident">ndims</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.nn_library"><code class="name flex">
<span>def <span class="ident">nn_library</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>self, values, length=None, fill_value=-1)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor with only spatial dimensions</dd>
<dt><strong><code>length</code></strong></dt>
<dd>(Optional) Length of the resulting array. If specified, the result array will be padded with <code>fill_value</code> or trimmed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>non-zero multi-indices as tensor of shape (nnz/length, vector)</p></div>
</dd>
<dt id="phiml.backend.Backend.not_equal"><code class="name flex">
<span>def <span class="ident">not_equal</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, tensor) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a NumPy representation of the given tensor.
If <code>tensor</code> is already a NumPy array, it is returned without modification.</p>
<p>This method raises an error if the value of the tensor is not known at this point, e.g. because it represents a node in a graph.
Use <code>is_available(tensor)</code> to check if the value can be represented as a NumPy array.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>backend-compatible tensor or sparse tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation of the values stored in the tensor</p></div>
</dd>
<dt id="phiml.backend.Backend.numpy_call"><code class="name flex">
<span>def <span class="ident">numpy_call</span></span>(<span>self, f, output_shapes, output_dtypes, *args, **aux_args)</span>
</code></dt>
<dd>
<div class="desc"><p>This call can be used in jit-compiled code but is not differentiable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function operating on numpy arrays.</dd>
<dt><strong><code>output_shapes</code></strong></dt>
<dd>Single shape <code>tuple</code> or tuple of shapes declaring the shapes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>output_dtypes</code></strong></dt>
<dd>Single <code>DType</code> or tuple of DTypes declaring the dtypes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Tensor arguments to be converted to NumPy arrays and then passed to <code>f</code>.</dd>
<dt><strong><code>**aux_args</code></strong></dt>
<dd>Keyword arguments to be passed to <code>f</code> without conversion.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Returned arrays of <code>f</code> converted to tensors.</p></div>
</dd>
<dt id="phiml.backend.Backend.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>self, shape, dtype: phiml.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.or_"><code class="name flex">
<span>def <span class="ident">or_</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, value, pad_width, mode: str = 'constant', constant_values=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Pad a tensor with values as specified by <code>mode</code> and <code>constant_values</code>.</p>
<p>If the mode is not supported, returns NotImplemented.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor</dd>
<dt><strong><code>pad_width</code></strong></dt>
<dd>2D tensor specifying the number of values padded to the edges of each axis in the form [[axis 0 lower, axis 0 upper], &hellip;] including batch and component axes.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>constant', 'boundary', 'periodic', 'symmetric', 'reflect'</dd>
<dt><strong><code>constant_values</code></strong></dt>
<dd>Scalar value used for out-of-bounds points if mode='constant'. Must be a Python primitive type or scalar tensor.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>str:
(Default value = 'constant')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>padded tensor or <code>NotImplemented</code></p></div>
</dd>
<dt id="phiml.backend.Backend.pad_stack"><code class="name flex">
<span>def <span class="ident">pad_stack</span></span>(<span>self, tensors, shape, pad_value=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.pad_to"><code class="name flex">
<span>def <span class="ident">pad_to</span></span>(<span>self, x, axis, new_size, fill_value)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.pow"><code class="name flex">
<span>def <span class="ident">pow</span></span>(<span>self, base, exp)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.prefers_channels_last"><code class="name flex">
<span>def <span class="ident">prefers_channels_last</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>self, value, axis=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.quantile"><code class="name flex">
<span>def <span class="ident">quantile</span></span>(<span>self, x, quantiles)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces the last / inner axis of x.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor</dd>
<dt><strong><code>quantiles</code></strong></dt>
<dd>List or 1D tensor of quantiles to compute.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor with shape (quantiles, *x.shape[:-1])</p></div>
</dd>
<dt id="phiml.backend.Backend.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>self, shape, dtype: phiml.backend._dtype.DType)</span>
</code></dt>
<dd>
<div class="desc"><p>Float tensor of selected precision containing random values sampled from a normal distribution with mean 0 and std 1.</p></div>
</dd>
<dt id="phiml.backend.Backend.random_permutations"><code class="name flex">
<span>def <span class="ident">random_permutations</span></span>(<span>self, permutations: int, n: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate <code>permutations</code> stacked arrays of shuffled integers between <code>0</code> and <code>n</code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.random_subsets"><code class="name flex">
<span>def <span class="ident">random_subsets</span></span>(<span>self, element_count: int, subset_size: int, subset_count: int, allow_duplicates: bool, element_weights=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>self, shape, low, high, dtype: Optional[phiml.backend._dtype.DType])</span>
</code></dt>
<dd>
<div class="desc"><p>Float tensor of selected precision containing random values in the range [0, 1)</p></div>
</dd>
<dt id="phiml.backend.Backend.range"><code class="name flex">
<span>def <span class="ident">range</span></span>(<span>self, start, limit=None, delta=1, dtype: phiml.backend._dtype.DType = int32)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.ravel_multi_index"><code class="name flex">
<span>def <span class="ident">ravel_multi_index</span></span>(<span>self, multi_index, shape, mode: Union[str, int] = 'undefined')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>multi_index</code></strong></dt>
<dd>(batch&hellip;, index_dim)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>1D tensor or tuple/list</dd>
<dt><strong><code>mode</code></strong></dt>
<dd><code>'undefined'</code>, <code>'periodic'</code>, <code>'clamp'</code> or an <code>int</code> to use for all invalid indices.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Integer tensor of shape (batch&hellip;) of same dtype as <code>multi_index</code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.repeat"><code class="name flex">
<span>def <span class="ident">repeat</span></span>(<span>self, x, repeats, axis: int, new_length=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeats the elements along <code>axis</code> <code>repeats</code> times.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor</dd>
<dt><strong><code>repeats</code></strong></dt>
<dd>How often to repeat each element. 1D tensor of length x.shape[axis]</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Which axis to repeat elements along</dd>
<dt><strong><code>new_length</code></strong></dt>
<dd>Set the length of <code>axis</code> after repeating. This is required for jit compilation with Jax.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>repeated Tensor</p></div>
</dd>
<dt id="phiml.backend.Backend.requires_fixed_shapes_when_tracing"><code class="name flex">
<span>def <span class="ident">requires_fixed_shapes_when_tracing</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.reset_peak_memory"><code class="name flex">
<span>def <span class="ident">reset_peak_memory</span></span>(<span>self, device: phiml.backend._backend.ComputeDevice)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.reshape"><code class="name flex">
<span>def <span class="ident">reshape</span></span>(<span>self, value, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>self, base_grid, indices, values, mode: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Batched n-dimensional scatter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd>Tensor into which scatter values are inserted at indices. Tensor of shape (batch_size, spatial&hellip;, channels)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>Tensor of shape (batch_size or 1, update_count, index_vector)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Values to scatter at indices. Tensor of shape (batch_size or 1, update_count or 1, channels or 1)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>One of ('update', 'add', 'max', 'min')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of base_grid with values at <code>indices</code> updated by <code>values</code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.scatter_1d_scalar"><code class="name flex">
<span>def <span class="ident">scatter_1d_scalar</span></span>(<span>self, base_grid, indices, values, mode: str)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd>(spatial,)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>(update_count,)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>(update_count or 1,)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>One of ('update', 'add')</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.scatter_nd"><code class="name flex">
<span>def <span class="ident">scatter_nd</span></span>(<span>self, base_grid, indices, values, mode: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Non-batched scatter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd>(spatial&hellip;, channels)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>(update_count, index_vector)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>(update_count or 1, channels or 1)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>One of ('update', 'add')</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.scatter_nd_scalar"><code class="name flex">
<span>def <span class="ident">scatter_nd_scalar</span></span>(<span>self, base_grid, indices, values, mode: str)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd>(spatial&hellip;,)</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>(update_count, index_vector)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>(update_count or 1,)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>One of ('update', 'add')</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.searchsorted"><code class="name flex">
<span>def <span class="ident">searchsorted</span></span>(<span>self, sorted_sequence, search_values, side: str, dtype=int32)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>self, seed: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.set_default_device"><code class="name flex">
<span>def <span class="ident">set_default_device</span></span>(<span>self, device: Union[phiml.backend._backend.ComputeDevice, str]) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the device new tensors will be allocated on.
This function will do nothing if the target device type is not available.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.list_devices" href="#phiml.backend.Backend.list_devices">Backend.list_devices()</a></code>, <code><a title="phiml.backend.Backend.get_default_device" href="#phiml.backend.Backend.get_default_device">Backend.get_default_device()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong></dt>
<dd><code><a title="phiml.backend.ComputeDevice" href="#phiml.backend.ComputeDevice">ComputeDevice</a></code> or device type as <code>str</code>, such as <code>'CPU'</code> or <code>'GPU'</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code> whether the device was successfully set.</p></div>
</dd>
<dt id="phiml.backend.Backend.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the shape of a tensor.
The shape is iterable and implements <code>len()</code>.
For non-eager tensors, undefined dimensions should return a placeholder value representing the size.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.staticshape" href="#phiml.backend.Backend.staticshape">Backend.staticshape()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Native tensor compatible with this backend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape of <code>tensor</code></p></div>
</dd>
<dt id="phiml.backend.Backend.shift_bits_left"><code class="name flex">
<span>def <span class="ident">shift_bits_left</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.shift_bits_right"><code class="name flex">
<span>def <span class="ident">shift_bits_right</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sinh"><code class="name flex">
<span>def <span class="ident">sinh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.size"><code class="name flex">
<span>def <span class="ident">size</span></span>(<span>self, array)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sizeof"><code class="name flex">
<span>def <span class="ident">sizeof</span></span>(<span>self, tensor) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the size in bytes</p></div>
</dd>
<dt id="phiml.backend.Backend.softplus"><code class="name flex">
<span>def <span class="ident">softplus</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.solve_triangular"><code class="name flex">
<span>def <span class="ident">solve_triangular</span></span>(<span>self, matrix, rhs, lower: bool, unit_diagonal: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a sparse or dense triangular solve, depending on the format of <code>matrix</code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.solve_triangular_dense"><code class="name flex">
<span>def <span class="ident">solve_triangular_dense</span></span>(<span>self, matrix, rhs, lower: bool, unit_diagonal: bool)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>(batch_size, rows, cols)</dd>
<dt><strong><code>rhs</code></strong></dt>
<dd>(batch_size, cols)</dd>
</dl>
<p>lower:
unit_diagonal:</p>
<h2 id="returns">Returns</h2>
<p>(batch_size, cols)</p></div>
</dd>
<dt id="phiml.backend.Backend.solve_triangular_sparse"><code class="name flex">
<span>def <span class="ident">solve_triangular_sparse</span></span>(<span>self, matrix, rhs, lower: bool, unit_diagonal: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sort"><code class="name flex">
<span>def <span class="ident">sort</span></span>(<span>self, x, axis=-1)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sparse_coo_tensor"><code class="name flex">
<span>def <span class="ident">sparse_coo_tensor</span></span>(<span>self, indices: ~TensorType, values: ~TensorType, shape: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a sparse matrix in coordinate list (COO) format.</p>
<p>Optional feature.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.csr_matrix" href="#phiml.backend.Backend.csr_matrix">Backend.csr_matrix()</a></code>, <code><a title="phiml.backend.Backend.csc_matrix" href="#phiml.backend.Backend.csc_matrix">Backend.csc_matrix()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>2D tensor of shape <code>(nnz, dims)</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>1D values tensor matching <code>indices</code></dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape of the sparse matrix</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native representation of the sparse matrix</p></div>
</dd>
<dt id="phiml.backend.Backend.sparse_coo_tensor_batched"><code class="name flex">
<span>def <span class="ident">sparse_coo_tensor_batched</span></span>(<span>self, indices: Union[tuple, list], values, shape: tuple)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>shape (batch_size, dims, nnz)</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Values tensor matching <code>indices</code>, shape (batch_size, nnz)</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tuple of two ints representing the dense shape, (dims&hellip;)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.stack"><code class="name flex">
<span>def <span class="ident">stack</span></span>(<span>self, values, axis=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.stack_leaves"><code class="name flex">
<span>def <span class="ident">stack_leaves</span></span>(<span>self, trees: Union[tuple, list], axis=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.staticshape"><code class="name flex">
<span>def <span class="ident">staticshape</span></span>(<span>self, tensor) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the static shape of a native tensor.
If the tensor is eager, the shape is a <code>tuple[int]</code>.
For placeholder tensors, unknown dimensions are represented as <code>None</code>.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.shape" href="#phiml.backend.Backend.shape">Backend.shape()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Native tensor compatible with this backend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of sizes. Each size is an <code>int</code> if the size is defined, else <code>None</code>.</p></div>
</dd>
<dt id="phiml.backend.Backend.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, x, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>self, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.stop_gradient_tree"><code class="name flex">
<span>def <span class="ident">stop_gradient_tree</span></span>(<span>self, tree)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sub"><code class="name flex">
<span>def <span class="ident">sub</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>self, value, axis=None, keepdims=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.supports"><code class="name flex">
<span>def <span class="ident">supports</span></span>(<span>self, feature: Union[str, Callable]) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if this backend supports the given feature.
Features correspond to a method of this backend that must be implemented if the feature is supported.</p>
<p>Possible features:</p>
<ul>
<li><code>sparse_coo_tensor</code></li>
<li>`gradients</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>feature</code></strong></dt>
<dd><code>str</code> or unbound Backend method, e.g. <code><a title="phiml.backend.Backend.sparse_coo_tensor" href="#phiml.backend.Backend.sparse_coo_tensor">Backend.sparse_coo_tensor()</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Whether the feature is supported.</p></div>
</dd>
<dt id="phiml.backend.Backend.svd"><code class="name flex">
<span>def <span class="ident">svd</span></span>(<span>self, matrix: ~TensorType, full_matrices=True) ‑> Tuple[~TensorType, ~TensorType, ~TensorType]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>matrix</code></strong></dt>
<dd>(batch&hellip;, m, n)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>eigenvalues</code></dt>
<dd>(batch&hellip;, n,)</dd>
<dt><code>eigenvectors</code></dt>
<dd>(batch&hellip;, n, n)</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.tan"><code class="name flex">
<span>def <span class="ident">tan</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.tanh"><code class="name flex">
<span>def <span class="ident">tanh</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.tensordot"><code class="name flex">
<span>def <span class="ident">tensordot</span></span>(<span>self, a, a_axes: Union[tuple, list], b, b_axes: Union[tuple, list])</span>
</code></dt>
<dd>
<div class="desc"><p>Multiply-sum-reduce a_axes of a with b_axes of b.</p></div>
</dd>
<dt id="phiml.backend.Backend.tile"><code class="name flex">
<span>def <span class="ident">tile</span></span>(<span>self, value, multiples)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeats the full tensor along each axis the number of times given by multiples.
If <code>multiples</code> has more dimensions than <code>value</code>, these dimensions are added to <code>value</code> as outer dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor</dd>
<dt><strong><code>multiples</code></strong></dt>
<dd>tuple or list of integers</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tiled tensor</p></div>
</dd>
<dt id="phiml.backend.Backend.tile_to"><code class="name flex">
<span>def <span class="ident">tile_to</span></span>(<span>self, x, axis, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.to_dlpack"><code class="name flex">
<span>def <span class="ident">to_dlpack</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a tensor to floating point values with precision equal to the currently set default precision.</p>
<p>See Also:
<code><a title="phiml.backend.Backend.precision" href="#phiml.backend.Backend.precision">Backend.precision</a></code>.</p>
<p>If <code>x</code> is mutable and of the correct floating type, returns a copy of <code>x</code>.</p>
<p>To convert float tensors to the backend precision but leave non-float tensors untouched, use <code><a title="phiml.backend.Backend.as_tensor" href="#phiml.backend.Backend.as_tensor">Backend.as_tensor()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of bool, int or float</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Values of <code>x</code> as float tensor</p></div>
</dd>
<dt id="phiml.backend.Backend.to_int32"><code class="name flex">
<span>def <span class="ident">to_int32</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.to_int64"><code class="name flex">
<span>def <span class="ident">to_int64</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>self, tensor, axes)</span>
</code></dt>
<dd>
<div class="desc"><p>Transposes the dimensions of <code>tensor</code> given the new axes order. The tensor will be cast to the default precision in the process.</p></div>
</dd>
<dt id="phiml.backend.Backend.unique"><code class="name flex">
<span>def <span class="ident">unique</span></span>(<span>self, x: ~TensorType, return_inverse: bool, return_counts: bool, axis: int) ‑> Tuple[~TensorType, ...]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional int array. Will compare <code>axis</code>-slices of <code>x</code> for multidimensional <code>x</code>.</dd>
<dt><strong><code>return_inverse</code></strong></dt>
<dd>Whether to return the inverse</dd>
<dt><strong><code>return_counts</code></strong></dt>
<dd>Whether to return the counts.</dd>
<dt><strong><code>axis</code></strong></dt>
<dd>Axis along which slices of <code>x</code> should be compared.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>unique_slices</code></dt>
<dd>Sorted unique slices of <code>x</code></dd>
<dt><code>unique_inverse</code></dt>
<dd>(optional) index of the unique slice for each slice of <code>x</code></dd>
<dt><code>unique_counts</code></dt>
<dd>Number of occurrences of each unique slices</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.unravel_index"><code class="name flex">
<span>def <span class="ident">unravel_index</span></span>(<span>self, flat_index, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, tensor, axis=0, keepdims=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.variable"><code class="name flex">
<span>def <span class="ident">variable</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.vectorized_call"><code class="name flex">
<span>def <span class="ident">vectorized_call</span></span>(<span>self, f, *args, output_dtypes=None, **aux_args)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function with only positional tensor argument, returning one or multiple tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Batched inputs for <code>f</code>. The first dimension of all <code>args</code> is vectorized.
All tensors in <code>args</code> must have the same size or <code>1</code> in their first dimension.</dd>
<dt><strong><code>output_dtypes</code></strong></dt>
<dd>Single <code>DType</code> or tuple of DTypes declaring the dtypes of the tensors returned by <code>f</code>.</dd>
<dt><strong><code>**aux_args</code></strong></dt>
<dd>Non-vectorized keyword arguments to be passed to <code>f</code>.</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Backend.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>self, condition, x=None, y=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.while_loop"><code class="name flex">
<span>def <span class="ident">while_loop</span></span>(<span>self, loop: Callable, values: tuple, max_iter: Union[int, Tuple[int, ...], List[int]])</span>
</code></dt>
<dd>
<div class="desc"><p>If <code>max_iter is None</code>, runs</p>
<pre><code class="language-python">while any(values[0]):
    values = loop(*values)
return values
</code></pre>
<p>This operation does not support backpropagation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>loop</code></strong></dt>
<dd>Loop function, must return a <code>tuple</code> with entries equal to <code>values</code> in shape and data type.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Initial values of loop variables.</dd>
<dt><strong><code>max_iter</code></strong></dt>
<dd>Maximum number of iterations to run, single <code>int</code> or sequence of integers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Loop variables upon loop completion if <code>max_iter</code> is a single integer.
If <code>max_iter</code> is a sequence, stacks the variables after each entry in <code>max_iter</code>, adding an outer dimension of size <code>&lt;= len(max_iter)</code>.
If the condition is fulfilled before the maximum max_iter is reached, the loop may be broken or not, depending on the implementation.
If the loop is broken, the values returned by the last loop are expected to be constant and filled.</p></div>
</dd>
<dt id="phiml.backend.Backend.xor"><code class="name flex">
<span>def <span class="ident">xor</span></span>(<span>self, a, b)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>self, shape, dtype: phiml.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phiml.backend.Backend.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>self, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="phiml.backend.ComputeDevice"><code class="flex name class">
<span>class <span class="ident">ComputeDevice</span></span>
</code></dt>
<dd>
<div class="desc"><p>A physical device that can be selected to perform backend computations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ComputeDevice:
    &#34;&#34;&#34;
    A physical device that can be selected to perform backend computations.
    &#34;&#34;&#34;

    def __init__(self, backend: &#39;Backend&#39;, name: str, device_type: str, memory: int, processor_count: int, description: str, ref):
        assert device_type in (&#39;CPU&#39;, &#39;GPU&#39;, &#39;TPU&#39;)
        self.name: str = name
        &#34;&#34;&#34; Name of the compute device. CPUs are typically called `&#39;CPU&#39;`. &#34;&#34;&#34;
        self.device_type: str = device_type
        &#34;&#34;&#34; Type of device such as `&#39;CPU&#39;`, `&#39;GPU&#39;` or `&#39;TPU&#39;`. &#34;&#34;&#34;
        self.memory: int = memory
        &#34;&#34;&#34; Maximum memory of the device that can be allocated (in bytes). -1 for n/a. &#34;&#34;&#34;
        self.processor_count: int = processor_count
        &#34;&#34;&#34; Number of CPU cores or GPU multiprocessors. -1 for n/a. &#34;&#34;&#34;
        self.description: str = description
        &#34;&#34;&#34; Further information about the device such as driver version. &#34;&#34;&#34;
        self.ref = ref
        &#34;&#34;&#34; Reference to the internal device representation. Two devices are equal if their refs are equal. &#34;&#34;&#34;
        self.backend: &#39;Backend&#39; = backend
        &#34;&#34;&#34; Backend that this device belongs to. Different backends represent the same device with different objects. &#34;&#34;&#34;

    def __repr__(self):
        mem = f&#34;{(self.memory / 1024 ** 2):.0f} MB&#34; if self.memory &gt; 0 else &#34;memory: n/a&#34;
        pro = f&#34;{self.processor_count} processors&#34; if self.processor_count &gt; 0 else &#34;processors: n/a&#34;
        ref = f&#34; &#39;{self.ref}&#39;&#34; if isinstance(self.ref, str) else &#34;&#34;
        descr = self.description.replace(&#39;\n&#39;, &#39;  &#39;)
        if len(descr) &gt; 30:
            descr = descr[:28] + &#34;...&#34;
        return f&#34;{self.backend} device &#39;{self.name}&#39; ({self.device_type}{ref}) | {mem} | {pro} | {descr}&#34;

    def __eq__(self, other):
        return isinstance(other, ComputeDevice) and other.ref == self.ref

    def __hash__(self):
        return hash(self.ref)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.backend.ComputeDevice.backend"><code class="name">var <span class="ident">backend</span></code></dt>
<dd>
<div class="desc"><p>Backend that this device belongs to. Different backends represent the same device with different objects.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.description"><code class="name">var <span class="ident">description</span></code></dt>
<dd>
<div class="desc"><p>Further information about the device such as driver version.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.device_type"><code class="name">var <span class="ident">device_type</span></code></dt>
<dd>
<div class="desc"><p>Type of device such as <code>'CPU'</code>, <code>'GPU'</code> or <code>'TPU'</code>.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.memory"><code class="name">var <span class="ident">memory</span></code></dt>
<dd>
<div class="desc"><p>Maximum memory of the device that can be allocated (in bytes). -1 for n/a.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.name"><code class="name">var <span class="ident">name</span></code></dt>
<dd>
<div class="desc"><p>Name of the compute device. CPUs are typically called <code>'CPU'</code>.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.processor_count"><code class="name">var <span class="ident">processor_count</span></code></dt>
<dd>
<div class="desc"><p>Number of CPU cores or GPU multiprocessors. -1 for n/a.</p></div>
</dd>
<dt id="phiml.backend.ComputeDevice.ref"><code class="name">var <span class="ident">ref</span></code></dt>
<dd>
<div class="desc"><p>Reference to the internal device representation. Two devices are equal if their refs are equal.</p></div>
</dd>
</dl>
</dd>
<dt id="phiml.backend.NoBackendFound"><code class="flex name class">
<span>class <span class="ident">NoBackendFound</span></span>
</code></dt>
<dd>
<div class="desc"><p>Thrown by <code><a title="phiml.backend.choose_backend" href="#phiml.backend.choose_backend">choose_backend()</a></code> if no backend can handle the given values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NoBackendFound(Exception):
    &#34;&#34;&#34;
    Thrown by `choose_backend` if no backend can handle the given values.
    &#34;&#34;&#34;

    def __init__(self, msg):
        Exception.__init__(self, msg)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phiml.backend.Profile"><code class="flex name class">
<span>class <span class="ident">Profile</span></span>
</code></dt>
<dd>
<div class="desc"><p>Stores information about calls to backends and their timing.</p>
<p>Profile may be created through <code><a title="phiml.backend.profile" href="#phiml.backend.profile">profile()</a></code> or <code><a title="phiml.backend.profile_function" href="#phiml.backend.profile_function">profile_function()</a></code>.</p>
<p>Profiles can be printed or saved to disc.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Profile:
    &#34;&#34;&#34;
    Stores information about calls to backends and their timing.

    Profile may be created through `profile()` or `profile_function()`.

    Profiles can be printed or saved to disc.
    &#34;&#34;&#34;

    def __init__(self, trace: bool, backends: Union[tuple, list], subtract_trace_time: bool):
        self._start = perf_counter()
        self._stop = None
        self._root = ExtCall(None, &#34;&#34;, 0, &#34;&#34;, &#34;&#34;, &#34;&#34;, -1)
        self._last_ext_call = self._root
        self._messages = []
        self._trace = trace
        self._backend_calls = []
        self._retime_index = -1
        self._accumulating = False
        self._backends = backends
        self._subtract_trace_time = subtract_trace_time
        self._total_trace_time = 0

    def _add_call(self, backend_call: BackendCall, args: tuple, kwargs: dict, result):
        if self._retime_index &gt;= 0:
            prev_call = self._backend_calls[self._retime_index]
            assert prev_call._function_name == backend_call._function_name
            if self._accumulating:
                prev_call._start += backend_call._start
                prev_call._stop += backend_call._stop
            else:
                prev_call._start = backend_call._start
                prev_call._stop = backend_call._stop
            self._retime_index = (self._retime_index + 1) % len(self._backend_calls)
        else:
            self._backend_calls.append(backend_call)
            args = {i: arg for i, arg in enumerate(args)}
            args.update(kwargs)
            backend_call.add_arg(&#34;Inputs&#34;, _format_values(args, backend_call._backend))
            if isinstance(result, (tuple, list)):
                backend_call.add_arg(&#34;Outputs&#34;, _format_values({i: res for i, res in enumerate(result)}, backend_call._backend))
            else:
                backend_call.add_arg(&#34;Outputs&#34;, _format_values({0: result}, backend_call._backend))
            if self._trace:
                stack = inspect.stack()[2:]
                call = self._last_ext_call.common_call(stack)
                for i in range(call._level, len(stack)):
                    stack_frame = stack[len(stack) - i - 1]
                    name = ExtCall.determine_name(stack_frame)  # if len(stack) - i &gt; 1 else &#34;&#34;
                    sub_call = ExtCall(call, name, i + 1, stack_frame.function, stack_frame.code_context, stack_frame.filename, stack_frame.lineno)
                    call.add(sub_call)
                    call = sub_call
                call.add(backend_call)
                self._last_ext_call = call
            if self._subtract_trace_time:
                delta_trace_time = perf_counter() - backend_call._stop
                backend_call._start -= self._total_trace_time
                backend_call._stop -= self._total_trace_time
                self._total_trace_time += delta_trace_time

    def _finish(self):
        self._stop = perf_counter()
        self._children_to_properties()

    @property
    def duration(self) -&gt; float:
        &#34;&#34;&#34; Total time passed from creation of the profile to the end of the last operation. &#34;&#34;&#34;
        return self._stop - self._start if self._stop is not None else None

    def print(self, min_duration=1e-3, code_col=80, code_len=50):
        &#34;&#34;&#34;
        Prints this profile to the console.

        Args:
            min_duration: Hides elements with less time spent on backend calls than `min_duration` (seconds)
            code_col: Formatting option for where the context code is printed.
            code_len: Formatting option for cropping the context code
        &#34;&#34;&#34;
        print(f&#34;Profile: {self.duration:.4f} seconds total. Skipping elements shorter than {1000 * min_duration:.2f} ms&#34;)
        if self._messages:
            print(&#34;External profiling:&#34;)
            for message in self._messages:
                print(f&#34;  {message}&#34;)
            print()
        self._root.print(min_duration=min_duration, code_col=code_col, code_len=code_len)

    def save(self, json_file: str):
        &#34;&#34;&#34;
        Saves this profile to disc using the *trace event format* described at
        https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit

        This file can be viewed with external applications such as Google chrome.

        Args:
            json_file: filename
        &#34;&#34;&#34;
        data = [
            {&#39;name&#39;: &#34;process_name&#34;, &#39;ph&#39;: &#39;M&#39;, &#39;pid&#39;: 0, &#39;tid&#39;: 0, &#34;args&#34;: {&#34;name&#34;: &#34;0 Python calls&#34;}},
            {&#39;name&#39;: &#34;process_name&#34;, &#39;ph&#39;: &#39;M&#39;, &#39;pid&#39;: 1, &#39;tid&#39;: 1, &#34;args&#34;: {&#34;name&#34;: &#34;1 Operations&#34;}},
        ] + [
            {&#39;name&#39;: &#34;thread_name&#34;, &#39;ph&#39;: &#39;M&#39;, &#39;pid&#39;: 1, &#39;tid&#39;: i + 1, &#34;args&#34;: {&#34;name&#34;: backend.name}}
            for i, backend in enumerate(self._backends)
        ]
        if self._trace:
            if len(self._root._children) &gt; 0:
                data.extend(self._root.trace_json_events())
        else:
            data.extend(sum([call.trace_json_events(()) for call in self._backend_calls], []))
        with open(json_file, &#39;w&#39;) as file:
            json.dump(data, file)

    save_trace = save

    def _children_to_properties(self):
        children = self._root.children_to_properties()
        for name, child in children.items():
            setattr(self, name, child)

    def add_external_message(self, message: str):
        &#34;&#34;&#34; Stores an external message in this profile. External messages are printed in `Profile.print()`. &#34;&#34;&#34;
        self._messages.append(message)

    @contextmanager
    def retime(self):
        &#34;&#34;&#34;
        To be used in `with` statements, `with prof.retime(): ...`.

        Updates this profile by running the same operations again but without tracing.
        This gives a much better indication of the true timing.
        The code within the `with` block must perform the same operations as the code that created this profile.

        *Warning:* Internal caching may reduce the number of operations after the first time a function is called.
        To prevent this, run the function before profiling it, see `warmup` in `profile_function()`.
        &#34;&#34;&#34;
        self._retime_index = 0
        restore_data = _start_profiling(self, self._backends)
        try:
            yield None
        finally:
            _stop_profiling(self, *restore_data)
            assert self._retime_index == 0, f&#34;Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, &#34;
            self._retime_index = -1

    @contextmanager
    def _accumulate_average(self, n):
        self._retime_index = 0
        self._accumulating = True
        restore_data = _start_profiling(self, self._backends)
        try:
            yield None
        finally:
            _stop_profiling(self, *restore_data)
            assert self._retime_index == 0, f&#34;Number of calls during retime did not match original profile, originally {len(self._backend_calls)}, now {self._retime_index}, &#34;
            self._retime_index = -1
            for call in self._backend_calls:
                call._start /= n
                call._stop /= n
            self._accumulating = False</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phiml.backend.Profile.duration"><code class="name">prop <span class="ident">duration</span> : float</code></dt>
<dd>
<div class="desc"><p>Total time passed from creation of the profile to the end of the last operation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def duration(self) -&gt; float:
    &#34;&#34;&#34; Total time passed from creation of the profile to the end of the last operation. &#34;&#34;&#34;
    return self._stop - self._start if self._stop is not None else None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phiml.backend.Profile.add_external_message"><code class="name flex">
<span>def <span class="ident">add_external_message</span></span>(<span>self, message: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Stores an external message in this profile. External messages are printed in <code><a title="phiml.backend.Profile.print" href="#phiml.backend.Profile.print">Profile.print()</a></code>.</p></div>
</dd>
<dt id="phiml.backend.Profile.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>self, min_duration=0.001, code_col=80, code_len=50)</span>
</code></dt>
<dd>
<div class="desc"><p>Prints this profile to the console.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>min_duration</code></strong></dt>
<dd>Hides elements with less time spent on backend calls than <code>min_duration</code> (seconds)</dd>
<dt><strong><code>code_col</code></strong></dt>
<dd>Formatting option for where the context code is printed.</dd>
<dt><strong><code>code_len</code></strong></dt>
<dd>Formatting option for cropping the context code</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Profile.retime"><code class="name flex">
<span>def <span class="ident">retime</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>To be used in <code>with</code> statements, <code>with prof.retime(): ...</code>.</p>
<p>Updates this profile by running the same operations again but without tracing.
This gives a much better indication of the true timing.
The code within the <code>with</code> block must perform the same operations as the code that created this profile.</p>
<p><em>Warning:</em> Internal caching may reduce the number of operations after the first time a function is called.
To prevent this, run the function before profiling it, see <code>warmup</code> in <code><a title="phiml.backend.profile_function" href="#phiml.backend.profile_function">profile_function()</a></code>.</p></div>
</dd>
<dt id="phiml.backend.Profile.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, json_file: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves this profile to disc using the <em>trace event format</em> described at
<a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit">https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit</a></p>
<p>This file can be viewed with external applications such as Google chrome.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>json_file</code></strong></dt>
<dd>filename</dd>
</dl></div>
</dd>
<dt id="phiml.backend.Profile.save_trace"><code class="name flex">
<span>def <span class="ident">save_trace</span></span>(<span>self, json_file: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves this profile to disc using the <em>trace event format</em> described at
<a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit">https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit</a></p>
<p>This file can be viewed with external applications such as Google chrome.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>json_file</code></strong></dt>
<dd>filename</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phiml" href="../index.html">phiml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phiml.backend.jax" href="jax/index.html">phiml.backend.jax</a></code></li>
<li><code><a title="phiml.backend.tensorflow" href="tensorflow/index.html">phiml.backend.tensorflow</a></code></li>
<li><code><a title="phiml.backend.torch" href="torch/index.html">phiml.backend.torch</a></code></li>
<li><code><a title="phiml.backend.xops" href="xops.html">phiml.backend.xops</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="phiml.backend.OBJECTS" href="#phiml.backend.OBJECTS">OBJECTS</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phiml.backend.choose_backend" href="#phiml.backend.choose_backend">choose_backend</a></code></li>
<li><code><a title="phiml.backend.context_backend" href="#phiml.backend.context_backend">context_backend</a></code></li>
<li><code><a title="phiml.backend.convert" href="#phiml.backend.convert">convert</a></code></li>
<li><code><a title="phiml.backend.default_backend" href="#phiml.backend.default_backend">default_backend</a></code></li>
<li><code><a title="phiml.backend.get_current_profile" href="#phiml.backend.get_current_profile">get_current_profile</a></code></li>
<li><code><a title="phiml.backend.get_precision" href="#phiml.backend.get_precision">get_precision</a></code></li>
<li><code><a title="phiml.backend.precision" href="#phiml.backend.precision">precision</a></code></li>
<li><code><a title="phiml.backend.profile" href="#phiml.backend.profile">profile</a></code></li>
<li><code><a title="phiml.backend.profile_function" href="#phiml.backend.profile_function">profile_function</a></code></li>
<li><code><a title="phiml.backend.set_global_default_backend" href="#phiml.backend.set_global_default_backend">set_global_default_backend</a></code></li>
<li><code><a title="phiml.backend.set_global_precision" href="#phiml.backend.set_global_precision">set_global_precision</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phiml.backend.Backend" href="#phiml.backend.Backend">Backend</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.Backend.abs" href="#phiml.backend.Backend.abs">abs</a></code></li>
<li><code><a title="phiml.backend.Backend.add" href="#phiml.backend.Backend.add">add</a></code></li>
<li><code><a title="phiml.backend.Backend.all" href="#phiml.backend.Backend.all">all</a></code></li>
<li><code><a title="phiml.backend.Backend.allocate_on_device" href="#phiml.backend.Backend.allocate_on_device">allocate_on_device</a></code></li>
<li><code><a title="phiml.backend.Backend.and_" href="#phiml.backend.Backend.and_">and_</a></code></li>
<li><code><a title="phiml.backend.Backend.any" href="#phiml.backend.Backend.any">any</a></code></li>
<li><code><a title="phiml.backend.Backend.arccos" href="#phiml.backend.Backend.arccos">arccos</a></code></li>
<li><code><a title="phiml.backend.Backend.arccosh" href="#phiml.backend.Backend.arccosh">arccosh</a></code></li>
<li><code><a title="phiml.backend.Backend.arcsin" href="#phiml.backend.Backend.arcsin">arcsin</a></code></li>
<li><code><a title="phiml.backend.Backend.arcsinh" href="#phiml.backend.Backend.arcsinh">arcsinh</a></code></li>
<li><code><a title="phiml.backend.Backend.arctan" href="#phiml.backend.Backend.arctan">arctan</a></code></li>
<li><code><a title="phiml.backend.Backend.arctan2" href="#phiml.backend.Backend.arctan2">arctan2</a></code></li>
<li><code><a title="phiml.backend.Backend.arctanh" href="#phiml.backend.Backend.arctanh">arctanh</a></code></li>
<li><code><a title="phiml.backend.Backend.argmax" href="#phiml.backend.Backend.argmax">argmax</a></code></li>
<li><code><a title="phiml.backend.Backend.argmin" href="#phiml.backend.Backend.argmin">argmin</a></code></li>
<li><code><a title="phiml.backend.Backend.argsort" href="#phiml.backend.Backend.argsort">argsort</a></code></li>
<li><code><a title="phiml.backend.Backend.as_registered" href="#phiml.backend.Backend.as_registered">as_registered</a></code></li>
<li><code><a title="phiml.backend.Backend.as_tensor" href="#phiml.backend.Backend.as_tensor">as_tensor</a></code></li>
<li><code><a title="phiml.backend.Backend.auto_cast" href="#phiml.backend.Backend.auto_cast">auto_cast</a></code></li>
<li><code><a title="phiml.backend.Backend.auto_cast1" href="#phiml.backend.Backend.auto_cast1">auto_cast1</a></code></li>
<li><code><a title="phiml.backend.Backend.batch_gather" href="#phiml.backend.Backend.batch_gather">batch_gather</a></code></li>
<li><code><a title="phiml.backend.Backend.batched_bincount" href="#phiml.backend.Backend.batched_bincount">batched_bincount</a></code></li>
<li><code><a title="phiml.backend.Backend.batched_gather_1d" href="#phiml.backend.Backend.batched_gather_1d">batched_gather_1d</a></code></li>
<li><code><a title="phiml.backend.Backend.batched_gather_nd" href="#phiml.backend.Backend.batched_gather_nd">batched_gather_nd</a></code></li>
<li><code><a title="phiml.backend.Backend.bi_conjugate_gradient" href="#phiml.backend.Backend.bi_conjugate_gradient">bi_conjugate_gradient</a></code></li>
<li><code><a title="phiml.backend.Backend.bincount" href="#phiml.backend.Backend.bincount">bincount</a></code></li>
<li><code><a title="phiml.backend.Backend.block_until_ready" href="#phiml.backend.Backend.block_until_ready">block_until_ready</a></code></li>
<li><code><a title="phiml.backend.Backend.boolean_mask" href="#phiml.backend.Backend.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phiml.backend.Backend.call" href="#phiml.backend.Backend.call">call</a></code></li>
<li><code><a title="phiml.backend.Backend.cast" href="#phiml.backend.Backend.cast">cast</a></code></li>
<li><code><a title="phiml.backend.Backend.ceil" href="#phiml.backend.Backend.ceil">ceil</a></code></li>
<li><code><a title="phiml.backend.Backend.clip" href="#phiml.backend.Backend.clip">clip</a></code></li>
<li><code><a title="phiml.backend.Backend.combine_types" href="#phiml.backend.Backend.combine_types">combine_types</a></code></li>
<li><code><a title="phiml.backend.Backend.complex_type" href="#phiml.backend.Backend.complex_type">complex_type</a></code></li>
<li><code><a title="phiml.backend.Backend.concat" href="#phiml.backend.Backend.concat">concat</a></code></li>
<li><code><a title="phiml.backend.Backend.conj" href="#phiml.backend.Backend.conj">conj</a></code></li>
<li><code><a title="phiml.backend.Backend.conjugate_gradient" href="#phiml.backend.Backend.conjugate_gradient">conjugate_gradient</a></code></li>
<li><code><a title="phiml.backend.Backend.conjugate_gradient_adaptive" href="#phiml.backend.Backend.conjugate_gradient_adaptive">conjugate_gradient_adaptive</a></code></li>
<li><code><a title="phiml.backend.Backend.conv" href="#phiml.backend.Backend.conv">conv</a></code></li>
<li><code><a title="phiml.backend.Backend.coo_to_dense" href="#phiml.backend.Backend.coo_to_dense">coo_to_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.copy" href="#phiml.backend.Backend.copy">copy</a></code></li>
<li><code><a title="phiml.backend.Backend.copy_leaves" href="#phiml.backend.Backend.copy_leaves">copy_leaves</a></code></li>
<li><code><a title="phiml.backend.Backend.cos" href="#phiml.backend.Backend.cos">cos</a></code></li>
<li><code><a title="phiml.backend.Backend.cosh" href="#phiml.backend.Backend.cosh">cosh</a></code></li>
<li><code><a title="phiml.backend.Backend.csc_matrix" href="#phiml.backend.Backend.csc_matrix">csc_matrix</a></code></li>
<li><code><a title="phiml.backend.Backend.csc_matrix_batched" href="#phiml.backend.Backend.csc_matrix_batched">csc_matrix_batched</a></code></li>
<li><code><a title="phiml.backend.Backend.csr_matrix" href="#phiml.backend.Backend.csr_matrix">csr_matrix</a></code></li>
<li><code><a title="phiml.backend.Backend.csr_matrix_batched" href="#phiml.backend.Backend.csr_matrix_batched">csr_matrix_batched</a></code></li>
<li><code><a title="phiml.backend.Backend.csr_to_coo" href="#phiml.backend.Backend.csr_to_coo">csr_to_coo</a></code></li>
<li><code><a title="phiml.backend.Backend.csr_to_dense" href="#phiml.backend.Backend.csr_to_dense">csr_to_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.cumsum" href="#phiml.backend.Backend.cumsum">cumsum</a></code></li>
<li><code><a title="phiml.backend.Backend.custom_gradient" href="#phiml.backend.Backend.custom_gradient">custom_gradient</a></code></li>
<li><code><a title="phiml.backend.Backend.determine_size" href="#phiml.backend.Backend.determine_size">determine_size</a></code></li>
<li><code><a title="phiml.backend.Backend.disassemble" href="#phiml.backend.Backend.disassemble">disassemble</a></code></li>
<li><code><a title="phiml.backend.Backend.div" href="#phiml.backend.Backend.div">div</a></code></li>
<li><code><a title="phiml.backend.Backend.divide_no_nan" href="#phiml.backend.Backend.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="phiml.backend.Backend.dtype" href="#phiml.backend.Backend.dtype">dtype</a></code></li>
<li><code><a title="phiml.backend.Backend.eig" href="#phiml.backend.Backend.eig">eig</a></code></li>
<li><code><a title="phiml.backend.Backend.eigvals" href="#phiml.backend.Backend.eigvals">eigvals</a></code></li>
<li><code><a title="phiml.backend.Backend.einsum" href="#phiml.backend.Backend.einsum">einsum</a></code></li>
<li><code><a title="phiml.backend.Backend.equal" href="#phiml.backend.Backend.equal">equal</a></code></li>
<li><code><a title="phiml.backend.Backend.erf" href="#phiml.backend.Backend.erf">erf</a></code></li>
<li><code><a title="phiml.backend.Backend.exp" href="#phiml.backend.Backend.exp">exp</a></code></li>
<li><code><a title="phiml.backend.Backend.expand_dims" href="#phiml.backend.Backend.expand_dims">expand_dims</a></code></li>
<li><code><a title="phiml.backend.Backend.factorial" href="#phiml.backend.Backend.factorial">factorial</a></code></li>
<li><code><a title="phiml.backend.Backend.fft" href="#phiml.backend.Backend.fft">fft</a></code></li>
<li><code><a title="phiml.backend.Backend.flatten" href="#phiml.backend.Backend.flatten">flatten</a></code></li>
<li><code><a title="phiml.backend.Backend.flip" href="#phiml.backend.Backend.flip">flip</a></code></li>
<li><code><a title="phiml.backend.Backend.float_type" href="#phiml.backend.Backend.float_type">float_type</a></code></li>
<li><code><a title="phiml.backend.Backend.floor" href="#phiml.backend.Backend.floor">floor</a></code></li>
<li><code><a title="phiml.backend.Backend.floordiv" href="#phiml.backend.Backend.floordiv">floordiv</a></code></li>
<li><code><a title="phiml.backend.Backend.from_dlpack" href="#phiml.backend.Backend.from_dlpack">from_dlpack</a></code></li>
<li><code><a title="phiml.backend.Backend.gamma_inc_l" href="#phiml.backend.Backend.gamma_inc_l">gamma_inc_l</a></code></li>
<li><code><a title="phiml.backend.Backend.gamma_inc_u" href="#phiml.backend.Backend.gamma_inc_u">gamma_inc_u</a></code></li>
<li><code><a title="phiml.backend.Backend.gather" href="#phiml.backend.Backend.gather">gather</a></code></li>
<li><code><a title="phiml.backend.Backend.gather_1d" href="#phiml.backend.Backend.gather_1d">gather_1d</a></code></li>
<li><code><a title="phiml.backend.Backend.gather_by_component_indices" href="#phiml.backend.Backend.gather_by_component_indices">gather_by_component_indices</a></code></li>
<li><code><a title="phiml.backend.Backend.gather_nd" href="#phiml.backend.Backend.gather_nd">gather_nd</a></code></li>
<li><code><a title="phiml.backend.Backend.get_default_device" href="#phiml.backend.Backend.get_default_device">get_default_device</a></code></li>
<li><code><a title="phiml.backend.Backend.get_device" href="#phiml.backend.Backend.get_device">get_device</a></code></li>
<li><code><a title="phiml.backend.Backend.get_device_by_ref" href="#phiml.backend.Backend.get_device_by_ref">get_device_by_ref</a></code></li>
<li><code><a title="phiml.backend.Backend.get_diagonal" href="#phiml.backend.Backend.get_diagonal">get_diagonal</a></code></li>
<li><code><a title="phiml.backend.Backend.get_peak_memory" href="#phiml.backend.Backend.get_peak_memory">get_peak_memory</a></code></li>
<li><code><a title="phiml.backend.Backend.get_sparse_format" href="#phiml.backend.Backend.get_sparse_format">get_sparse_format</a></code></li>
<li><code><a title="phiml.backend.Backend.greater_or_equal" href="#phiml.backend.Backend.greater_or_equal">greater_or_equal</a></code></li>
<li><code><a title="phiml.backend.Backend.greater_than" href="#phiml.backend.Backend.greater_than">greater_than</a></code></li>
<li><code><a title="phiml.backend.Backend.grid_sample" href="#phiml.backend.Backend.grid_sample">grid_sample</a></code></li>
<li><code><a title="phiml.backend.Backend.hessian" href="#phiml.backend.Backend.hessian">hessian</a></code></li>
<li><code><a title="phiml.backend.Backend.histogram1d" href="#phiml.backend.Backend.histogram1d">histogram1d</a></code></li>
<li><code><a title="phiml.backend.Backend.ifft" href="#phiml.backend.Backend.ifft">ifft</a></code></li>
<li><code><a title="phiml.backend.Backend.imag" href="#phiml.backend.Backend.imag">imag</a></code></li>
<li><code><a title="phiml.backend.Backend.indexed_segment_sum" href="#phiml.backend.Backend.indexed_segment_sum">indexed_segment_sum</a></code></li>
<li><code><a title="phiml.backend.Backend.invert" href="#phiml.backend.Backend.invert">invert</a></code></li>
<li><code><a title="phiml.backend.Backend.is_available" href="#phiml.backend.Backend.is_available">is_available</a></code></li>
<li><code><a title="phiml.backend.Backend.is_module" href="#phiml.backend.Backend.is_module">is_module</a></code></li>
<li><code><a title="phiml.backend.Backend.is_sparse" href="#phiml.backend.Backend.is_sparse">is_sparse</a></code></li>
<li><code><a title="phiml.backend.Backend.is_tensor" href="#phiml.backend.Backend.is_tensor">is_tensor</a></code></li>
<li><code><a title="phiml.backend.Backend.isfinite" href="#phiml.backend.Backend.isfinite">isfinite</a></code></li>
<li><code><a title="phiml.backend.Backend.isinf" href="#phiml.backend.Backend.isinf">isinf</a></code></li>
<li><code><a title="phiml.backend.Backend.isnan" href="#phiml.backend.Backend.isnan">isnan</a></code></li>
<li><code><a title="phiml.backend.Backend.jacobian" href="#phiml.backend.Backend.jacobian">jacobian</a></code></li>
<li><code><a title="phiml.backend.Backend.jit_compile" href="#phiml.backend.Backend.jit_compile">jit_compile</a></code></li>
<li><code><a title="phiml.backend.Backend.jit_compile_grad" href="#phiml.backend.Backend.jit_compile_grad">jit_compile_grad</a></code></li>
<li><code><a title="phiml.backend.Backend.jit_compile_hessian" href="#phiml.backend.Backend.jit_compile_hessian">jit_compile_hessian</a></code></li>
<li><code><a title="phiml.backend.Backend.linear" href="#phiml.backend.Backend.linear">linear</a></code></li>
<li><code><a title="phiml.backend.Backend.linear_solve" href="#phiml.backend.Backend.linear_solve">linear_solve</a></code></li>
<li><code><a title="phiml.backend.Backend.linspace" href="#phiml.backend.Backend.linspace">linspace</a></code></li>
<li><code><a title="phiml.backend.Backend.linspace_without_last" href="#phiml.backend.Backend.linspace_without_last">linspace_without_last</a></code></li>
<li><code><a title="phiml.backend.Backend.list_devices" href="#phiml.backend.Backend.list_devices">list_devices</a></code></li>
<li><code><a title="phiml.backend.Backend.log" href="#phiml.backend.Backend.log">log</a></code></li>
<li><code><a title="phiml.backend.Backend.log10" href="#phiml.backend.Backend.log10">log10</a></code></li>
<li><code><a title="phiml.backend.Backend.log2" href="#phiml.backend.Backend.log2">log2</a></code></li>
<li><code><a title="phiml.backend.Backend.log_gamma" href="#phiml.backend.Backend.log_gamma">log_gamma</a></code></li>
<li><code><a title="phiml.backend.Backend.matrix_rank_dense" href="#phiml.backend.Backend.matrix_rank_dense">matrix_rank_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.matrix_solve_least_squares" href="#phiml.backend.Backend.matrix_solve_least_squares">matrix_solve_least_squares</a></code></li>
<li><code><a title="phiml.backend.Backend.max" href="#phiml.backend.Backend.max">max</a></code></li>
<li><code><a title="phiml.backend.Backend.maximum" href="#phiml.backend.Backend.maximum">maximum</a></code></li>
<li><code><a title="phiml.backend.Backend.mean" href="#phiml.backend.Backend.mean">mean</a></code></li>
<li><code><a title="phiml.backend.Backend.meshgrid" href="#phiml.backend.Backend.meshgrid">meshgrid</a></code></li>
<li><code><a title="phiml.backend.Backend.min" href="#phiml.backend.Backend.min">min</a></code></li>
<li><code><a title="phiml.backend.Backend.minimize" href="#phiml.backend.Backend.minimize">minimize</a></code></li>
<li><code><a title="phiml.backend.Backend.minimum" href="#phiml.backend.Backend.minimum">minimum</a></code></li>
<li><code><a title="phiml.backend.Backend.mod" href="#phiml.backend.Backend.mod">mod</a></code></li>
<li><code><a title="phiml.backend.Backend.module" href="#phiml.backend.Backend.module">module</a></code></li>
<li><code><a title="phiml.backend.Backend.mul" href="#phiml.backend.Backend.mul">mul</a></code></li>
<li><code><a title="phiml.backend.Backend.mul_coo_dense" href="#phiml.backend.Backend.mul_coo_dense">mul_coo_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.mul_csr_dense" href="#phiml.backend.Backend.mul_csr_dense">mul_csr_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.mul_matrix_batched_vector" href="#phiml.backend.Backend.mul_matrix_batched_vector">mul_matrix_batched_vector</a></code></li>
<li><code><a title="phiml.backend.Backend.multi_slice" href="#phiml.backend.Backend.multi_slice">multi_slice</a></code></li>
<li><code><a title="phiml.backend.Backend.name" href="#phiml.backend.Backend.name">name</a></code></li>
<li><code><a title="phiml.backend.Backend.ndims" href="#phiml.backend.Backend.ndims">ndims</a></code></li>
<li><code><a title="phiml.backend.Backend.nn_library" href="#phiml.backend.Backend.nn_library">nn_library</a></code></li>
<li><code><a title="phiml.backend.Backend.nonzero" href="#phiml.backend.Backend.nonzero">nonzero</a></code></li>
<li><code><a title="phiml.backend.Backend.not_equal" href="#phiml.backend.Backend.not_equal">not_equal</a></code></li>
<li><code><a title="phiml.backend.Backend.numpy" href="#phiml.backend.Backend.numpy">numpy</a></code></li>
<li><code><a title="phiml.backend.Backend.numpy_call" href="#phiml.backend.Backend.numpy_call">numpy_call</a></code></li>
<li><code><a title="phiml.backend.Backend.ones" href="#phiml.backend.Backend.ones">ones</a></code></li>
<li><code><a title="phiml.backend.Backend.ones_like" href="#phiml.backend.Backend.ones_like">ones_like</a></code></li>
<li><code><a title="phiml.backend.Backend.or_" href="#phiml.backend.Backend.or_">or_</a></code></li>
<li><code><a title="phiml.backend.Backend.pad" href="#phiml.backend.Backend.pad">pad</a></code></li>
<li><code><a title="phiml.backend.Backend.pad_stack" href="#phiml.backend.Backend.pad_stack">pad_stack</a></code></li>
<li><code><a title="phiml.backend.Backend.pad_to" href="#phiml.backend.Backend.pad_to">pad_to</a></code></li>
<li><code><a title="phiml.backend.Backend.pow" href="#phiml.backend.Backend.pow">pow</a></code></li>
<li><code><a title="phiml.backend.Backend.precision" href="#phiml.backend.Backend.precision">precision</a></code></li>
<li><code><a title="phiml.backend.Backend.prefers_channels_last" href="#phiml.backend.Backend.prefers_channels_last">prefers_channels_last</a></code></li>
<li><code><a title="phiml.backend.Backend.prod" href="#phiml.backend.Backend.prod">prod</a></code></li>
<li><code><a title="phiml.backend.Backend.quantile" href="#phiml.backend.Backend.quantile">quantile</a></code></li>
<li><code><a title="phiml.backend.Backend.random_normal" href="#phiml.backend.Backend.random_normal">random_normal</a></code></li>
<li><code><a title="phiml.backend.Backend.random_permutations" href="#phiml.backend.Backend.random_permutations">random_permutations</a></code></li>
<li><code><a title="phiml.backend.Backend.random_subsets" href="#phiml.backend.Backend.random_subsets">random_subsets</a></code></li>
<li><code><a title="phiml.backend.Backend.random_uniform" href="#phiml.backend.Backend.random_uniform">random_uniform</a></code></li>
<li><code><a title="phiml.backend.Backend.range" href="#phiml.backend.Backend.range">range</a></code></li>
<li><code><a title="phiml.backend.Backend.ravel_multi_index" href="#phiml.backend.Backend.ravel_multi_index">ravel_multi_index</a></code></li>
<li><code><a title="phiml.backend.Backend.real" href="#phiml.backend.Backend.real">real</a></code></li>
<li><code><a title="phiml.backend.Backend.repeat" href="#phiml.backend.Backend.repeat">repeat</a></code></li>
<li><code><a title="phiml.backend.Backend.requires_fixed_shapes_when_tracing" href="#phiml.backend.Backend.requires_fixed_shapes_when_tracing">requires_fixed_shapes_when_tracing</a></code></li>
<li><code><a title="phiml.backend.Backend.reset_peak_memory" href="#phiml.backend.Backend.reset_peak_memory">reset_peak_memory</a></code></li>
<li><code><a title="phiml.backend.Backend.reshape" href="#phiml.backend.Backend.reshape">reshape</a></code></li>
<li><code><a title="phiml.backend.Backend.round" href="#phiml.backend.Backend.round">round</a></code></li>
<li><code><a title="phiml.backend.Backend.scatter" href="#phiml.backend.Backend.scatter">scatter</a></code></li>
<li><code><a title="phiml.backend.Backend.scatter_1d_scalar" href="#phiml.backend.Backend.scatter_1d_scalar">scatter_1d_scalar</a></code></li>
<li><code><a title="phiml.backend.Backend.scatter_nd" href="#phiml.backend.Backend.scatter_nd">scatter_nd</a></code></li>
<li><code><a title="phiml.backend.Backend.scatter_nd_scalar" href="#phiml.backend.Backend.scatter_nd_scalar">scatter_nd_scalar</a></code></li>
<li><code><a title="phiml.backend.Backend.searchsorted" href="#phiml.backend.Backend.searchsorted">searchsorted</a></code></li>
<li><code><a title="phiml.backend.Backend.seed" href="#phiml.backend.Backend.seed">seed</a></code></li>
<li><code><a title="phiml.backend.Backend.set_default_device" href="#phiml.backend.Backend.set_default_device">set_default_device</a></code></li>
<li><code><a title="phiml.backend.Backend.shape" href="#phiml.backend.Backend.shape">shape</a></code></li>
<li><code><a title="phiml.backend.Backend.shift_bits_left" href="#phiml.backend.Backend.shift_bits_left">shift_bits_left</a></code></li>
<li><code><a title="phiml.backend.Backend.shift_bits_right" href="#phiml.backend.Backend.shift_bits_right">shift_bits_right</a></code></li>
<li><code><a title="phiml.backend.Backend.sigmoid" href="#phiml.backend.Backend.sigmoid">sigmoid</a></code></li>
<li><code><a title="phiml.backend.Backend.sign" href="#phiml.backend.Backend.sign">sign</a></code></li>
<li><code><a title="phiml.backend.Backend.sin" href="#phiml.backend.Backend.sin">sin</a></code></li>
<li><code><a title="phiml.backend.Backend.sinh" href="#phiml.backend.Backend.sinh">sinh</a></code></li>
<li><code><a title="phiml.backend.Backend.size" href="#phiml.backend.Backend.size">size</a></code></li>
<li><code><a title="phiml.backend.Backend.sizeof" href="#phiml.backend.Backend.sizeof">sizeof</a></code></li>
<li><code><a title="phiml.backend.Backend.softplus" href="#phiml.backend.Backend.softplus">softplus</a></code></li>
<li><code><a title="phiml.backend.Backend.solve_triangular" href="#phiml.backend.Backend.solve_triangular">solve_triangular</a></code></li>
<li><code><a title="phiml.backend.Backend.solve_triangular_dense" href="#phiml.backend.Backend.solve_triangular_dense">solve_triangular_dense</a></code></li>
<li><code><a title="phiml.backend.Backend.solve_triangular_sparse" href="#phiml.backend.Backend.solve_triangular_sparse">solve_triangular_sparse</a></code></li>
<li><code><a title="phiml.backend.Backend.sort" href="#phiml.backend.Backend.sort">sort</a></code></li>
<li><code><a title="phiml.backend.Backend.sparse_coo_tensor" href="#phiml.backend.Backend.sparse_coo_tensor">sparse_coo_tensor</a></code></li>
<li><code><a title="phiml.backend.Backend.sparse_coo_tensor_batched" href="#phiml.backend.Backend.sparse_coo_tensor_batched">sparse_coo_tensor_batched</a></code></li>
<li><code><a title="phiml.backend.Backend.sqrt" href="#phiml.backend.Backend.sqrt">sqrt</a></code></li>
<li><code><a title="phiml.backend.Backend.stack" href="#phiml.backend.Backend.stack">stack</a></code></li>
<li><code><a title="phiml.backend.Backend.stack_leaves" href="#phiml.backend.Backend.stack_leaves">stack_leaves</a></code></li>
<li><code><a title="phiml.backend.Backend.staticshape" href="#phiml.backend.Backend.staticshape">staticshape</a></code></li>
<li><code><a title="phiml.backend.Backend.std" href="#phiml.backend.Backend.std">std</a></code></li>
<li><code><a title="phiml.backend.Backend.stop_gradient" href="#phiml.backend.Backend.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="phiml.backend.Backend.stop_gradient_tree" href="#phiml.backend.Backend.stop_gradient_tree">stop_gradient_tree</a></code></li>
<li><code><a title="phiml.backend.Backend.sub" href="#phiml.backend.Backend.sub">sub</a></code></li>
<li><code><a title="phiml.backend.Backend.sum" href="#phiml.backend.Backend.sum">sum</a></code></li>
<li><code><a title="phiml.backend.Backend.supports" href="#phiml.backend.Backend.supports">supports</a></code></li>
<li><code><a title="phiml.backend.Backend.svd" href="#phiml.backend.Backend.svd">svd</a></code></li>
<li><code><a title="phiml.backend.Backend.tan" href="#phiml.backend.Backend.tan">tan</a></code></li>
<li><code><a title="phiml.backend.Backend.tanh" href="#phiml.backend.Backend.tanh">tanh</a></code></li>
<li><code><a title="phiml.backend.Backend.tensordot" href="#phiml.backend.Backend.tensordot">tensordot</a></code></li>
<li><code><a title="phiml.backend.Backend.tile" href="#phiml.backend.Backend.tile">tile</a></code></li>
<li><code><a title="phiml.backend.Backend.tile_to" href="#phiml.backend.Backend.tile_to">tile_to</a></code></li>
<li><code><a title="phiml.backend.Backend.to_complex" href="#phiml.backend.Backend.to_complex">to_complex</a></code></li>
<li><code><a title="phiml.backend.Backend.to_dlpack" href="#phiml.backend.Backend.to_dlpack">to_dlpack</a></code></li>
<li><code><a title="phiml.backend.Backend.to_float" href="#phiml.backend.Backend.to_float">to_float</a></code></li>
<li><code><a title="phiml.backend.Backend.to_int32" href="#phiml.backend.Backend.to_int32">to_int32</a></code></li>
<li><code><a title="phiml.backend.Backend.to_int64" href="#phiml.backend.Backend.to_int64">to_int64</a></code></li>
<li><code><a title="phiml.backend.Backend.transpose" href="#phiml.backend.Backend.transpose">transpose</a></code></li>
<li><code><a title="phiml.backend.Backend.unique" href="#phiml.backend.Backend.unique">unique</a></code></li>
<li><code><a title="phiml.backend.Backend.unravel_index" href="#phiml.backend.Backend.unravel_index">unravel_index</a></code></li>
<li><code><a title="phiml.backend.Backend.unstack" href="#phiml.backend.Backend.unstack">unstack</a></code></li>
<li><code><a title="phiml.backend.Backend.variable" href="#phiml.backend.Backend.variable">variable</a></code></li>
<li><code><a title="phiml.backend.Backend.vectorized_call" href="#phiml.backend.Backend.vectorized_call">vectorized_call</a></code></li>
<li><code><a title="phiml.backend.Backend.where" href="#phiml.backend.Backend.where">where</a></code></li>
<li><code><a title="phiml.backend.Backend.while_loop" href="#phiml.backend.Backend.while_loop">while_loop</a></code></li>
<li><code><a title="phiml.backend.Backend.xor" href="#phiml.backend.Backend.xor">xor</a></code></li>
<li><code><a title="phiml.backend.Backend.zeros" href="#phiml.backend.Backend.zeros">zeros</a></code></li>
<li><code><a title="phiml.backend.Backend.zeros_like" href="#phiml.backend.Backend.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.ComputeDevice" href="#phiml.backend.ComputeDevice">ComputeDevice</a></code></h4>
<ul class="two-column">
<li><code><a title="phiml.backend.ComputeDevice.backend" href="#phiml.backend.ComputeDevice.backend">backend</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.description" href="#phiml.backend.ComputeDevice.description">description</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.device_type" href="#phiml.backend.ComputeDevice.device_type">device_type</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.memory" href="#phiml.backend.ComputeDevice.memory">memory</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.name" href="#phiml.backend.ComputeDevice.name">name</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.processor_count" href="#phiml.backend.ComputeDevice.processor_count">processor_count</a></code></li>
<li><code><a title="phiml.backend.ComputeDevice.ref" href="#phiml.backend.ComputeDevice.ref">ref</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phiml.backend.NoBackendFound" href="#phiml.backend.NoBackendFound">NoBackendFound</a></code></h4>
</li>
<li>
<h4><code><a title="phiml.backend.Profile" href="#phiml.backend.Profile">Profile</a></code></h4>
<ul class="">
<li><code><a title="phiml.backend.Profile.add_external_message" href="#phiml.backend.Profile.add_external_message">add_external_message</a></code></li>
<li><code><a title="phiml.backend.Profile.duration" href="#phiml.backend.Profile.duration">duration</a></code></li>
<li><code><a title="phiml.backend.Profile.print" href="#phiml.backend.Profile.print">print</a></code></li>
<li><code><a title="phiml.backend.Profile.retime" href="#phiml.backend.Profile.retime">retime</a></code></li>
<li><code><a title="phiml.backend.Profile.save" href="#phiml.backend.Profile.save">save</a></code></li>
<li><code><a title="phiml.backend.Profile.save_trace" href="#phiml.backend.Profile.save_trace">save_trace</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
