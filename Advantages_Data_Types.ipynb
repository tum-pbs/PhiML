{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Why Œ¶<sub>ML</sub> has Precision Management\n",
    "\n",
    "Having control over the floating-point (FP) precision is essential for many scientific applications.\n",
    "For example, some linear systems of equations are only solvable with FP64, even if the desired tolerance lies within FP32 territory.\n",
    "To accommodate these requirements, Œ¶<sub>ML</sub> provides custom precision management tools that differ from the common machine learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from phiml import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Precision in ML libraries\n",
    "\n",
    "First, let's look at the behavior of the backend libraries that Œ¶<sub>ML</sub> supports.\n",
    "\n",
    "**Tensor creation**:\n",
    "Consider creating a float tensor from primitive floats. Can you guess what the data type will be for `tensor(1.)` (or the analogous operations) in NumPy, PyTorch, TensorFlow and Jax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: float64\n",
      "PyTorch: torch.float32\n",
      "TensorFlow: <dtype: 'float32'>\n",
      "Jax: float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"NumPy: {np.array(1.).dtype}\\nPyTorch: {torch.tensor(1.).dtype}\\nTensorFlow: {tf.constant(1.).dtype}\\nJax: {jnp.array(1.).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "IF you guessed `float64` for NumPy, `float32` for PyTorch and TensorFlow, and *depends on the configuration* for Jax, you are correct!\n",
    "Yes, Jax disables FP64 by default! Let's repeat that with FP64 enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax: float64\n"
     ]
    }
   ],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(f\"Jax: {jnp.array(1.).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, Jax behaves like NumPy! Or does it...?\n",
    "\n",
    "**Combining different precisions**:\n",
    "What do you think will happen in each of the base libraries if we sum a FP64 and FP32 tensor?\n",
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(1., dtype=np.float32) + np.array(1., dtype=np.float64)).dtype  # NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(1., dtype=torch.float32) + torch.tensor(1., dtype=torch.float64)).dtype  # PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NumPy and PyTorch automatically upgrade to the highest precision.\n",
    "However, unlike NumPy, PyTorch does not upgrade its `dtype` when adding a primitive `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(1., dtype=np.float32) + 1.).dtype  # NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(1., dtype=torch.float32) + 1.).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at TensorFlow and Jax next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    (tf.constant(1., dtype=tf.float32) + tf.constant(1., dtype=tf.float64)).dtype  # TensorFlow\n",
    "except tf.errors.InvalidArgumentError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TensorFlow outright refuses to mix different precisions and requires manual casting.\n",
    "This is not the case when passing a primitive `float` which is also FP64. Here, TensorFlow keeps the tensor `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.constant(1., dtype=tf.float32) + 1.).dtype  # TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At first glance, Jax seems to upgrade the different precisions like NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jnp.array(1., dtype=jnp.float32) + jnp.array(1., dtype=jnp.float64)).dtype  # Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Let's modify the expression a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t64 = jnp.array(1.)\n",
    "print(t64.dtype)\n",
    "(jnp.array(1., dtype=jnp.float32) + t64).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we also add a `float64` to a `float32` tensor but the result now is `float32`.\n",
    "Jax remembers that we did not explicitly specify the type of the `t64` tensor and treats it differently.\n",
    "\n",
    "Also, Jax does not upgrade the precision when adding a `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jnp.array(1., dtype=jnp.float32) + 1.).dtype  # Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Converting integer tensors**:\n",
    "Let's look at the behavior when combining a `float32` and an `int` tensor in the different libraries. Can you guess what the result type will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(1., dtype=np.float32) + np.array(1)).dtype  # NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(1., dtype=torch.float32) + torch.tensor(1)).dtype  # PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    (tf.constant(1., dtype=tf.float32) + tf.constant(1)).dtype  # TensorFlow\n",
    "except tf.errors.InvalidArgumentError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jnp.array(1., dtype=jnp.float32) + jnp.array(1)).dtype  # Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that NumPy upgrades to 64 bit while PyTorch and Jax keep 32. Like before, TensorFlow refuses to combine different types.\n",
    "When adding a primitive `int` instead, TensorFlow can perform the operation, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.constant(1., dtype=tf.float32) + 1).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Observations\n",
    "\n",
    "We have seen that there is no consistent type handling between the four libraries. In fact no two libraries behave the same.\n",
    "\n",
    "* NumPy defaults to `float64` and upgrades when combining tensors and primitives, including `int`.\n",
    "* PyTorch defaults to `float32` and upgrades only for float tensors, not primitives or integer tensors.\n",
    "* Jax defaults to the precision specified by its configuration and uses involved upgrading rules that take into account whether the initial precision was set or inferred.\n",
    "* TensorFlow defaults to `float32` but requires all tensors to have the same precision, except for Python primitives.\n",
    "\n",
    "| Library    | `f32+f64` | `f32` + primitive `f64` | `f32+i32` |\n",
    "|------------|-----------|-------------------------|-----------|\n",
    "| NumPy      | `f64`     | `f64`                   | `f64`     |\n",
    "| PyTorch    | `f64`     | `f32`                   | `f32`     |\n",
    "| TensorFlow | Error     | `f32`                   | Error     |\n",
    "| Jax        | Depends   | `f32`                   | `f32`     |\n",
    "\n",
    "These inconsistencies indicate that there is not one obvious correct way to handle precision with the data type system these libraries employ, i.e. where the output `dtype` is determined solely by the input types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Precision Management in Œ¶<sub>ML</sub>\n",
    "\n",
    "In Œ¶<sub>ML</sub> the operation / output precision is independent of the inputs. Instead, it can be set globally or by context.\n",
    "The default precision is FP32.\n",
    "\n",
    "**Tensor creation:** Let's create a tensor like above. Can you guess the resulting `dtype`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.tensor(1.).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we have not changed the precision, Œ¶<sub>ML</sub> creates an FP32 tensor.\n",
    "\n",
    "**Combining different precisions**:\n",
    "Can you guess what will happen if we add a `float32` and `float64` tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(math.ones(dtype=(float, 32)) + math.ones(dtype=(float, 64))).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The precision is still set to `float32` so that's what we get.\n",
    "Of course this also applies to adding Python primitives or `int` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(math.ones(dtype=(float, 32)) + math.ones(dtype=int)).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we want to use FP64, we can either set the global precision or execute the code within a precision context.\n",
    "The following line sets the global precision to 64 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "math.set_global_precision(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Executing the above cells now yields `float64` in all cases.\n",
    "Likewise, the precision can be set to 16 bit. In that case we get `float16` even when adding a `float32` and `float64` tensor.\n",
    "\n",
    "As you can see, this system is much simpler and more predictable than the alternatives.\n",
    "It also makes writing code much easier. Upgrading a script that was written for FP32 to FP64 is as simple as setting the global precision, and executing parts of your code with a different precision is as simple as embedding it into a precision block (see example below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## An Example of Mixing Precisions\n",
    "\n",
    "Let's look at a simple application where we want to run operations with both FP32 and PF64, specifically iterate the map `35 (1-cos(x))^2`. The operation `1-cos` is much more sensitive to rounding errors than multiplication, so we wish to compute it using FP64.\n",
    "The expected values after 5 iterations are: 0.2659 (FP64), 0.2663 (FP32), 0.2657 (mixed).\n",
    "\n",
    "Here's the Œ¶<sub>ML</sub> code. We use a `precision` context to execute the inner part with FP64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[94m0.265725\u001b[0m"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.set_global_precision(32)  # reset precision to 32 bit\n",
    "x = math.tensor(.5)\n",
    "for i in range(5):\n",
    "    with math.precision(64):\n",
    "        x = 1 - math.cos(x)\n",
    "    x = x ** 2 * 35\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's implement this using PyTorch. Here we need to manually convert `x` between FP32 and PF64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2657)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(.5)\n",
    "for i in range(5):\n",
    "    x = x.double()\n",
    "    x = 1 - torch.cos(x)\n",
    "    x = x.float()\n",
    "    x = x ** 2 * 35\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These conversions seem relatively tame here, but imagine we had a bunch of variables to keep track of!\n",
    "Making sure they all have the correct precision can be a time sink, especially when one variable with a too-high precision can upgrade all following intermediate results.\n",
    "The danger of this going unnoticed is why TensorFlow and Jax have taken the extreme measures of banning operations with mixed inputs and disabling FP64 by default, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "[Data types in Œ¶<sub>ML</sub>](Data_Types.html)\n",
    "\n",
    "[üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
