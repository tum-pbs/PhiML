{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Matrices in Œ¶<sub>ML</sub>\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Matrices.ipynb)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)\n",
    "\n",
    "This notebook introduces matrices in Œ¶<sub>ML</sub>.\n",
    "Also check out the introduction to [linear solves](Linear_Solves.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install phiml"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Primal and Dual Dimensions\n",
    "\n",
    "Matrices are typically represented as a 2D array with N rows and M columns.\n",
    "They can be multiplied with an M-vector, producing an N-vector, i.e. the horizontal axis of the matrix is reduced in the operation.\n",
    "\n",
    "Œ¶<sub>ML</sub> generally deals with higher-dimensional tensors.\n",
    "Say we want to represent a matrix that transforms one image into another, then both vectors would have shape `(height, width, channels)`, which is too much for a matrix.\n",
    "This is typically resolved by packing these dimensions into one using reshaping, adding boilerplate and making code less readable.\n",
    "\n",
    "Œ¶<sub>ML</sub> allows you to keep all dimensions.\n",
    "This is possible because of dimension types.\n",
    "Œ¶<sub>ML</sub> provides the [`dual`](phiml/math/#phiml.math.dual) dimension type to mark dimensions that will be reduced during a matrix multiplication."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mrows=0\u001B[0m    \u001B[94m  0   1 \u001B[0m along \u001B[92m~cols\u001B[0m\n",
      "\u001B[92mrows=1\u001B[0m    \u001B[94m -1   0 \u001B[0m along \u001B[92m~cols\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from phiml import math\n",
    "from phiml.math import wrap, channel, dual\n",
    "\n",
    "matrix = wrap([[0, 1], [-1, 0]], channel('rows'), dual('cols'))\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the `cols` dimension is marked as `dual`and will be reduced against vectors in a matrix multiplication.\n",
    "Note that the names of dual dimensions begin with `~` to differentiate them from primal (non-dual) dimensions and avoid name conflicts.\n",
    "A matrix mapping images to images would have shape `(~height, ~width, ~channels, height, width, channels)` and the first three dimensions would be reduced during multiplication.\n",
    "\n",
    "Let's perform a matrix multiplication with `matrix`!\n",
    "Dual dimensions are matched against vector dimensions of the same name."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(3, -2)\u001B[0m along \u001B[92mrows·∂ú\u001B[0m"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = wrap([2, 3], channel('cols'))\n",
    "matrix @ vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For matrices with only one dual dimension, other vector dimension names are also allowed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(3, -2)\u001B[0m along \u001B[92mrows·∂ú\u001B[0m"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = wrap([2, 3], channel('vector_dim'))\n",
    "matrix @ vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dot Product between Vectors\n",
    "\n",
    "Matrix multiplications are a special case of a dot product.\n",
    "You can perform general multiply-reduce using [`math.dot()`](phiml/math/#phiml.math.dot) or using the dimension referencing syntax:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(3, -2)\u001B[0m along \u001B[92mc1·∂ú\u001B[0m"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = wrap([[0, 1], [-1, 0]], channel('c1,c2'))\n",
    "vec2 = wrap([2, 3], channel('c3'))\n",
    "vec1.c2 @ vec2.c3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we performed matrix multiplication by explicitly specifying the dimensions to be reduced."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Matrices from Linear Functions\n",
    "\n",
    "Œ¶<sub>ML</sub> can convert linear Python functions into (sparse) matrices.\n",
    "To do this, pass an example input vector (matching the dual / column dimensions of the matrix) to [`math.matrix_from_function()`](phiml/math/#phiml.math.matrix_from_function)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m2.0\u001B[0m"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phiml.math import matrix_from_function, zeros\n",
    "\n",
    "def mul_by_2(x):\n",
    "    return 2 * x\n",
    "\n",
    "example_input = zeros(channel(vector=3))\n",
    "matrix, bias = matrix_from_function(mul_by_2, example_input)\n",
    "matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this simple case, the matrix was identified to be a scalar.\n",
    "Let's multiply all but the *n*th element by 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m 0.  0.  0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m 0.  0.  0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=2\u001B[0m    \u001B[94m 0.  0.  1. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "def mask_first(x, n: int):\n",
    "    one_hot = math.range(x.shape) == n\n",
    "    return x * one_hot\n",
    "\n",
    "matrix, bias = matrix_from_function(mask_first, example_input, n=2)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we got a proper 3x3 matrix.\n",
    "Since our example input had a dimension named `vector`, the columns of the resulting matrix are called `~vector`.\n",
    "\n",
    "In this case, the returned matrix only contains a single non-zero entry."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "sparse coo \u001B[92m(~vector·µà=3, vector·∂ú=3)\u001B[0m with 1 entries: \u001B[92m(entries‚Å±=1)\u001B[0m \u001B[94mconst 1.0\u001B[0m"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create a banded matrix."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m -1.   1.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m  0.  -1.   1. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "def finite_difference_gradient(x):\n",
    "    return x[1:] - x[:-1]\n",
    "\n",
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This gives us a 2x3 matrix since the output is one shorter than the input.\n",
    "We can fix this by padding the input first."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m -1.   1.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m  0.  -1.   1. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=2\u001B[0m    \u001B[94m  0.   0.  -1. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "def finite_difference_gradient(x, padding):\n",
    "    x = math.pad(x, (0, 1), padding)\n",
    "    return x[1:] - x[:-1]\n",
    "\n",
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input, padding=0)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on what padding we use, we get different matrices."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m -1.   1.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m  0.  -1.   1. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=2\u001B[0m    \u001B[94m  1.   0.  -1. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input, padding=math.extrapolation.PERIODIC)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m -1.   1.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m  0.  -1.   1. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=2\u001B[0m    \u001B[94m  0.   0.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input, padding=math.extrapolation.ZERO_GRADIENT)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far, the bias has always been zero because our functions did not add any constants to the output.\n",
    "With a constant non-zero padding, this changes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mvector=0\u001B[0m    \u001B[94m -1.   1.   0. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=1\u001B[0m    \u001B[94m  0.  -1.   1. \u001B[0m along \u001B[92m~vector\u001B[0m\n",
      "\u001B[92mvector=2\u001B[0m    \u001B[94m  0.   0.  -1. \u001B[0m along \u001B[92m~vector\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[94m(0.000, 0.000, 1.000)\u001B[0m"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input, padding=1)\n",
    "math.print(matrix)\n",
    "bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we get the same matrix as with `padding=0`, but the bias has picked up a non-zero term for the last entry.\n",
    "\n",
    "Note that the constructed matrix will prefer NumPy, even when a different backend is selected.\n",
    "When running a linear solve within a [JIT-compiled](JIT.html) function, this allows for matrix and preconditioner construction [at compile time](NumPy_Constants.html), not at runtime."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.use('torch')\n",
    "matrix, bias = matrix_from_function(finite_difference_gradient, example_input, padding=1)\n",
    "matrix.default_backend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matrices can be converted to the selected backend using [`convert`](phiml/math/#phiml.math.convert)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "torch"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = math.convert(matrix)\n",
    "matrix.default_backend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dense and Sparse Matrices\n",
    "\n",
    "Dense matrices are simply tensors that have one or multiple *dual* dimensions.\n",
    "Creating a sparse matrix is therefore as simple as setting the correct dimension type on a tensor.\n",
    "\n",
    "Sparse matrices can be created using [`math.sparse_tensor()`](phiml/math/#phiml.math.sparse_tensor).\n",
    "All non-zero values need to be specified together with their indices.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mrows=0\u001B[0m    \u001B[94m 1.  0.  0. \u001B[0m along \u001B[92m~cols\u001B[0m\n",
      "\u001B[92mrows=1\u001B[0m    \u001B[94m 0.  1.  0. \u001B[0m along \u001B[92m~cols\u001B[0m\n",
      "\u001B[92mrows=2\u001B[0m    \u001B[94m 0.  0.  0. \u001B[0m along \u001B[92m~cols\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "from phiml.math import instance, expand\n",
    "\n",
    "indices = wrap([(0, 0), (1, 1)], instance('indices'), channel(vector='rows,~cols'))\n",
    "values = expand(wrap(1.), instance(indices))\n",
    "dense_shape = channel(rows=3) & dual(cols=3)\n",
    "matrix = math.sparse_tensor(indices, values, dense_shape, format='coo', can_contain_double_entries=False)\n",
    "math.print(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `indices` specify the index for each sparse dimension (`rows` and `~cols` in this case) along a *channel* dimension named `vector`.\n",
    "The labels must match the sparse dimension names but the order is irrelevant.\n",
    "The dimension enumerating the different non-zero values must be an instance dimension and must also be present on `values`.\n",
    "\n",
    "The `format` option allows the creation of different kinds of sparse matrices. `coo` stands for coordinate format, which basically keeps the `indices` and `values¬¥ tensors as-is."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[93mfloat64\u001B[0m sparse coo \u001B[92m(~cols·µà=3, rows·∂ú=3)\u001B[0m with 2 entries: \u001B[92m(indices‚Å±=2)\u001B[0m \u001B[93mfloat64\u001B[0m \u001B[94mconst 1.0\u001B[0m"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other formats include [compressed sparse row](https://de.wikipedia.org/wiki/Compressed_Row_Storage), `csr`, which compresses the row (primal) indices, and `csc`, which compresses the column (dual) indices."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "sparse csr \u001B[92m(~cols·µà=3, rows·∂ú=3)\u001B[0m with 2 entries: \u001B[92m(indices‚Å±=2)\u001B[0m \u001B[94mconst 1.0\u001B[0m"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sparse_tensor(indices, values, dense_shape, format='csr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "sparse csc \u001B[92m(~cols·µà=3, rows·∂ú=3)\u001B[0m with 2 entries: \u001B[92m(indices‚Å±=2)\u001B[0m \u001B[94mconst 1.0\u001B[0m"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sparse_tensor(indices, values, dense_shape, format='csc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you need to perform many matrix multiplications with a single matrix, `csr` is usually a good choice.\n",
    "If you only use the matrix a handful of times, use `coo`.\n",
    "The `csc` format is good for transposed matrix multiplications as well as slicing and concatenating the along column (dual) dimensions.\n",
    "\n",
    "Matrices can be sliced and concatenated like regular tensors. Note the `.dual` instead of `~` for dual dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(1.000, 0.000, 0.000)\u001B[0m along \u001B[92m~cols·µà\u001B[0m"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.rows[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(1.000, 0.000, 0.000)\u001B[0m along \u001B[92mrows·∂ú\u001B[0m"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.cols.dual[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[94m(0.000, 0.000)\u001B[0m; \u001B[94m(1.000, 0.000)\u001B[0m; \u001B[94m(0.000, 0.000)\u001B[0m \u001B[92m(~cols·µà=3, rows·∂ú=2)\u001B[0m"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.rows[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "sparse coo \u001B[92m(~cols·µà=3, rows·∂ú=3)\u001B[0m with 2 entries: \u001B[92m(indices‚Å±=2)\u001B[0m \u001B[94m1.500 ¬± 0.500\u001B[0m \u001B[37m(1e+00...2e+00)\u001B[0m"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row0 = matrix.rows[0:1] * 2\n",
    "other_rows = matrix.rows[1:]\n",
    "math.concat([row0, other_rows], 'rows')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further Reading\n",
    "\n",
    "Matrices with dual dimensions can be used in [linear solves](Linear_Solves.html).\n",
    "\n",
    "[üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}