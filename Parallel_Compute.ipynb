{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parallel Computation in Œ¶<sub>ML</sub>\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Parallel_Compute.ipynb)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)\n",
    "&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)\n",
    "&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()\n",
    "&nbsp; ‚Ä¢ &nbsp; [<img src=\"images/colab_logo_small.png\" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)\n",
    "\n",
    "Œ¶<sub>ML</sub> offers a powerful, flexible and easy-to-use framework for parallel computation based on dataclasses.\n",
    "\n",
    "Prior knowledge of Python's `dataclasses` module and `@cached_property` is recommended. If you are not familiar with dataclasses, please refer to the [official documentation](https://docs.python.org/3/library/dataclasses.html) or the [Dataclasses tutorial](https://realpython.com/python-data-classes/) on Real Python. For `@cached_property`, please refer to the [Cached Properties documentation](https://docs.python.org/3/library/functools.html#functools.cached_property)."
   ],
   "id": "c2bd118ad836bd70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:39:19.638945Z",
     "start_time": "2025-09-13T18:39:19.108377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install phiml"
   ],
   "id": "b5a781751aafdd53",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parallelization with `@parallel_property` and `parallel_compute`\n",
    "\n",
    "Parallelization is always performed over one or multiple Œ¶<sub>ML</sub> dims. Take the following example:"
   ],
   "id": "fd00f7204cd0ddd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:11.984017Z",
     "start_time": "2025-09-13T18:40:11.742036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from phiml.math import Tensor, tensor, map\n",
    "\n",
    "def expensive_to_compute(x: float) -> float:\n",
    "    import time\n",
    "    time.sleep(1)  # Simulate an expensive computation\n",
    "    return 2 * x\n",
    "\n",
    "data = tensor([1.0, 2.0, 3.0], \"data:b\")"
   ],
   "id": "ebcda66d70444c71",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have an expensive computation which we want to perform on each element of the `data` tensor. Without parallelization, this would look like this:",
   "id": "a399cf3cb92f17d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:15.125496Z",
     "start_time": "2025-09-13T18:40:12.117857Z"
    }
   },
   "cell_type": "code",
   "source": "map(expensive_to_compute, data)",
   "id": "3f711774ecd142c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[94m(2.000, 4.000, 6.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's first refactor this computation into a dataclass, expressing the computation as a `@cached_property`:",
   "id": "c2e2a5c9da0d5ac4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:18.146938Z",
     "start_time": "2025-09-13T18:40:15.137052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import cached_property\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SequentialComputation:\n",
    "    data: Tensor\n",
    "\n",
    "    @cached_property\n",
    "    def result(self) -> Tensor:\n",
    "        return map(expensive_to_compute, self.data)\n",
    "\n",
    "SequentialComputation(data).result"
   ],
   "id": "70d1ed91c254f54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[94m(2.000, 4.000, 6.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataclass offers a convenient way of storing the result for future use. Now let's parallelize it!",
   "id": "a12c69da0d611982"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:18.204066Z",
     "start_time": "2025-09-13T18:40:18.151949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "from phiml.dataclasses import parallel_property, parallel_compute\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParallelComputation:\n",
    "    data: Tensor\n",
    "\n",
    "    @parallel_property\n",
    "    def result(self) -> Union[Tensor, float]:\n",
    "        return expensive_to_compute(float(self.data))"
   ],
   "id": "457050c057d2006c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, we have replaced `@cached_property` with `@parallel_property`. The property will now be evaluated on each element of  `data` individually and in parallel.\n",
    "We can trigger the parallel computation using the `parallel_compute` function.\n",
    "\n",
    "Note that `multiprocessing` is used under the hood, so we need to protect the entry point of the program using `if __name__ == \"__main__\":`. To avoid issues with Jupyter notebooks, we recommend declaring the worker code in a separate module outside the notebook."
   ],
   "id": "1a4b8d14f5bff6c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:19.707210Z",
     "start_time": "2025-09-13T18:40:18.218895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parallel_compute import ParallelComputation  # identical to above cell, but can be imported by workers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    computation = ParallelComputation(data)\n",
    "    parallel_compute(computation, [ParallelComputation.result], max_workers=3)\n",
    "    print(computation.result)"
   ],
   "id": "7838d8466a6ad5b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94m(2.000, 4.000, 6.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m \u001B[93mfloat64\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Under the hood, Œ¶<sub>ML</sub> unstacked the `data` tensor and distributed the computation of `result` over 3 worker processes. The results were then stacked back together into a single tensor.",
   "id": "c55acf452085a56d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Property Dependencies\n",
    "\n",
    "In more complex situations, the property to evaluate may depend on other properties. Œ¶<sub>ML</sub> will automatically resolve the dependencies and compute them in the correct order."
   ],
   "id": "d4bf5c730fcae9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:19.716923Z",
     "start_time": "2025-09-13T18:40:19.712497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class ParallelDepComputation:\n",
    "    data: Tensor\n",
    "\n",
    "    @cached_property\n",
    "    def tmp_result(self) -> Union[Tensor, float]:\n",
    "        return expensive_to_compute(float(self.data))\n",
    "\n",
    "    @cached_property\n",
    "    def result(self) -> Union[Tensor, float]:\n",
    "        return self.tmp_result + 1"
   ],
   "id": "ed0cc14d078bc6a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that we can use `@cached_property` for instead of `@parallel_property` for all properties. The only difference is that `@parallel_property` offers additional customization options and does not prevent accidental evaluation outside of `parallel_compute`.",
   "id": "af5bf7fcefa0c30f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:21.251738Z",
     "start_time": "2025-09-13T18:40:19.735628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parallel_compute import ParallelDepComputation  # identical to above cell, but can be imported by workers\n",
    "if __name__ == \"__main__\":\n",
    "    computation = ParallelDepComputation(data)\n",
    "    parallel_compute(computation, [ParallelDepComputation.result], max_workers=3)\n",
    "    print(computation.result)"
   ],
   "id": "24ca9e25afcd5594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94m(3.000, 5.000, 7.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m \u001B[93mfloat64\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The workers automatically computed `tmp_result` first, as it is a dependency of `result`. Note however, that `tmp_result` is not stored in the main process, as it was not requested.",
   "id": "92262fc8c63b7bac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:21.270213Z",
     "start_time": "2025-09-13T18:40:21.266113Z"
    }
   },
   "cell_type": "code",
   "source": "computation.__dict__",
   "id": "4add581c8630f0fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': \u001B[94m(1.000, 2.000, 3.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m,\n",
       " 'result': \u001B[94m(3.000, 5.000, 7.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m \u001B[93mfloat64\u001B[0m}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Non-parallelizable Properties\n",
    "\n",
    "Some properties may not be parallelizable, e.g., because they access values across the parallelization dim. We can mark required dims in the `@parallel_property` decorator to prevent Œ¶<sub>ML</sub> from attempting to parallelize them.\n",
    "\n",
    "Say we want to compute the mean of `expensive_computation` over all elements of `data`:"
   ],
   "id": "b7f24286ee08e984"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:21.305492Z",
     "start_time": "2025-09-13T18:40:21.301088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from phiml import mean, batch\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParallelMeanComputation:\n",
    "    data: Tensor\n",
    "\n",
    "    @parallel_property\n",
    "    def individual_result(self) -> Union[Tensor, float]:\n",
    "        print(f\"Computing individual_result pid={os.getpid()}\")\n",
    "        return expensive_to_compute(float(self.data))\n",
    "\n",
    "    @parallel_property(requires=batch)\n",
    "    def mean(self) -> Union[Tensor, float]:\n",
    "        print(f\"Computing mean, pid={os.getpid()}\")\n",
    "        return mean(self.individual_result, batch)"
   ],
   "id": "48a942d9eaff3554",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:22.833086Z",
     "start_time": "2025-09-13T18:40:21.322379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parallel_compute import ParallelMeanComputation  # identical to above cell, but can be imported by workers\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Host pid={os.getpid()}\")\n",
    "    computation = ParallelMeanComputation(data)\n",
    "    parallel_compute(computation, [ParallelMeanComputation.mean], max_workers=3)\n",
    "    print(computation.mean)"
   ],
   "id": "a067281a06e2efe1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host pid=33696\n",
      "Computing mean, pid=33696\n",
      "\u001B[93mfloat64\u001B[0m \u001B[94m4.0\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The computation is now split into two stages: First, `individual_result` is computed in parallel on each element of `data`. Then, the results are gathered in the host process and the mean is computed over all elements.\n",
    "This is achieved by building a dependency graph of all involved properties under-the-hood and determining the optimal execution strategy.\n",
    "\n",
    "Note that the notebook output only captures the print statements from the main process. You can see the print statements from the worker processes when running the code in a script.\n",
    "\n",
    "As a final example, let's look at the three-stage computation of computing the normalized (mean-subtracted) result. Here, both `individual_result` and `normalized_result` can be parallelized, while `mean` cannot."
   ],
   "id": "b97c8e20bd46e21c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:22.850021Z",
     "start_time": "2025-09-13T18:40:22.845827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class ParallelNormComputation:\n",
    "    data: Tensor\n",
    "\n",
    "    @parallel_property\n",
    "    def individual_result(self) -> Union[Tensor, float]:\n",
    "        print(f\"Computing individual_result pid={os.getpid()}\")\n",
    "        return expensive_to_compute(float(self.data))\n",
    "\n",
    "    @parallel_property(requires=batch)\n",
    "    def mean(self) -> Union[Tensor, float]:\n",
    "        print(f\"Computing mean, pid={os.getpid()}\")\n",
    "        return mean(self.individual_result, batch)\n",
    "\n",
    "    @parallel_property\n",
    "    def normalized_result(self) -> Union[Tensor, float]:\n",
    "        print(f\"Computing normalized_result pid={os.getpid()}\")\n",
    "        return self.individual_result - self.mean"
   ],
   "id": "790e9d48bced8d80",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T18:40:24.390761Z",
     "start_time": "2025-09-13T18:40:22.873207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from parallel_compute import ParallelNormComputation\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Host pid={os.getpid()}\")\n",
    "    computation = ParallelNormComputation(data)\n",
    "    parallel_compute(computation, [ParallelNormComputation.normalized_result], max_workers=3)\n",
    "    print(computation.normalized_result)"
   ],
   "id": "2dd0eba8e573010",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host pid=33696\n",
      "Computing mean, pid=33696\n",
      "\u001B[94m(-2.000, 0.000, 2.000)\u001B[0m along \u001B[92mdata·µá\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Caching Properties on Disk\n",
    "\n",
    "For large data, it may be beneficial to cache intermediate results on disk instead of in memory. This can be achieved by setting the `memory_limit` and `cache_dir` arguments of `parallel_execute`.\n",
    "Storing intermediate results on disk also reduces the transferred data between the main process and the workers, which can be a bottleneck for large data.\n",
    "See [this example](Cached_Parallel_Example.html)."
   ],
   "id": "3ac4ca03c6c02668"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
